Using cuda device.
Training size: 7366 Validation size: 1885 

@@@@  hydrophobic
Model weights: 31985
loss: 0.6886427680  recall:   0.00%  precision:   0.00%  [    1/  250]
loss: 0.6223221473  recall:   0.00%  precision:   0.00%  [    2/  250]
loss: 0.3746081832  recall:   0.00%  precision:   0.00%  [    3/  250]
loss: 0.1623336006  recall:   0.00%  precision:   0.00%  [    4/  250]
loss: 0.0862825388  recall:   0.00%  precision:   0.00%  [    5/  250]
loss: 0.0530806599  recall:   0.01%  precision:   3.83%  [    6/  250]
loss: 0.0384870584  recall:   0.69%  precision:  55.44%  [    7/  250]
loss: 0.0306908297  recall:   3.88%  precision:  65.70%  [    8/  250]
loss: 0.0260203878  recall:  10.55%  precision:  78.95%  [    9/  250]
loss: 0.0228132097  recall:  21.27%  precision:  87.30%  [   10/  250]
loss: 0.0202160227  recall:  36.98%  precision:  91.96%  [   11/  250]
loss: 0.0181501670  recall:  51.13%  precision:  93.96%  [   12/  250]
loss: 0.0163865317  recall:  62.04%  precision:  96.04%  [   13/  250]
loss: 0.0149506172  recall:  73.03%  precision:  97.65%  [   14/  250]
loss: 0.0138584126  recall:  80.87%  precision:  98.60%  [   15/  250]
loss: 0.0130034754  recall:  86.41%  precision:  99.05%  [   16/  250]
loss: 0.0124033816  recall:  89.29%  precision:  99.21%  [   17/  250]
loss: 0.0119430948  recall:  91.20%  precision:  99.33%  [   18/  250]
loss: 0.0115715753  recall:  92.46%  precision:  99.43%  [   19/  250]
loss: 0.0112978026  recall:  93.74%  precision:  99.48%  [   20/  250]
loss: 0.0110564169  recall:  94.57%  precision:  99.54%  [   21/  250]
loss: 0.0108596674  recall:  95.22%  precision:  99.57%  [   22/  250]
loss: 0.0106582639  recall:  96.10%  precision:  99.61%  [   23/  250]
loss: 0.0104969166  recall:  96.46%  precision:  99.63%  [   24/  250]
loss: 0.0103477535  recall:  96.89%  precision:  99.65%  [   25/  250]
loss: 0.0102115843  recall:  97.16%  precision:  99.68%  [   26/  250]
loss: 0.0100873549  recall:  97.55%  precision:  99.71%  [   27/  250]
loss: 0.0099825562  recall:  97.80%  precision:  99.72%  [   28/  250]
loss: 0.0098689652  recall:  98.03%  precision:  99.73%  [   29/  250]
loss: 0.0097621620  recall:  98.03%  precision:  99.74%  [   30/  250]
loss: 0.0096693187  recall:  98.42%  precision:  99.73%  [   31/  250]
loss: 0.0095776535  recall:  98.51%  precision:  99.76%  [   32/  250]
loss: 0.0094853697  recall:  98.64%  precision:  99.76%  [   33/  250]
loss: 0.0094222918  recall:  98.54%  precision:  99.76%  [   34/  250]
loss: 0.0093535027  recall:  98.79%  precision:  99.75%  [   35/  250]
loss: 0.0092559253  recall:  98.90%  precision:  99.77%  [   36/  250]
loss: 0.0092030416  recall:  98.81%  precision:  99.78%  [   37/  250]
loss: 0.0092233397  recall:  98.81%  precision:  99.65%  [   38/  250]
loss: 0.0091164520  recall:  98.98%  precision:  99.74%  [   39/  250]
loss: 0.0090194544  recall:  99.01%  precision:  99.80%  [   40/  250]
loss: 0.0089636446  recall:  98.99%  precision:  99.80%  [   41/  250]
loss: 0.0088968200  recall:  99.13%  precision:  99.80%  [   42/  250]
loss: 0.0088404455  recall:  99.08%  precision:  99.79%  [   43/  250]
loss: 0.0087767864  recall:  99.18%  precision:  99.81%  [   44/  250]
loss: 0.0087315847  recall:  99.06%  precision:  99.81%  [   45/  250]
loss: 0.0086795783  recall:  99.21%  precision:  99.80%  [   46/  250]
loss: 0.0086633200  recall:  99.04%  precision:  99.78%  [   47/  250]
loss: 0.0085894560  recall:  99.18%  precision:  99.81%  [   48/  250]
loss: 0.0085556902  recall:  99.18%  precision:  99.79%  [   49/  250]
loss: 0.0084911362  recall:  99.18%  precision:  99.82%  [   50/  250]
####  eval set loss: 0.0207794277  recall:  98.46%  precision:  98.12%
loss: 0.0084503588  recall:  99.30%  precision:  99.82%  [   51/  250]
loss: 0.0084293111  recall:  99.28%  precision:  99.80%  [   52/  250]
loss: 0.0083569969  recall:  99.36%  precision:  99.82%  [   53/  250]
loss: 0.0083241137  recall:  99.36%  precision:  99.82%  [   54/  250]
loss: 0.0082896806  recall:  99.32%  precision:  99.83%  [   55/  250]
loss: 0.0082785387  recall:  99.32%  precision:  99.78%  [   56/  250]
loss: 0.0082345702  recall:  99.37%  precision:  99.82%  [   57/  250]
loss: 0.0082054476  recall:  99.26%  precision:  99.79%  [   58/  250]
loss: 0.0081374607  recall:  99.47%  precision:  99.84%  [   59/  250]
loss: 0.0081562696  recall:  99.15%  precision:  99.79%  [   60/  250]
loss: 0.0080725370  recall:  99.42%  precision:  99.83%  [   61/  250]
loss: 0.0080346323  recall:  99.42%  precision:  99.84%  [   62/  250]
loss: 0.0080266723  recall:  99.39%  precision:  99.81%  [   63/  250]
loss: 0.0079845431  recall:  99.47%  precision:  99.84%  [   64/  250]
loss: 0.0079672068  recall:  99.39%  precision:  99.79%  [   65/  250]
loss: 0.0079373755  recall:  99.38%  precision:  99.82%  [   66/  250]
loss: 0.0079163910  recall:  99.45%  precision:  99.82%  [   67/  250]
loss: 0.0078779299  recall:  99.49%  precision:  99.83%  [   68/  250]
loss: 0.0078402999  recall:  99.46%  precision:  99.84%  [   69/  250]
loss: 0.0078164890  recall:  99.46%  precision:  99.83%  [   70/  250]
loss: 0.0077839439  recall:  99.48%  precision:  99.82%  [   71/  250]
loss: 0.0077680569  recall:  99.45%  precision:  99.83%  [   72/  250]
loss: 0.0077310949  recall:  99.50%  precision:  99.84%  [   73/  250]
loss: 0.0077371703  recall:  99.44%  precision:  99.82%  [   74/  250]
loss: 0.0077390094  recall:  99.40%  precision:  99.80%  [   75/  250]
loss: 0.0076672323  recall:  99.53%  precision:  99.84%  [   76/  250]
loss: 0.0076393473  recall:  99.43%  precision:  99.83%  [   77/  250]
loss: 0.0076343144  recall:  99.46%  precision:  99.83%  [   78/  250]
loss: 0.0076198122  recall:  99.51%  precision:  99.84%  [   79/  250]
loss: 0.0075705878  recall:  99.44%  precision:  99.84%  [   80/  250]
loss: 0.0075636740  recall:  99.51%  precision:  99.83%  [   81/  250]
loss: 0.0075692885  recall:  99.43%  precision:  99.80%  [   82/  250]
loss: 0.0075291290  recall:  99.50%  precision:  99.83%  [   83/  250]
loss: 0.0075443395  recall:  99.41%  precision:  99.78%  [   84/  250]
loss: 0.0075772762  recall:  99.33%  precision:  99.74%  [   85/  250]
loss: 0.0080532439  recall:  98.91%  precision:  99.46%  [   86/  250]
loss: 0.0081683003  recall:  99.34%  precision:  99.58%  [   87/  250]
loss: 0.0081061232  recall:  98.98%  precision:  99.56%  [   88/  250]
loss: 0.0080925287  recall:  99.47%  precision:  99.78%  [   89/  250]
loss: 0.0076860830  recall:  99.42%  precision:  99.78%  [   90/  250]
loss: 0.0075736466  recall:  99.62%  precision:  99.84%  [   91/  250]
loss: 0.0075038878  recall:  99.67%  precision:  99.85%  [   92/  250]
loss: 0.0074678301  recall:  99.62%  precision:  99.85%  [   93/  250]
loss: 0.0074307882  recall:  99.66%  precision:  99.86%  [   94/  250]
loss: 0.0073997062  recall:  99.64%  precision:  99.86%  [   95/  250]
loss: 0.0073826931  recall:  99.64%  precision:  99.85%  [   96/  250]
loss: 0.0073356552  recall:  99.66%  precision:  99.86%  [   97/  250]
loss: 0.0073249901  recall:  99.65%  precision:  99.86%  [   98/  250]
loss: 0.0072999087  recall:  99.68%  precision:  99.86%  [   99/  250]
loss: 0.0072934519  recall:  99.67%  precision:  99.83%  [  100/  250]
####  eval set loss: 0.0380819285  recall:  98.98%  precision:  98.34%
loss: 0.0072538072  recall:  99.67%  precision:  99.86%  [  101/  250]
loss: 0.0072554780  recall:  99.63%  precision:  99.85%  [  102/  250]
loss: 0.0072242696  recall:  99.67%  precision:  99.87%  [  103/  250]
loss: 0.0072150484  recall:  99.69%  precision:  99.85%  [  104/  250]
loss: 0.0071868747  recall:  99.63%  precision:  99.85%  [  105/  250]
loss: 0.0071775685  recall:  99.68%  precision:  99.85%  [  106/  250]
loss: 0.0071718148  recall:  99.63%  precision:  99.83%  [  107/  250]
loss: 0.0071350476  recall:  99.68%  precision:  99.86%  [  108/  250]
loss: 0.0071416611  recall:  99.65%  precision:  99.84%  [  109/  250]
loss: 0.0070991869  recall:  99.66%  precision:  99.85%  [  110/  250]
loss: 0.0070920456  recall:  99.70%  precision:  99.84%  [  111/  250]
loss: 0.0071059628  recall:  99.62%  precision:  99.83%  [  112/  250]
loss: 0.0070720493  recall:  99.68%  precision:  99.85%  [  113/  250]
loss: 0.0070526479  recall:  99.68%  precision:  99.86%  [  114/  250]
loss: 0.0070440135  recall:  99.66%  precision:  99.85%  [  115/  250]
loss: 0.0070262489  recall:  99.69%  precision:  99.84%  [  116/  250]
loss: 0.0070164839  recall:  99.67%  precision:  99.84%  [  117/  250]
loss: 0.0069835906  recall:  99.69%  precision:  99.86%  [  118/  250]
loss: 0.0069963973  recall:  99.65%  precision:  99.83%  [  119/  250]
loss: 0.0069670721  recall:  99.69%  precision:  99.85%  [  120/  250]
loss: 0.0069412374  recall:  99.69%  precision:  99.86%  [  121/  250]
loss: 0.0069573187  recall:  99.57%  precision:  99.83%  [  122/  250]
loss: 0.0069254415  recall:  99.72%  precision:  99.85%  [  123/  250]
loss: 0.0069412595  recall:  99.57%  precision:  99.82%  [  124/  250]
loss: 0.0069330961  recall:  99.62%  precision:  99.83%  [  125/  250]
loss: 0.0068889770  recall:  99.70%  precision:  99.86%  [  126/  250]
loss: 0.0068695111  recall:  99.67%  precision:  99.86%  [  127/  250]
loss: 0.0068465198  recall:  99.69%  precision:  99.85%  [  128/  250]
loss: 0.0068599611  recall:  99.64%  precision:  99.84%  [  129/  250]
loss: 0.0068477908  recall:  99.68%  precision:  99.85%  [  130/  250]
loss: 0.0068202843  recall:  99.72%  precision:  99.85%  [  131/  250]
loss: 0.0068119931  recall:  99.69%  precision:  99.84%  [  132/  250]
loss: 0.0068095050  recall:  99.73%  precision:  99.85%  [  133/  250]
loss: 0.0068224256  recall:  99.55%  precision:  99.81%  [  134/  250]
loss: 0.0068037046  recall:  99.67%  precision:  99.84%  [  135/  250]
loss: 0.0067919570  recall:  99.58%  precision:  99.81%  [  136/  250]
loss: 0.0068197077  recall:  99.70%  precision:  99.83%  [  137/  250]
loss: 0.0067847691  recall:  99.68%  precision:  99.84%  [  138/  250]
loss: 0.0067984374  recall:  99.65%  precision:  99.83%  [  139/  250]
loss: 0.0067512640  recall:  99.65%  precision:  99.84%  [  140/  250]
loss: 0.0067591935  recall:  99.60%  precision:  99.83%  [  141/  250]
loss: 0.0067400947  recall:  99.62%  precision:  99.82%  [  142/  250]
loss: 0.0066973289  recall:  99.71%  precision:  99.85%  [  143/  250]
loss: 0.0067022038  recall:  99.74%  precision:  99.85%  [  144/  250]
loss: 0.0067233548  recall:  99.60%  precision:  99.83%  [  145/  250]
loss: 0.0067488556  recall:  99.61%  precision:  99.81%  [  146/  250]
loss: 0.0066568563  recall:  99.77%  precision:  99.86%  [  147/  250]
loss: 0.0066481366  recall:  99.71%  precision:  99.86%  [  148/  250]
loss: 0.0066442180  recall:  99.76%  precision:  99.84%  [  149/  250]
loss: 0.0066311651  recall:  99.74%  precision:  99.85%  [  150/  250]
####  eval set loss: 0.0714477545  recall:  99.49%  precision:  98.53%
loss: 0.0066244390  recall:  99.73%  precision:  99.85%  [  151/  250]
loss: 0.0066421924  recall:  99.73%  precision:  99.83%  [  152/  250]
loss: 0.0066202967  recall:  99.73%  precision:  99.85%  [  153/  250]
loss: 0.0066303733  recall:  99.66%  precision:  99.82%  [  154/  250]
loss: 0.0144291359  recall:  98.04%  precision:  98.70%  [  155/  250]
loss: 0.0075441045  recall:  98.36%  precision:  99.49%  [  156/  250]
loss: 0.0070156878  recall:  99.20%  precision:  99.77%  [  157/  250]
loss: 0.0069526752  recall:  99.54%  precision:  99.79%  [  158/  250]
loss: 0.0069105602  recall:  99.54%  precision:  99.83%  [  159/  250]
loss: 0.0068932402  recall:  99.61%  precision:  99.82%  [  160/  250]
loss: 0.0068599611  recall:  99.61%  precision:  99.85%  [  161/  250]
loss: 0.0068370692  recall:  99.69%  precision:  99.85%  [  162/  250]
loss: 0.0068168954  recall:  99.69%  precision:  99.85%  [  163/  250]
loss: 0.0068039711  recall:  99.71%  precision:  99.84%  [  164/  250]
loss: 0.0067942177  recall:  99.71%  precision:  99.85%  [  165/  250]
loss: 0.0067724124  recall:  99.73%  precision:  99.86%  [  166/  250]
loss: 0.0067656004  recall:  99.73%  precision:  99.86%  [  167/  250]
loss: 0.0067407466  recall:  99.72%  precision:  99.85%  [  168/  250]
loss: 0.0067365449  recall:  99.75%  precision:  99.85%  [  169/  250]
loss: 0.0067115509  recall:  99.76%  precision:  99.86%  [  170/  250]
loss: 0.0067041283  recall:  99.76%  precision:  99.86%  [  171/  250]
loss: 0.0066904070  recall:  99.79%  precision:  99.86%  [  172/  250]
loss: 0.0066718830  recall:  99.79%  precision:  99.86%  [  173/  250]
loss: 0.0066585895  recall:  99.77%  precision:  99.87%  [  174/  250]
loss: 0.0066520504  recall:  99.77%  precision:  99.86%  [  175/  250]
loss: 0.0066413494  recall:  99.77%  precision:  99.86%  [  176/  250]
loss: 0.0066345425  recall:  99.78%  precision:  99.86%  [  177/  250]
loss: 0.0066329628  recall:  99.77%  precision:  99.86%  [  178/  250]
loss: 0.0066230924  recall:  99.77%  precision:  99.86%  [  179/  250]
loss: 0.0065965484  recall:  99.81%  precision:  99.86%  [  180/  250]
loss: 0.0065948364  recall:  99.76%  precision:  99.85%  [  181/  250]
loss: 0.0065813608  recall:  99.79%  precision:  99.86%  [  182/  250]
loss: 0.0065690366  recall:  99.80%  precision:  99.87%  [  183/  250]
loss: 0.0065602171  recall:  99.78%  precision:  99.86%  [  184/  250]
loss: 0.0065595980  recall:  99.80%  precision:  99.85%  [  185/  250]
loss: 0.0065351131  recall:  99.78%  precision:  99.86%  [  186/  250]
loss: 0.0065586150  recall:  99.73%  precision:  99.84%  [  187/  250]
loss: 0.0065341365  recall:  99.79%  precision:  99.86%  [  188/  250]
loss: 0.0065247260  recall:  99.80%  precision:  99.86%  [  189/  250]
loss: 0.0065149007  recall:  99.78%  precision:  99.86%  [  190/  250]
loss: 0.0065117138  recall:  99.81%  precision:  99.85%  [  191/  250]
loss: 0.0065049707  recall:  99.78%  precision:  99.86%  [  192/  250]
loss: 0.0064944022  recall:  99.78%  precision:  99.87%  [  193/  250]
loss: 0.0064794199  recall:  99.82%  precision:  99.87%  [  194/  250]
loss: 0.0064956686  recall:  99.77%  precision:  99.84%  [  195/  250]
loss: 0.0064774196  recall:  99.75%  precision:  99.85%  [  196/  250]
loss: 0.0064685708  recall:  99.80%  precision:  99.86%  [  197/  250]
loss: 0.0064577515  recall:  99.80%  precision:  99.86%  [  198/  250]
loss: 0.0064528480  recall:  99.78%  precision:  99.87%  [  199/  250]
loss: 0.0064314613  recall:  99.81%  precision:  99.87%  [  200/  250]
####  eval set loss: 0.0086333892  recall:  99.92%  precision:  99.65%
loss: 0.0064320956  recall:  99.82%  precision:  99.86%  [  201/  250]
loss: 0.0064179553  recall:  99.80%  precision:  99.87%  [  202/  250]
loss: 0.0064160920  recall:  99.79%  precision:  99.87%  [  203/  250]
loss: 0.0064076465  recall:  99.79%  precision:  99.86%  [  204/  250]
loss: 0.0064057326  recall:  99.81%  precision:  99.86%  [  205/  250]
loss: 0.0063968590  recall:  99.79%  precision:  99.85%  [  206/  250]
loss: 0.0063900467  recall:  99.83%  precision:  99.87%  [  207/  250]
loss: 0.0064180482  recall:  99.70%  precision:  99.84%  [  208/  250]
loss: 0.0063879417  recall:  99.79%  precision:  99.85%  [  209/  250]
loss: 0.0063713977  recall:  99.80%  precision:  99.86%  [  210/  250]
loss: 0.0063739998  recall:  99.79%  precision:  99.85%  [  211/  250]
loss: 0.0063447361  recall:  99.81%  precision:  99.86%  [  212/  250]
loss: 0.0063596362  recall:  99.80%  precision:  99.86%  [  213/  250]
loss: 0.0063803364  recall:  99.66%  precision:  99.82%  [  214/  250]
loss: 0.0063392509  recall:  99.82%  precision:  99.86%  [  215/  250]
loss: 0.0063299474  recall:  99.81%  precision:  99.87%  [  216/  250]
loss: 0.0063214446  recall:  99.82%  precision:  99.86%  [  217/  250]
loss: 0.0063130020  recall:  99.79%  precision:  99.85%  [  218/  250]
loss: 0.0063496757  recall:  99.73%  precision:  99.84%  [  219/  250]
loss: 0.0063338848  recall:  99.75%  precision:  99.85%  [  220/  250]
loss: 0.0063204347  recall:  99.81%  precision:  99.87%  [  221/  250]
loss: 0.0063125100  recall:  99.81%  precision:  99.85%  [  222/  250]
loss: 0.0062835419  recall:  99.81%  precision:  99.87%  [  223/  250]
loss: 0.0062814889  recall:  99.80%  precision:  99.87%  [  224/  250]
loss: 0.0062816538  recall:  99.81%  precision:  99.87%  [  225/  250]
loss: 0.0062822633  recall:  99.83%  precision:  99.87%  [  226/  250]
loss: 0.0062697192  recall:  99.81%  precision:  99.86%  [  227/  250]
loss: 0.0062851121  recall:  99.79%  precision:  99.86%  [  228/  250]
loss: 0.0062597595  recall:  99.83%  precision:  99.86%  [  229/  250]
loss: 0.0062485442  recall:  99.82%  precision:  99.86%  [  230/  250]
loss: 0.0062674811  recall:  99.82%  precision:  99.86%  [  231/  250]
loss: 0.0062661880  recall:  99.76%  precision:  99.85%  [  232/  250]
loss: 0.0062347057  recall:  99.83%  precision:  99.87%  [  233/  250]
loss: 0.0062354416  recall:  99.81%  precision:  99.86%  [  234/  250]
loss: 0.0062330429  recall:  99.78%  precision:  99.86%  [  235/  250]
loss: 0.0062432601  recall:  99.79%  precision:  99.85%  [  236/  250]
loss: 0.0062566688  recall:  99.77%  precision:  99.84%  [  237/  250]
loss: 0.0062308500  recall:  99.81%  precision:  99.85%  [  238/  250]
loss: 0.0062396514  recall:  99.75%  precision:  99.85%  [  239/  250]
loss: 0.0062229437  recall:  99.76%  precision:  99.85%  [  240/  250]
loss: 0.0061948568  recall:  99.82%  precision:  99.87%  [  241/  250]
loss: 0.0061931741  recall:  99.86%  precision:  99.87%  [  242/  250]
loss: 0.0061945133  recall:  99.82%  precision:  99.87%  [  243/  250]
loss: 0.0061897713  recall:  99.78%  precision:  99.86%  [  244/  250]
loss: 0.0062159843  recall:  99.77%  precision:  99.83%  [  245/  250]
loss: 0.0061744319  recall:  99.81%  precision:  99.86%  [  246/  250]
loss: 0.0062137052  recall:  99.77%  precision:  99.85%  [  247/  250]
loss: 0.0061911196  recall:  99.79%  precision:  99.85%  [  248/  250]
loss: 0.0061639818  recall:  99.83%  precision:  99.87%  [  249/  250]
loss: 0.0061619217  recall:  99.82%  precision:  99.86%  [  250/  250]
####  eval set loss: 0.0090065716  recall:  99.60%  precision:  99.71%

-------------------------------------------------------------

@@@@  hbond
Model weights: 31985
loss: 0.6851790157  recall:   0.00%  precision:   0.00%  [    1/  250]
loss: 0.5488446786  recall:   0.00%  precision:   0.00%  [    2/  250]
loss: 0.2331014401  recall:   0.00%  precision:   0.00%  [    3/  250]
loss: 0.0900580543  recall:   0.00%  precision:   0.00%  [    4/  250]
loss: 0.0602687363  recall:   0.00%  precision:   0.00%  [    5/  250]
loss: 0.0466939520  recall:   0.00%  precision:   0.00%  [    6/  250]
loss: 0.0378831470  recall:   0.00%  precision:   0.00%  [    7/  250]
loss: 0.0319273188  recall:   0.00%  precision:   0.00%  [    8/  250]
loss: 0.0282754452  recall:   0.03%  precision:   4.10%  [    9/  250]
loss: 0.0257250543  recall:   0.05%  precision:   9.48%  [   10/  250]
loss: 0.0237046174  recall:   0.08%  precision:  17.74%  [   11/  250]
loss: 0.0223436370  recall:   0.09%  precision:  26.85%  [   12/  250]
loss: 0.0213159624  recall:   0.10%  precision:  40.00%  [   13/  250]
loss: 0.0204767331  recall:   0.13%  precision:  52.38%  [   14/  250]
loss: 0.0197483564  recall:   0.17%  precision:  58.02%  [   15/  250]
loss: 0.0191452741  recall:   0.33%  precision:  71.72%  [   16/  250]
loss: 0.0185300004  recall:   0.67%  precision:  82.95%  [   17/  250]
loss: 0.0179756532  recall:   1.16%  precision:  87.14%  [   18/  250]
loss: 0.0174531444  recall:   2.01%  precision:  88.77%  [   19/  250]
loss: 0.0169432866  recall:   3.21%  precision:  89.97%  [   20/  250]
loss: 0.0164701549  recall:   5.61%  precision:  90.85%  [   21/  250]
loss: 0.0159941785  recall:   8.48%  precision:  91.57%  [   22/  250]
loss: 0.0155521778  recall:  12.70%  precision:  91.98%  [   23/  250]
loss: 0.0151278351  recall:  17.02%  precision:  92.09%  [   24/  250]
loss: 0.0147389466  recall:  21.05%  precision:  91.63%  [   25/  250]
loss: 0.0143195784  recall:  24.41%  precision:  91.15%  [   26/  250]
loss: 0.0139880690  recall:  27.63%  precision:  90.94%  [   27/  250]
loss: 0.0137086648  recall:  30.43%  precision:  91.19%  [   28/  250]
loss: 0.0134724142  recall:  33.22%  precision:  91.85%  [   29/  250]
loss: 0.0132577383  recall:  35.52%  precision:  92.29%  [   30/  250]
loss: 0.0130596496  recall:  37.07%  precision:  92.93%  [   31/  250]
loss: 0.0128783069  recall:  38.54%  precision:  93.43%  [   32/  250]
loss: 0.0127235889  recall:  40.82%  precision:  93.73%  [   33/  250]
loss: 0.0125744879  recall:  41.77%  precision:  94.33%  [   34/  250]
loss: 0.0124273983  recall:  43.45%  precision:  94.65%  [   35/  250]
loss: 0.0122831804  recall:  44.78%  precision:  94.95%  [   36/  250]
loss: 0.0121428127  recall:  45.93%  precision:  95.27%  [   37/  250]
loss: 0.0120152311  recall:  47.50%  precision:  95.59%  [   38/  250]
loss: 0.0118856508  recall:  47.95%  precision:  95.90%  [   39/  250]
loss: 0.0117554985  recall:  49.01%  precision:  96.04%  [   40/  250]
loss: 0.0116414580  recall:  50.00%  precision:  96.05%  [   41/  250]
loss: 0.0115326090  recall:  50.43%  precision:  96.31%  [   42/  250]
loss: 0.0114194970  recall:  50.56%  precision:  96.34%  [   43/  250]
loss: 0.0113181104  recall:  51.65%  precision:  96.34%  [   44/  250]
loss: 0.0112317409  recall:  51.70%  precision:  96.43%  [   45/  250]
loss: 0.0111851390  recall:  51.22%  precision:  96.29%  [   46/  250]
loss: 0.0110773247  recall:  52.78%  precision:  96.39%  [   47/  250]
loss: 0.0109975159  recall:  52.65%  precision:  96.57%  [   48/  250]
loss: 0.0109181023  recall:  53.59%  precision:  96.51%  [   49/  250]
loss: 0.0108493662  recall:  53.80%  precision:  96.45%  [   50/  250]
####  eval set loss: 0.0101275343  recall:  55.86%  precision:  96.39%
loss: 0.0107813563  recall:  54.02%  precision:  96.50%  [   51/  250]
loss: 0.0107519839  recall:  54.47%  precision:  96.41%  [   52/  250]
loss: 0.0106594668  recall:  54.55%  precision:  96.56%  [   53/  250]
loss: 0.0106121464  recall:  54.98%  precision:  96.39%  [   54/  250]
loss: 0.0105529883  recall:  54.98%  precision:  96.58%  [   55/  250]
loss: 0.0104967729  recall:  55.71%  precision:  96.44%  [   56/  250]
loss: 0.0104541769  recall:  55.56%  precision:  96.54%  [   57/  250]
loss: 0.0104162173  recall:  55.43%  precision:  96.43%  [   58/  250]
loss: 0.0103652198  recall:  55.97%  precision:  96.49%  [   59/  250]
loss: 0.0103125645  recall:  55.89%  precision:  96.61%  [   60/  250]
loss: 0.0102692965  recall:  56.30%  precision:  96.53%  [   61/  250]
loss: 0.0102186011  recall:  56.45%  precision:  96.66%  [   62/  250]
loss: 0.0101853150  recall:  56.44%  precision:  96.57%  [   63/  250]
loss: 0.0101515289  recall:  56.68%  precision:  96.43%  [   64/  250]
loss: 0.0101015585  recall:  56.80%  precision:  96.56%  [   65/  250]
loss: 0.0100653099  recall:  56.94%  precision:  96.53%  [   66/  250]
loss: 0.0100362047  recall:  57.06%  precision:  96.54%  [   67/  250]
loss: 0.0099930142  recall:  57.14%  precision:  96.58%  [   68/  250]
loss: 0.0099737228  recall:  57.18%  precision:  96.58%  [   69/  250]
loss: 0.0099326403  recall:  57.45%  precision:  96.56%  [   70/  250]
loss: 0.0099010160  recall:  57.66%  precision:  96.55%  [   71/  250]
loss: 0.0098636174  recall:  57.69%  precision:  96.64%  [   72/  250]
loss: 0.0098415160  recall:  57.89%  precision:  96.52%  [   73/  250]
loss: 0.0098073287  recall:  57.85%  precision:  96.58%  [   74/  250]
loss: 0.0097753456  recall:  58.09%  precision:  96.59%  [   75/  250]
loss: 0.0098258040  recall:  57.61%  precision:  96.33%  [   76/  250]
loss: 0.0097518071  recall:  58.26%  precision:  96.48%  [   77/  250]
loss: 0.0097014250  recall:  58.21%  precision:  96.67%  [   78/  250]
loss: 0.0096815257  recall:  58.38%  precision:  96.59%  [   79/  250]
loss: 0.0096474452  recall:  58.67%  precision:  96.67%  [   80/  250]
loss: 0.0096516937  recall:  58.71%  precision:  96.57%  [   81/  250]
loss: 0.0096029096  recall:  58.80%  precision:  96.63%  [   82/  250]
loss: 0.0095915995  recall:  58.93%  precision:  96.47%  [   83/  250]
loss: 0.0095645074  recall:  58.95%  precision:  96.60%  [   84/  250]
loss: 0.0095486025  recall:  58.92%  precision:  96.62%  [   85/  250]
loss: 0.0095199152  recall:  58.99%  precision:  96.65%  [   86/  250]
loss: 0.0094976416  recall:  59.26%  precision:  96.63%  [   87/  250]
loss: 0.0094739710  recall:  59.10%  precision:  96.65%  [   88/  250]
loss: 0.0095083707  recall:  59.75%  precision:  96.25%  [   89/  250]
loss: 0.0094493603  recall:  59.39%  precision:  96.61%  [   90/  250]
loss: 0.0094079727  recall:  59.36%  precision:  96.81%  [   91/  250]
loss: 0.0094015128  recall:  59.51%  precision:  96.73%  [   92/  250]
loss: 0.0093907565  recall:  59.66%  precision:  96.54%  [   93/  250]
loss: 0.0093670264  recall:  59.85%  precision:  96.69%  [   94/  250]
loss: 0.0093357332  recall:  59.91%  precision:  96.74%  [   95/  250]
loss: 0.0093180923  recall:  59.84%  precision:  96.60%  [   96/  250]
loss: 0.0093071268  recall:  59.89%  precision:  96.60%  [   97/  250]
loss: 0.0092972289  recall:  59.66%  precision:  96.54%  [   98/  250]
loss: 0.0092612348  recall:  59.96%  precision:  96.65%  [   99/  250]
loss: 0.0092624067  recall:  60.04%  precision:  96.60%  [  100/  250]
####  eval set loss: 0.0086063383  recall:  63.31%  precision:  96.09%
loss: 0.0092592157  recall:  60.03%  precision:  96.55%  [  101/  250]
loss: 0.0092351391  recall:  59.95%  precision:  96.62%  [  102/  250]
loss: 0.0091883657  recall:  60.24%  precision:  96.71%  [  103/  250]
loss: 0.0091981262  recall:  60.30%  precision:  96.67%  [  104/  250]
loss: 0.0091633702  recall:  60.33%  precision:  96.67%  [  105/  250]
loss: 0.0091899343  recall:  60.31%  precision:  96.42%  [  106/  250]
loss: 0.0091549634  recall:  60.10%  precision:  96.54%  [  107/  250]
loss: 0.0091354830  recall:  60.37%  precision:  96.77%  [  108/  250]
loss: 0.0091156907  recall:  60.61%  precision:  96.64%  [  109/  250]
loss: 0.0091116648  recall:  60.48%  precision:  96.70%  [  110/  250]
loss: 0.0090879964  recall:  60.64%  precision:  96.73%  [  111/  250]
loss: 0.0090752366  recall:  60.47%  precision:  96.81%  [  112/  250]
loss: 0.0103976527  recall:  61.69%  precision:  94.93%  [  113/  250]
loss: 0.0102476653  recall:  58.51%  precision:  95.42%  [  114/  250]
loss: 0.0094981338  recall:  57.90%  precision:  96.35%  [  115/  250]
loss: 0.0093911563  recall:  58.06%  precision:  96.71%  [  116/  250]
loss: 0.0093492908  recall:  58.48%  precision:  96.56%  [  117/  250]
loss: 0.0093167410  recall:  58.33%  precision:  96.80%  [  118/  250]
loss: 0.0092757239  recall:  59.02%  precision:  96.65%  [  119/  250]
loss: 0.0092449294  recall:  58.48%  precision:  96.85%  [  120/  250]
loss: 0.0092263960  recall:  59.11%  precision:  96.71%  [  121/  250]
loss: 0.0091966117  recall:  59.13%  precision:  96.77%  [  122/  250]
loss: 0.0091895311  recall:  59.21%  precision:  96.67%  [  123/  250]
loss: 0.0091608067  recall:  59.38%  precision:  96.84%  [  124/  250]
loss: 0.0091362565  recall:  59.38%  precision:  96.90%  [  125/  250]
loss: 0.0091313158  recall:  59.53%  precision:  96.81%  [  126/  250]
loss: 0.0091224228  recall:  59.50%  precision:  96.79%  [  127/  250]
loss: 0.0090869181  recall:  59.75%  precision:  96.91%  [  128/  250]
loss: 0.0090663324  recall:  59.97%  precision:  96.91%  [  129/  250]
loss: 0.0090519818  recall:  59.86%  precision:  96.84%  [  130/  250]
loss: 0.0090648176  recall:  59.74%  precision:  96.83%  [  131/  250]
loss: 0.0090343838  recall:  59.97%  precision:  96.80%  [  132/  250]
loss: 0.0090186674  recall:  59.92%  precision:  96.92%  [  133/  250]
loss: 0.0089996902  recall:  60.07%  precision:  96.89%  [  134/  250]
loss: 0.0089968349  recall:  60.08%  precision:  96.79%  [  135/  250]
loss: 0.0089957719  recall:  59.95%  precision:  96.88%  [  136/  250]
loss: 0.0089838988  recall:  60.12%  precision:  96.84%  [  137/  250]
loss: 0.0089716486  recall:  60.22%  precision:  96.82%  [  138/  250]
loss: 0.0089593987  recall:  60.27%  precision:  96.90%  [  139/  250]
loss: 0.0089441727  recall:  60.28%  precision:  96.80%  [  140/  250]
loss: 0.0089358067  recall:  60.19%  precision:  96.95%  [  141/  250]
loss: 0.0089199146  recall:  60.32%  precision:  96.87%  [  142/  250]
loss: 0.0089097834  recall:  60.34%  precision:  96.90%  [  143/  250]
loss: 0.0089059459  recall:  60.35%  precision:  96.92%  [  144/  250]
loss: 0.0089031163  recall:  60.43%  precision:  96.86%  [  145/  250]
loss: 0.0088791332  recall:  60.65%  precision:  96.89%  [  146/  250]
loss: 0.0088830588  recall:  60.45%  precision:  96.96%  [  147/  250]
loss: 0.0088839495  recall:  60.30%  precision:  96.95%  [  148/  250]
loss: 0.0088643299  recall:  60.62%  precision:  96.81%  [  149/  250]
loss: 0.0088422902  recall:  60.46%  precision:  96.91%  [  150/  250]
####  eval set loss: 0.0081855373  recall:  62.01%  precision:  96.65%
loss: 0.0088393848  recall:  60.69%  precision:  96.86%  [  151/  250]
loss: 0.0088155538  recall:  60.69%  precision:  96.95%  [  152/  250]
loss: 0.0088342136  recall:  60.33%  precision:  96.95%  [  153/  250]
loss: 0.0088040777  recall:  60.65%  precision:  96.93%  [  154/  250]
loss: 0.0088075207  recall:  60.73%  precision:  96.91%  [  155/  250]
loss: 0.0087981655  recall:  60.60%  precision:  96.97%  [  156/  250]
loss: 0.0088025754  recall:  60.67%  precision:  96.91%  [  157/  250]
loss: 0.0087804854  recall:  60.76%  precision:  96.94%  [  158/  250]
loss: 0.0087713928  recall:  60.81%  precision:  96.81%  [  159/  250]
loss: 0.0087727116  recall:  60.43%  precision:  96.85%  [  160/  250]
loss: 0.0087563715  recall:  60.90%  precision:  96.89%  [  161/  250]
loss: 0.0087423948  recall:  60.79%  precision:  96.99%  [  162/  250]
loss: 0.0087356817  recall:  60.60%  precision:  97.03%  [  163/  250]
loss: 0.0087290037  recall:  60.70%  precision:  96.94%  [  164/  250]
loss: 0.0087347037  recall:  60.84%  precision:  96.93%  [  165/  250]
loss: 0.0087119771  recall:  60.81%  precision:  96.95%  [  166/  250]
loss: 0.0087129620  recall:  60.57%  precision:  97.01%  [  167/  250]
loss: 0.0087080505  recall:  61.12%  precision:  96.90%  [  168/  250]
loss: 0.0086910698  recall:  60.65%  precision:  97.00%  [  169/  250]
loss: 0.0086963195  recall:  60.73%  precision:  96.96%  [  170/  250]
loss: 0.0086750423  recall:  60.88%  precision:  96.98%  [  171/  250]
loss: 0.0086737347  recall:  60.87%  precision:  96.92%  [  172/  250]
loss: 0.0086538098  recall:  60.92%  precision:  96.97%  [  173/  250]
loss: 0.0086720554  recall:  60.76%  precision:  97.03%  [  174/  250]
loss: 0.0086562151  recall:  61.09%  precision:  96.87%  [  175/  250]
loss: 0.0086493076  recall:  60.85%  precision:  96.96%  [  176/  250]
loss: 0.0086359649  recall:  60.80%  precision:  96.98%  [  177/  250]
loss: 0.0086484132  recall:  61.14%  precision:  96.86%  [  178/  250]
loss: 0.0086273332  recall:  60.80%  precision:  97.00%  [  179/  250]
loss: 0.0086074314  recall:  61.14%  precision:  97.00%  [  180/  250]
loss: 0.0086104607  recall:  60.83%  precision:  97.08%  [  181/  250]
loss: 0.0086026669  recall:  61.15%  precision:  97.07%  [  182/  250]
loss: 0.0086039727  recall:  60.97%  precision:  96.95%  [  183/  250]
loss: 0.0086100647  recall:  61.21%  precision:  96.90%  [  184/  250]
loss: 0.0085921810  recall:  60.91%  precision:  96.90%  [  185/  250]
loss: 0.0085943387  recall:  60.95%  precision:  96.90%  [  186/  250]
loss: 0.0085713265  recall:  61.09%  precision:  97.05%  [  187/  250]
loss: 0.0085829515  recall:  61.20%  precision:  96.90%  [  188/  250]
loss: 0.0085612985  recall:  60.98%  precision:  96.98%  [  189/  250]
loss: 0.0085431509  recall:  61.02%  precision:  97.00%  [  190/  250]
loss: 0.0085614062  recall:  60.88%  precision:  96.93%  [  191/  250]
loss: 0.0085601016  recall:  60.94%  precision:  96.94%  [  192/  250]
loss: 0.0085352565  recall:  61.35%  precision:  97.04%  [  193/  250]
loss: 0.0085362890  recall:  61.01%  precision:  97.03%  [  194/  250]
loss: 0.0085328633  recall:  60.99%  precision:  96.95%  [  195/  250]
loss: 0.0085208534  recall:  60.92%  precision:  97.08%  [  196/  250]
loss: 0.0085275804  recall:  61.32%  precision:  96.92%  [  197/  250]
loss: 0.0085119928  recall:  61.14%  precision:  96.99%  [  198/  250]
loss: 0.0085249474  recall:  61.01%  precision:  96.93%  [  199/  250]
loss: 0.0084918556  recall:  61.26%  precision:  97.10%  [  200/  250]
####  eval set loss: 0.0078450128  recall:  62.48%  precision:  96.82%
loss: 0.0084974474  recall:  61.08%  precision:  96.99%  [  201/  250]
loss: 0.0084869994  recall:  61.05%  precision:  96.97%  [  202/  250]
loss: 0.0085016623  recall:  61.15%  precision:  96.97%  [  203/  250]
loss: 0.0084863267  recall:  61.22%  precision:  96.92%  [  204/  250]
loss: 0.0084697472  recall:  61.34%  precision:  96.99%  [  205/  250]
loss: 0.0084718123  recall:  60.98%  precision:  97.02%  [  206/  250]
loss: 0.0084976093  recall:  61.38%  precision:  96.74%  [  207/  250]
loss: 0.0084660431  recall:  61.25%  precision:  96.95%  [  208/  250]
loss: 0.0084671880  recall:  61.21%  precision:  96.89%  [  209/  250]
loss: 0.0084853387  recall:  61.31%  precision:  96.73%  [  210/  250]
loss: 0.0084489271  recall:  61.21%  precision:  96.95%  [  211/  250]
loss: 0.0084294719  recall:  61.05%  precision:  97.08%  [  212/  250]
loss: 0.0084304769  recall:  61.30%  precision:  96.97%  [  213/  250]
loss: 0.0084152583  recall:  61.37%  precision:  97.01%  [  214/  250]
loss: 0.0084416059  recall:  61.41%  precision:  96.97%  [  215/  250]
loss: 0.0084067913  recall:  61.21%  precision:  97.07%  [  216/  250]
loss: 0.0084273949  recall:  61.19%  precision:  97.02%  [  217/  250]
loss: 0.0084123405  recall:  61.29%  precision:  97.01%  [  218/  250]
loss: 0.0083870947  recall:  61.31%  precision:  97.07%  [  219/  250]
loss: 0.0084144865  recall:  61.35%  precision:  96.97%  [  220/  250]
loss: 0.0083831675  recall:  61.15%  precision:  97.07%  [  221/  250]
loss: 0.0083972118  recall:  61.29%  precision:  96.96%  [  222/  250]
loss: 0.0084027420  recall:  61.34%  precision:  96.83%  [  223/  250]
loss: 0.0083878608  recall:  61.43%  precision:  96.92%  [  224/  250]
loss: 0.0083782003  recall:  61.21%  precision:  97.09%  [  225/  250]
loss: 0.0083617439  recall:  61.28%  precision:  97.09%  [  226/  250]
loss: 0.0083661760  recall:  61.45%  precision:  97.00%  [  227/  250]
loss: 0.0083839863  recall:  61.31%  precision:  96.97%  [  228/  250]
loss: 0.0083516072  recall:  61.23%  precision:  97.08%  [  229/  250]
loss: 0.0083436420  recall:  61.42%  precision:  97.06%  [  230/  250]
loss: 0.0083755788  recall:  61.35%  precision:  96.98%  [  231/  250]
loss: 0.0083357489  recall:  61.40%  precision:  97.05%  [  232/  250]
loss: 0.0083426819  recall:  61.25%  precision:  97.02%  [  233/  250]
loss: 0.0083363290  recall:  61.59%  precision:  96.97%  [  234/  250]
loss: 0.0083251728  recall:  61.36%  precision:  97.09%  [  235/  250]
loss: 0.0083152270  recall:  61.54%  precision:  97.06%  [  236/  250]
loss: 0.0083224159  recall:  61.39%  precision:  97.06%  [  237/  250]
loss: 0.0082987112  recall:  61.60%  precision:  97.08%  [  238/  250]
loss: 0.0083042585  recall:  61.38%  precision:  97.03%  [  239/  250]
loss: 0.0082897667  recall:  61.34%  precision:  97.10%  [  240/  250]
loss: 0.0083001582  recall:  61.53%  precision:  97.13%  [  241/  250]
loss: 0.0083040025  recall:  61.80%  precision:  97.02%  [  242/  250]
loss: 0.0082776815  recall:  61.46%  precision:  97.11%  [  243/  250]
loss: 0.0082875961  recall:  61.46%  precision:  97.10%  [  244/  250]
loss: 0.0082657124  recall:  61.47%  precision:  97.15%  [  245/  250]
loss: 0.0082716633  recall:  61.61%  precision:  96.91%  [  246/  250]
loss: 0.0082893383  recall:  61.25%  precision:  97.04%  [  247/  250]
loss: 0.0082747611  recall:  61.47%  precision:  96.99%  [  248/  250]
loss: 0.0082844556  recall:  61.51%  precision:  96.88%  [  249/  250]
loss: 0.0082537629  recall:  61.62%  precision:  97.02%  [  250/  250]
####  eval set loss: 0.0076595101  recall:  59.16%  precision:  97.66%

-------------------------------------------------------------

@@@@  pistacking
Model weights: 31985
loss: 0.6852736658  recall:   0.00%  precision:   0.00%  [    1/  250]
loss: 0.6260399222  recall:   0.00%  precision:  15.00%  [    2/  250]
loss: 0.3478575339  recall:   0.00%  precision:   0.00%  [    3/  250]
loss: 0.1447230938  recall:   0.00%  precision:   0.11%  [    4/  250]
loss: 0.0946894415  recall:   0.00%  precision:   0.23%  [    5/  250]
loss: 0.0769611640  recall:   0.01%  precision:   1.56%  [    6/  250]
loss: 0.0685025608  recall:   0.05%  precision:   8.40%  [    7/  250]
loss: 0.0629458158  recall:   0.08%  precision:  19.06%  [    8/  250]
loss: 0.0589032318  recall:   0.09%  precision:  30.13%  [    9/  250]
loss: 0.0559056826  recall:   0.12%  precision:  37.74%  [   10/  250]
loss: 0.0534336181  recall:   0.15%  precision:  45.50%  [   11/  250]
loss: 0.0515793144  recall:   0.18%  precision:  48.55%  [   12/  250]
loss: 0.0499817405  recall:   0.21%  precision:  51.15%  [   13/  250]
loss: 0.0485720809  recall:   0.26%  precision:  57.69%  [   14/  250]
loss: 0.0475882698  recall:   0.32%  precision:  58.27%  [   15/  250]
loss: 0.0466634693  recall:   0.35%  precision:  64.49%  [   16/  250]
loss: 0.0458985732  recall:   0.44%  precision:  67.52%  [   17/  250]
loss: 0.0449739303  recall:   0.48%  precision:  71.41%  [   18/  250]
loss: 0.0442970308  recall:   0.57%  precision:  73.72%  [   19/  250]
loss: 0.0436538769  recall:   0.67%  precision:  76.58%  [   20/  250]
loss: 0.0431383947  recall:   0.81%  precision:  77.68%  [   21/  250]
loss: 0.0425158609  recall:   0.98%  precision:  81.79%  [   22/  250]
loss: 0.0420591187  recall:   1.18%  precision:  82.74%  [   23/  250]
loss: 0.0416393922  recall:   1.32%  precision:  82.90%  [   24/  250]
loss: 0.0411775844  recall:   1.38%  precision:  84.08%  [   25/  250]
loss: 0.0407209632  recall:   1.67%  precision:  84.50%  [   26/  250]
loss: 0.0404591555  recall:   1.86%  precision:  84.70%  [   27/  250]
loss: 0.0401430604  recall:   1.98%  precision:  85.70%  [   28/  250]
loss: 0.0399452791  recall:   2.00%  precision:  85.25%  [   29/  250]
loss: 0.0395514970  recall:   2.07%  precision:  86.61%  [   30/  250]
loss: 0.0394246343  recall:   2.29%  precision:  86.30%  [   31/  250]
loss: 0.0393569636  recall:   2.13%  precision:  85.90%  [   32/  250]
loss: 0.0390106185  recall:   2.22%  precision:  87.39%  [   33/  250]
loss: 0.0388502736  recall:   2.52%  precision:  85.86%  [   34/  250]
loss: 0.0388294331  recall:   2.34%  precision:  87.33%  [   35/  250]
loss: 0.0385417974  recall:   2.47%  precision:  86.34%  [   36/  250]
loss: 0.0384755115  recall:   2.43%  precision:  87.69%  [   37/  250]
loss: 0.0383282432  recall:   2.42%  precision:  86.76%  [   38/  250]
loss: 0.0381417853  recall:   2.72%  precision:  87.75%  [   39/  250]
loss: 0.0379771635  recall:   2.62%  precision:  88.24%  [   40/  250]
loss: 0.0378977611  recall:   2.64%  precision:  88.49%  [   41/  250]
loss: 0.0378389059  recall:   2.81%  precision:  88.20%  [   42/  250]
loss: 0.0376528036  recall:   2.68%  precision:  87.76%  [   43/  250]
loss: 0.0375240866  recall:   2.83%  precision:  87.65%  [   44/  250]
loss: 0.0375956979  recall:   2.76%  precision:  88.08%  [   45/  250]
loss: 0.0374317684  recall:   2.84%  precision:  87.87%  [   46/  250]
loss: 0.0373357053  recall:   2.92%  precision:  89.00%  [   47/  250]
loss: 0.0372322219  recall:   2.82%  precision:  89.01%  [   48/  250]
loss: 0.0371293176  recall:   3.07%  precision:  88.08%  [   49/  250]
loss: 0.0370858110  recall:   2.99%  precision:  88.65%  [   50/  250]
####  eval set loss: 0.0317401877  recall:   4.06%  precision:  89.80%
loss: 0.0369739729  recall:   3.16%  precision:  88.68%  [   51/  250]
loss: 0.0369464105  recall:   3.14%  precision:  89.14%  [   52/  250]
loss: 0.0368304792  recall:   3.20%  precision:  89.09%  [   53/  250]
loss: 0.0367543922  recall:   3.22%  precision:  89.44%  [   54/  250]
loss: 0.0366809304  recall:   3.07%  precision:  89.11%  [   55/  250]
loss: 0.0365889914  recall:   3.49%  precision:  88.54%  [   56/  250]
loss: 0.0365022359  recall:   3.28%  precision:  89.08%  [   57/  250]
loss: 0.0364900015  recall:   3.61%  precision:  88.81%  [   58/  250]
loss: 0.0364575298  recall:   3.33%  precision:  89.40%  [   59/  250]
loss: 0.0362878230  recall:   3.55%  precision:  89.07%  [   60/  250]
loss: 0.0363170241  recall:   3.45%  precision:  89.17%  [   61/  250]
loss: 0.0362246718  recall:   3.72%  precision:  89.05%  [   62/  250]
loss: 0.0363562901  recall:   3.27%  precision:  89.16%  [   63/  250]
loss: 0.0361503568  recall:   3.63%  precision:  89.07%  [   64/  250]
loss: 0.0360887284  recall:   3.69%  precision:  89.29%  [   65/  250]
loss: 0.0359892525  recall:   3.76%  precision:  89.07%  [   66/  250]
loss: 0.0359098113  recall:   3.87%  precision:  89.02%  [   67/  250]
loss: 0.0360071762  recall:   3.91%  precision:  88.52%  [   68/  250]
loss: 0.0357936448  recall:   3.60%  precision:  89.20%  [   69/  250]
loss: 0.0357177667  recall:   3.81%  precision:  89.10%  [   70/  250]
loss: 0.0356563423  recall:   4.05%  precision:  89.12%  [   71/  250]
loss: 0.0356418058  recall:   4.04%  precision:  89.01%  [   72/  250]
loss: 0.0355216897  recall:   4.12%  precision:  88.92%  [   73/  250]
loss: 0.0354966051  recall:   4.01%  precision:  89.01%  [   74/  250]
loss: 0.0354596585  recall:   4.12%  precision:  88.78%  [   75/  250]
loss: 0.0353518784  recall:   4.02%  precision:  89.05%  [   76/  250]
loss: 0.0354566378  recall:   4.15%  precision:  88.48%  [   77/  250]
loss: 0.0352880501  recall:   4.14%  precision:  88.82%  [   78/  250]
loss: 0.0353098981  recall:   3.99%  precision:  88.98%  [   79/  250]
loss: 0.0352344717  recall:   4.08%  precision:  88.92%  [   80/  250]
loss: 0.0352833267  recall:   4.26%  precision:  88.71%  [   81/  250]
loss: 0.0351117863  recall:   4.37%  precision:  88.80%  [   82/  250]
loss: 0.0351732898  recall:   4.16%  precision:  89.32%  [   83/  250]
loss: 0.0351474244  recall:   4.09%  precision:  88.56%  [   84/  250]
loss: 0.0349801041  recall:   4.56%  precision:  89.03%  [   85/  250]
loss: 0.0350406410  recall:   4.16%  precision:  88.87%  [   86/  250]
loss: 0.0349037061  recall:   4.44%  precision:  89.08%  [   87/  250]
loss: 0.0348959098  recall:   4.18%  precision:  89.30%  [   88/  250]
loss: 0.0348470762  recall:   4.42%  precision:  89.35%  [   89/  250]
loss: 0.0348348983  recall:   4.57%  precision:  89.65%  [   90/  250]
loss: 0.0349395769  recall:   4.37%  precision:  89.04%  [   91/  250]
loss: 0.0349853305  recall:   3.97%  precision:  89.25%  [   92/  250]
loss: 0.0347419478  recall:   4.56%  precision:  89.05%  [   93/  250]
loss: 0.0346408087  recall:   4.52%  precision:  89.11%  [   94/  250]
loss: 0.0346193592  recall:   4.27%  precision:  89.83%  [   95/  250]
loss: 0.0345962520  recall:   4.66%  precision:  89.37%  [   96/  250]
loss: 0.0345872812  recall:   4.51%  precision:  89.61%  [   97/  250]
loss: 0.0345811526  recall:   4.59%  precision:  88.84%  [   98/  250]
loss: 0.0345611953  recall:   4.44%  precision:  89.72%  [   99/  250]
loss: 0.0344791508  recall:   4.73%  precision:  89.15%  [  100/  250]
####  eval set loss: 0.0293813984  recall:   5.47%  precision:  89.87%
loss: 0.0344184723  recall:   4.48%  precision:  89.35%  [  101/  250]
loss: 0.0343922279  recall:   4.71%  precision:  88.77%  [  102/  250]
loss: 0.0343245795  recall:   4.73%  precision:  89.33%  [  103/  250]
loss: 0.0344652234  recall:   4.59%  precision:  89.34%  [  104/  250]
loss: 0.0343997425  recall:   4.46%  precision:  89.22%  [  105/  250]
loss: 0.0343015668  recall:   4.96%  precision:  88.85%  [  106/  250]
loss: 0.0342819785  recall:   4.62%  precision:  89.16%  [  107/  250]
loss: 0.0341712347  recall:   4.77%  precision:  88.86%  [  108/  250]
loss: 0.0342607285  recall:   4.62%  precision:  89.26%  [  109/  250]
loss: 0.0341794534  recall:   4.60%  precision:  88.97%  [  110/  250]
loss: 0.0341203078  recall:   4.90%  precision:  88.95%  [  111/  250]
loss: 0.0340698906  recall:   4.86%  precision:  88.63%  [  112/  250]
loss: 0.0340950814  recall:   4.69%  precision:  89.41%  [  113/  250]
loss: 0.0339920097  recall:   4.95%  precision:  89.37%  [  114/  250]
loss: 0.0341726924  recall:   4.53%  precision:  89.27%  [  115/  250]
loss: 0.0339958305  recall:   4.88%  precision:  89.40%  [  116/  250]
loss: 0.0339110526  recall:   5.13%  precision:  89.61%  [  117/  250]
loss: 0.0339009565  recall:   5.00%  precision:  89.07%  [  118/  250]
loss: 0.0338493653  recall:   4.63%  precision:  89.39%  [  119/  250]
loss: 0.0340902041  recall:   4.83%  precision:  89.13%  [  120/  250]
loss: 0.0337958823  recall:   4.90%  precision:  89.10%  [  121/  250]
loss: 0.0338296951  recall:   4.87%  precision:  89.29%  [  122/  250]
loss: 0.0337929511  recall:   4.96%  precision:  88.77%  [  123/  250]
loss: 0.0340052568  recall:   4.87%  precision:  89.28%  [  124/  250]
loss: 0.0338716967  recall:   4.98%  precision:  88.91%  [  125/  250]
loss: 0.0337659553  recall:   4.87%  precision:  88.88%  [  126/  250]
loss: 0.0336844365  recall:   4.96%  precision:  89.70%  [  127/  250]
loss: 0.0336678208  recall:   4.94%  precision:  89.09%  [  128/  250]
loss: 0.0337246651  recall:   5.22%  precision:  89.46%  [  129/  250]
loss: 0.0336846908  recall:   5.18%  precision:  89.15%  [  130/  250]
loss: 0.0336130706  recall:   5.13%  precision:  88.74%  [  131/  250]
loss: 0.0336225719  recall:   5.16%  precision:  89.23%  [  132/  250]
loss: 0.0336514644  recall:   5.04%  precision:  89.25%  [  133/  250]
loss: 0.0335120570  recall:   5.19%  precision:  89.08%  [  134/  250]
loss: 0.0335176405  recall:   4.91%  precision:  89.43%  [  135/  250]
loss: 0.0334818590  recall:   5.09%  precision:  89.32%  [  136/  250]
loss: 0.0334790216  recall:   5.34%  precision:  88.97%  [  137/  250]
loss: 0.0334955351  recall:   5.05%  precision:  89.50%  [  138/  250]
loss: 0.0335057940  recall:   5.26%  precision:  88.63%  [  139/  250]
loss: 0.0334918816  recall:   5.24%  precision:  88.41%  [  140/  250]
loss: 0.0335695353  recall:   5.16%  precision:  88.93%  [  141/  250]
loss: 0.0334487932  recall:   5.31%  precision:  88.98%  [  142/  250]
loss: 0.0334342286  recall:   4.84%  precision:  89.01%  [  143/  250]
loss: 0.0333518276  recall:   5.39%  precision:  88.69%  [  144/  250]
loss: 0.0333065737  recall:   5.29%  precision:  88.98%  [  145/  250]
loss: 0.0334149043  recall:   5.18%  precision:  89.22%  [  146/  250]
loss: 0.0333466477  recall:   5.48%  precision:  88.95%  [  147/  250]
loss: 0.0333487231  recall:   5.44%  precision:  88.66%  [  148/  250]
loss: 0.0332703474  recall:   5.45%  precision:  89.21%  [  149/  250]
loss: 0.0333057430  recall:   5.21%  precision:  89.51%  [  150/  250]
####  eval set loss: 0.0282094332  recall:   5.51%  precision:  90.74%
loss: 0.0334430260  recall:   5.05%  precision:  89.34%  [  151/  250]
loss: 0.0331714887  recall:   5.56%  precision:  89.25%  [  152/  250]
loss: 0.0332075386  recall:   5.39%  precision:  89.21%  [  153/  250]
loss: 0.0331807748  recall:   5.54%  precision:  88.69%  [  154/  250]
loss: 0.0332082263  recall:   5.18%  precision:  89.52%  [  155/  250]
loss: 0.0332085263  recall:   5.42%  precision:  89.56%  [  156/  250]
loss: 0.0331476373  recall:   5.37%  precision:  89.57%  [  157/  250]
loss: 0.0331503682  recall:   5.62%  precision:  88.98%  [  158/  250]
loss: 0.0331854103  recall:   5.44%  precision:  88.87%  [  159/  250]
loss: 0.0330984440  recall:   5.30%  precision:  89.63%  [  160/  250]
loss: 0.0331740401  recall:   5.57%  precision:  89.23%  [  161/  250]
loss: 0.0330645282  recall:   5.36%  precision:  89.43%  [  162/  250]
loss: 0.0330044714  recall:   5.40%  precision:  89.31%  [  163/  250]
loss: 0.0330616436  recall:   5.71%  precision:  88.64%  [  164/  250]
loss: 0.0329563811  recall:   5.47%  precision:  89.46%  [  165/  250]
loss: 0.0330031830  recall:   5.55%  precision:  89.77%  [  166/  250]
loss: 0.0330631255  recall:   5.45%  precision:  89.10%  [  167/  250]
loss: 0.0330089022  recall:   5.48%  precision:  89.76%  [  168/  250]
loss: 0.0329869315  recall:   5.55%  precision:  89.40%  [  169/  250]
loss: 0.0331594208  recall:   5.27%  precision:  88.80%  [  170/  250]
loss: 0.0329274526  recall:   5.96%  precision:  89.31%  [  171/  250]
loss: 0.0328867935  recall:   5.22%  precision:  90.06%  [  172/  250]
loss: 0.0328927626  recall:   5.84%  precision:  88.66%  [  173/  250]
loss: 0.0328694401  recall:   5.58%  precision:  89.47%  [  174/  250]
loss: 0.0329117813  recall:   5.60%  precision:  89.13%  [  175/  250]
loss: 0.0328470963  recall:   5.65%  precision:  89.10%  [  176/  250]
loss: 0.0330841685  recall:   5.19%  precision:  88.56%  [  177/  250]
loss: 0.0328062425  recall:   5.57%  precision:  89.10%  [  178/  250]
loss: 0.0328240483  recall:   5.59%  precision:  89.22%  [  179/  250]
loss: 0.0328066047  recall:   5.30%  precision:  90.08%  [  180/  250]
loss: 0.0327616091  recall:   5.58%  precision:  89.37%  [  181/  250]
loss: 0.0328446168  recall:   5.58%  precision:  89.55%  [  182/  250]
loss: 0.0327687465  recall:   5.66%  precision:  88.86%  [  183/  250]
loss: 0.0327842590  recall:   5.69%  precision:  89.33%  [  184/  250]
loss: 0.0327970059  recall:   5.58%  precision:  88.95%  [  185/  250]
loss: 0.0327599998  recall:   5.36%  precision:  89.29%  [  186/  250]
loss: 0.0327869352  recall:   5.71%  precision:  88.98%  [  187/  250]
loss: 0.0328269872  recall:   5.79%  precision:  89.27%  [  188/  250]
loss: 0.0327408693  recall:   5.41%  precision:  89.44%  [  189/  250]
loss: 0.0326974981  recall:   5.65%  precision:  89.19%  [  190/  250]
loss: 0.0327739388  recall:   5.40%  precision:  89.57%  [  191/  250]
loss: 0.0327184771  recall:   5.57%  precision:  89.76%  [  192/  250]
loss: 0.0326031473  recall:   5.59%  precision:  89.42%  [  193/  250]
loss: 0.0327597320  recall:   5.62%  precision:  89.10%  [  194/  250]
loss: 0.0328657687  recall:   5.24%  precision:  89.39%  [  195/  250]
loss: 0.0325582019  recall:   5.82%  precision:  89.54%  [  196/  250]
loss: 0.0326208198  recall:   5.61%  precision:  89.28%  [  197/  250]
loss: 0.0327041560  recall:   5.46%  precision:  89.50%  [  198/  250]
loss: 0.0326063180  recall:   5.48%  precision:  88.94%  [  199/  250]
loss: 0.0325823928  recall:   5.91%  precision:  89.25%  [  200/  250]
####  eval set loss: 0.0273698017  recall:   7.23%  precision:  89.38%
loss: 0.0325840903  recall:   5.47%  precision:  90.11%  [  201/  250]
loss: 0.0327548506  recall:   5.53%  precision:  89.29%  [  202/  250]
loss: 0.0325281001  recall:   5.67%  precision:  89.23%  [  203/  250]
loss: 0.0325102750  recall:   5.74%  precision:  89.45%  [  204/  250]
loss: 0.0325410527  recall:   5.71%  precision:  89.24%  [  205/  250]
loss: 0.0325347375  recall:   5.57%  precision:  89.52%  [  206/  250]
loss: 0.0325474385  recall:   5.63%  precision:  89.36%  [  207/  250]
loss: 0.0325454920  recall:   5.76%  precision:  89.16%  [  208/  250]
loss: 0.0324400946  recall:   5.68%  precision:  89.39%  [  209/  250]
loss: 0.0324695879  recall:   5.55%  precision:  89.27%  [  210/  250]
loss: 0.0324619493  recall:   5.93%  precision:  89.36%  [  211/  250]
loss: 0.0324557588  recall:   5.69%  precision:  89.65%  [  212/  250]
loss: 0.0324669882  recall:   5.56%  precision:  89.50%  [  213/  250]
loss: 0.0323815261  recall:   5.56%  precision:  88.98%  [  214/  250]
loss: 0.0325938839  recall:   5.65%  precision:  88.92%  [  215/  250]
loss: 0.0323971622  recall:   5.65%  precision:  89.55%  [  216/  250]
loss: 0.0323681720  recall:   5.51%  precision:  89.05%  [  217/  250]
loss: 0.0325794971  recall:   6.05%  precision:  88.94%  [  218/  250]
loss: 0.0325648196  recall:   5.30%  precision:  89.31%  [  219/  250]
loss: 0.0323754983  recall:   5.27%  precision:  89.84%  [  220/  250]
loss: 0.0323136946  recall:   5.78%  precision:  89.29%  [  221/  250]
loss: 0.0324129973  recall:   5.62%  precision:  89.58%  [  222/  250]
loss: 0.0323481547  recall:   5.69%  precision:  89.58%  [  223/  250]
loss: 0.0323503098  recall:   5.60%  precision:  89.66%  [  224/  250]
loss: 0.0324021840  recall:   5.71%  precision:  89.60%  [  225/  250]
loss: 0.0323286843  recall:   5.45%  precision:  89.41%  [  226/  250]
loss: 0.0322267832  recall:   5.87%  precision:  89.47%  [  227/  250]
loss: 0.0323432526  recall:   5.67%  precision:  89.23%  [  228/  250]
loss: 0.0323343216  recall:   5.87%  precision:  89.86%  [  229/  250]
loss: 0.0322098546  recall:   5.60%  precision:  89.24%  [  230/  250]
loss: 0.0321727810  recall:   5.72%  precision:  89.52%  [  231/  250]
loss: 0.0323275345  recall:   5.37%  precision:  89.36%  [  232/  250]
loss: 0.0323153769  recall:   5.92%  precision:  89.32%  [  233/  250]
loss: 0.0322613090  recall:   5.70%  precision:  89.36%  [  234/  250]
loss: 0.0321987941  recall:   5.84%  precision:  89.52%  [  235/  250]
loss: 0.0322172120  recall:   5.51%  precision:  89.49%  [  236/  250]
loss: 0.0323564439  recall:   5.72%  precision:  88.89%  [  237/  250]
loss: 0.0321830619  recall:   5.80%  precision:  89.78%  [  238/  250]
loss: 0.0322627610  recall:   5.48%  precision:  89.57%  [  239/  250]
loss: 0.0321786603  recall:   6.06%  precision:  89.26%  [  240/  250]
loss: 0.0321641018  recall:   5.66%  precision:  89.47%  [  241/  250]
loss: 0.0321762650  recall:   5.39%  precision:  89.75%  [  242/  250]
loss: 0.0321674388  recall:   5.98%  precision:  89.40%  [  243/  250]
loss: 0.0322191315  recall:   5.67%  precision:  89.19%  [  244/  250]
loss: 0.0322470390  recall:   5.96%  precision:  89.35%  [  245/  250]
loss: 0.0321288784  recall:   5.41%  precision:  89.47%  [  246/  250]
loss: 0.0321211189  recall:   5.88%  precision:  88.87%  [  247/  250]
loss: 0.0321038530  recall:   5.75%  precision:  89.49%  [  248/  250]
loss: 0.0321547305  recall:   5.45%  precision:  89.29%  [  249/  250]
loss: 0.0321418846  recall:   5.58%  precision:  89.33%  [  250/  250]
####  eval set loss: 0.0270962002  recall:   8.74%  precision:  88.23%

-------------------------------------------------------------

@@@@  pication
Model weights: 31985
loss: 0.6786161538  recall:   0.00%  precision:   0.00%  [    1/  250]
loss: 0.5186635307  recall:   0.00%  precision:   0.00%  [    2/  250]
loss: 0.2452636621  recall:   0.00%  precision:   0.00%  [    3/  250]
loss: 0.0991934743  recall:   0.00%  precision:   0.00%  [    4/  250]
loss: 0.0481137486  recall:   0.19%  precision:   1.13%  [    5/  250]
loss: 0.0326418754  recall:   0.51%  precision:   7.47%  [    6/  250]
loss: 0.0264948129  recall:   0.89%  precision:  18.55%  [    7/  250]
loss: 0.0234019983  recall:   0.61%  precision:  22.57%  [    8/  250]
loss: 0.0216207442  recall:   0.71%  precision:  26.70%  [    9/  250]
loss: 0.0204108140  recall:   0.81%  precision:  30.04%  [   10/  250]
loss: 0.0195421412  recall:   0.76%  precision:  44.06%  [   11/  250]
loss: 0.0188931242  recall:   0.79%  precision:  44.90%  [   12/  250]
loss: 0.0183606438  recall:   0.83%  precision:  55.65%  [   13/  250]
loss: 0.0178476550  recall:   0.85%  precision:  53.79%  [   14/  250]
loss: 0.0174305191  recall:   0.87%  precision:  64.86%  [   15/  250]
loss: 0.0171136217  recall:   0.96%  precision:  62.02%  [   16/  250]
loss: 0.0167673278  recall:   1.01%  precision:  64.62%  [   17/  250]
loss: 0.0164869099  recall:   0.96%  precision:  67.80%  [   18/  250]
loss: 0.0162430317  recall:   1.31%  precision:  58.60%  [   19/  250]
loss: 0.0160497313  recall:   1.31%  precision:  65.66%  [   20/  250]
loss: 0.0157927805  recall:   1.13%  precision:  59.12%  [   21/  250]
loss: 0.0155635815  recall:   1.50%  precision:  70.62%  [   22/  250]
loss: 0.0154260338  recall:   1.41%  precision:  62.23%  [   23/  250]
loss: 0.0152316380  recall:   1.54%  precision:  74.42%  [   24/  250]
loss: 0.0150018907  recall:   1.74%  precision:  71.78%  [   25/  250]
loss: 0.0148569255  recall:   1.65%  precision:  71.73%  [   26/  250]
loss: 0.0146903716  recall:   2.03%  precision:  69.55%  [   27/  250]
loss: 0.0146653639  recall:   1.46%  precision:  63.02%  [   28/  250]
loss: 0.0144602965  recall:   2.03%  precision:  75.45%  [   29/  250]
loss: 0.0142954703  recall:   2.36%  precision:  79.03%  [   30/  250]
loss: 0.0142201210  recall:   1.78%  precision:  71.50%  [   31/  250]
loss: 0.0140563331  recall:   2.18%  precision:  75.42%  [   32/  250]
loss: 0.0139226524  recall:   2.35%  precision:  74.43%  [   33/  250]
loss: 0.0140299154  recall:   1.50%  precision:  64.77%  [   34/  250]
loss: 0.0138026718  recall:   2.49%  precision:  74.46%  [   35/  250]
loss: 0.0136510896  recall:   1.89%  precision:  73.02%  [   36/  250]
loss: 0.0135396691  recall:   2.86%  precision:  76.53%  [   37/  250]
loss: 0.0134812068  recall:   2.51%  precision:  77.12%  [   38/  250]
loss: 0.0133467378  recall:   2.53%  precision:  79.55%  [   39/  250]
loss: 0.0136388886  recall:   1.25%  precision:  55.32%  [   40/  250]
loss: 0.0132221157  recall:   2.99%  precision:  78.55%  [   41/  250]
loss: 0.0131184827  recall:   2.81%  precision:  82.11%  [   42/  250]
loss: 0.0130568092  recall:   2.54%  precision:  81.15%  [   43/  250]
loss: 0.0129497800  recall:   2.92%  precision:  80.20%  [   44/  250]
loss: 0.0128693193  recall:   2.92%  precision:  80.46%  [   45/  250]
loss: 0.0127996463  recall:   2.77%  precision:  82.14%  [   46/  250]
loss: 0.0127182948  recall:   2.95%  precision:  81.40%  [   47/  250]
loss: 0.0128797013  recall:   1.79%  precision:  73.04%  [   48/  250]
loss: 0.0126541437  recall:   3.37%  precision:  80.00%  [   49/  250]
loss: 0.0125364250  recall:   2.80%  precision:  82.04%  [   50/  250]
####  eval set loss: 0.0133973086  recall:   1.19%  precision:  15.56%
loss: 0.0124946824  recall:   3.08%  precision:  82.05%  [   51/  250]
loss: 0.0124495770  recall:   3.03%  precision:  83.44%  [   52/  250]
loss: 0.0123785087  recall:   3.37%  precision:  82.35%  [   53/  250]
loss: 0.0123200470  recall:   2.79%  precision:  84.06%  [   54/  250]
loss: 0.0123117513  recall:   3.02%  precision:  81.49%  [   55/  250]
loss: 0.0121972975  recall:   3.48%  precision:  84.50%  [   56/  250]
loss: 0.0121675857  recall:   3.26%  precision:  84.42%  [   57/  250]
loss: 0.0121358795  recall:   3.40%  precision:  87.08%  [   58/  250]
loss: 0.0120456624  recall:   3.31%  precision:  85.40%  [   59/  250]
loss: 0.0120075284  recall:   3.36%  precision:  86.11%  [   60/  250]
loss: 0.0119711409  recall:   3.44%  precision:  86.67%  [   61/  250]
loss: 0.0119199707  recall:   3.36%  precision:  84.55%  [   62/  250]
loss: 0.0118719831  recall:   3.58%  precision:  86.38%  [   63/  250]
loss: 0.0118305522  recall:   3.50%  precision:  82.44%  [   64/  250]
loss: 0.0117890803  recall:   3.43%  precision:  84.07%  [   65/  250]
loss: 0.0117616918  recall:   3.45%  precision:  84.41%  [   66/  250]
loss: 0.0117369223  recall:   3.76%  precision:  86.46%  [   67/  250]
loss: 0.0116619649  recall:   3.60%  precision:  84.46%  [   68/  250]
loss: 0.0116705529  recall:   3.72%  precision:  82.18%  [   69/  250]
loss: 0.0115867661  recall:   3.67%  precision:  86.16%  [   70/  250]
loss: 0.0115420078  recall:   4.01%  precision:  82.43%  [   71/  250]
loss: 0.0115284140  recall:   3.40%  precision:  84.23%  [   72/  250]
loss: 0.0114779121  recall:   3.99%  precision:  86.01%  [   73/  250]
loss: 0.0114358097  recall:   3.56%  precision:  85.30%  [   74/  250]
loss: 0.0113982380  recall:   3.91%  precision:  83.12%  [   75/  250]
loss: 0.0113830646  recall:   3.52%  precision:  84.20%  [   76/  250]
loss: 0.0113414558  recall:   3.99%  precision:  84.91%  [   77/  250]
loss: 0.0113187795  recall:   3.98%  precision:  84.65%  [   78/  250]
loss: 0.0112760958  recall:   4.08%  precision:  85.18%  [   79/  250]
loss: 0.0112526903  recall:   3.84%  precision:  84.17%  [   80/  250]
loss: 0.0111973802  recall:   4.32%  precision:  81.78%  [   81/  250]
loss: 0.0112163240  recall:   3.67%  precision:  80.69%  [   82/  250]
loss: 0.0111610659  recall:   4.01%  precision:  84.73%  [   83/  250]
loss: 0.0111179627  recall:   3.61%  precision:  83.80%  [   84/  250]
loss: 0.0111180781  recall:   4.81%  precision:  83.86%  [   85/  250]
loss: 0.0110456344  recall:   3.75%  precision:  86.43%  [   86/  250]
loss: 0.0110852224  recall:   4.43%  precision:  79.14%  [   87/  250]
loss: 0.0110268830  recall:   4.02%  precision:  83.29%  [   88/  250]
loss: 0.0109680148  recall:   4.31%  precision:  84.63%  [   89/  250]
loss: 0.0109978537  recall:   4.25%  precision:  86.52%  [   90/  250]
loss: 0.0109204052  recall:   4.08%  precision:  86.70%  [   91/  250]
loss: 0.0108924647  recall:   4.35%  precision:  85.78%  [   92/  250]
loss: 0.0108898000  recall:   4.52%  precision:  83.93%  [   93/  250]
loss: 0.0108465501  recall:   4.37%  precision:  85.21%  [   94/  250]
loss: 0.0108343198  recall:   4.46%  precision:  86.08%  [   95/  250]
loss: 0.0108102457  recall:   4.32%  precision:  86.09%  [   96/  250]
loss: 0.0107966746  recall:   4.38%  precision:  84.85%  [   97/  250]
loss: 0.0107879078  recall:   4.47%  precision:  86.31%  [   98/  250]
loss: 0.0107330576  recall:   4.11%  precision:  86.58%  [   99/  250]
loss: 0.0107394440  recall:   4.57%  precision:  87.36%  [  100/  250]
####  eval set loss: 0.0107479957  recall:   1.42%  precision:  36.76%
loss: 0.0106993823  recall:   4.50%  precision:  83.86%  [  101/  250]
loss: 0.0106869894  recall:   4.38%  precision:  82.92%  [  102/  250]
loss: 0.0106471072  recall:   4.20%  precision:  84.50%  [  103/  250]
loss: 0.0106393777  recall:   4.63%  precision:  84.62%  [  104/  250]
loss: 0.0106146812  recall:   4.40%  precision:  84.92%  [  105/  250]
loss: 0.0105964514  recall:   4.15%  precision:  82.93%  [  106/  250]
loss: 0.0105649956  recall:   4.45%  precision:  83.90%  [  107/  250]
loss: 0.0105416549  recall:   4.74%  precision:  86.03%  [  108/  250]
loss: 0.0105393558  recall:   4.29%  precision:  80.04%  [  109/  250]
loss: 0.0105155400  recall:   4.80%  precision:  87.50%  [  110/  250]
loss: 0.0104829108  recall:   4.51%  precision:  84.84%  [  111/  250]
loss: 0.0104684250  recall:   4.46%  precision:  85.29%  [  112/  250]
loss: 0.0104874431  recall:   4.91%  precision:  83.10%  [  113/  250]
loss: 0.0104261269  recall:   4.29%  precision:  85.82%  [  114/  250]
loss: 0.0104324955  recall:   4.75%  precision:  87.58%  [  115/  250]
loss: 0.0104127662  recall:   4.58%  precision:  84.48%  [  116/  250]
loss: 0.0103812292  recall:   4.58%  precision:  86.39%  [  117/  250]
loss: 0.0103392716  recall:   4.78%  precision:  84.47%  [  118/  250]
loss: 0.0103328377  recall:   4.68%  precision:  84.20%  [  119/  250]
loss: 0.0103393467  recall:   4.67%  precision:  82.55%  [  120/  250]
loss: 0.0103007973  recall:   4.61%  precision:  84.73%  [  121/  250]
loss: 0.0102825788  recall:   4.82%  precision:  84.96%  [  122/  250]
loss: 0.0102873885  recall:   4.71%  precision:  85.22%  [  123/  250]
loss: 0.0102538599  recall:   4.51%  precision:  87.21%  [  124/  250]
loss: 0.0102159693  recall:   4.50%  precision:  81.66%  [  125/  250]
loss: 0.0102233844  recall:   4.91%  precision:  84.65%  [  126/  250]
loss: 0.0102186317  recall:   4.61%  precision:  83.62%  [  127/  250]
loss: 0.0101503655  recall:   4.81%  precision:  83.68%  [  128/  250]
loss: 0.0101612156  recall:   4.64%  precision:  82.48%  [  129/  250]
loss: 0.0101540491  recall:   5.14%  precision:  85.57%  [  130/  250]
loss: 0.0101741457  recall:   4.56%  precision:  84.79%  [  131/  250]
loss: 0.0101163331  recall:   5.00%  precision:  84.55%  [  132/  250]
loss: 0.0101056702  recall:   4.96%  precision:  85.30%  [  133/  250]
loss: 0.0100980457  recall:   4.32%  precision:  85.89%  [  134/  250]
loss: 0.0101087146  recall:   4.92%  precision:  80.35%  [  135/  250]
loss: 0.0100566304  recall:   4.49%  precision:  85.94%  [  136/  250]
loss: 0.0100604041  recall:   4.87%  precision:  84.91%  [  137/  250]
loss: 0.0100403138  recall:   4.97%  precision:  84.11%  [  138/  250]
loss: 0.0100614028  recall:   4.73%  precision:  84.15%  [  139/  250]
loss: 0.0099949233  recall:   4.62%  precision:  86.29%  [  140/  250]
loss: 0.0099834864  recall:   4.67%  precision:  82.91%  [  141/  250]
loss: 0.0099836311  recall:   4.86%  precision:  84.34%  [  142/  250]
loss: 0.0099474437  recall:   4.75%  precision:  85.13%  [  143/  250]
loss: 0.0099568029  recall:   4.65%  precision:  86.19%  [  144/  250]
loss: 0.0099676210  recall:   4.49%  precision:  83.07%  [  145/  250]
loss: 0.0099468564  recall:   4.98%  precision:  86.25%  [  146/  250]
loss: 0.0099044773  recall:   4.90%  precision:  83.92%  [  147/  250]
loss: 0.0098647359  recall:   4.91%  precision:  85.89%  [  148/  250]
loss: 0.0098770093  recall:   4.64%  precision:  85.40%  [  149/  250]
loss: 0.0098314746  recall:   5.14%  precision:  84.72%  [  150/  250]
####  eval set loss: 0.0098574304  recall:   1.59%  precision:  70.00%
loss: 0.0098438336  recall:   4.97%  precision:  85.15%  [  151/  250]
loss: 0.0098082089  recall:   4.56%  precision:  83.85%  [  152/  250]
loss: 0.0098331473  recall:   5.09%  precision:  83.93%  [  153/  250]
loss: 0.0098451142  recall:   5.16%  precision:  82.50%  [  154/  250]
loss: 0.0098187136  recall:   5.04%  precision:  85.16%  [  155/  250]
loss: 0.0097996532  recall:   4.52%  precision:  85.26%  [  156/  250]
loss: 0.0097818321  recall:   4.79%  precision:  84.32%  [  157/  250]
loss: 0.0097616812  recall:   5.09%  precision:  85.98%  [  158/  250]
loss: 0.0097624358  recall:   4.81%  precision:  83.16%  [  159/  250]
loss: 0.0097231370  recall:   4.74%  precision:  86.21%  [  160/  250]
loss: 0.0096851965  recall:   4.88%  precision:  86.57%  [  161/  250]
loss: 0.0097168566  recall:   4.97%  precision:  85.51%  [  162/  250]
loss: 0.0096828038  recall:   5.11%  precision:  85.69%  [  163/  250]
loss: 0.0096997926  recall:   4.99%  precision:  86.28%  [  164/  250]
loss: 0.0096583855  recall:   4.62%  precision:  83.12%  [  165/  250]
loss: 0.0096475122  recall:   5.20%  precision:  86.40%  [  166/  250]
loss: 0.0096281654  recall:   4.56%  precision:  82.57%  [  167/  250]
loss: 0.0096547113  recall:   4.85%  precision:  80.12%  [  168/  250]
loss: 0.0096398238  recall:   4.71%  precision:  84.48%  [  169/  250]
loss: 0.0095904934  recall:   5.03%  precision:  87.45%  [  170/  250]
loss: 0.0095840389  recall:   4.98%  precision:  85.71%  [  171/  250]
loss: 0.0095560960  recall:   5.27%  precision:  86.39%  [  172/  250]
loss: 0.0095238373  recall:   5.08%  precision:  86.48%  [  173/  250]
loss: 0.0095549107  recall:   4.96%  precision:  84.25%  [  174/  250]
loss: 0.0095558105  recall:   5.41%  precision:  85.23%  [  175/  250]
loss: 0.0094969263  recall:   4.70%  precision:  87.08%  [  176/  250]
loss: 0.0095459496  recall:   5.10%  precision:  84.97%  [  177/  250]
loss: 0.0094753600  recall:   4.78%  precision:  88.03%  [  178/  250]
loss: 0.0095056903  recall:   4.43%  precision:  84.60%  [  179/  250]
loss: 0.0094561636  recall:   5.22%  precision:  87.15%  [  180/  250]
loss: 0.0094522900  recall:   5.21%  precision:  87.12%  [  181/  250]
loss: 0.0094466291  recall:   4.67%  precision:  86.80%  [  182/  250]
loss: 0.0094782864  recall:   4.78%  precision:  86.49%  [  183/  250]
loss: 0.0094143745  recall:   4.98%  precision:  85.54%  [  184/  250]
loss: 0.0094320201  recall:   4.91%  precision:  85.36%  [  185/  250]
loss: 0.0094098021  recall:   5.16%  precision:  86.49%  [  186/  250]
loss: 0.0093832742  recall:   4.93%  precision:  85.95%  [  187/  250]
loss: 0.0096063057  recall:   4.43%  precision:  82.33%  [  188/  250]
loss: 0.0094006011  recall:   5.08%  precision:  86.65%  [  189/  250]
loss: 0.0093738822  recall:   5.35%  precision:  88.47%  [  190/  250]
loss: 0.0093608385  recall:   5.03%  precision:  87.27%  [  191/  250]
loss: 0.0093355975  recall:   5.17%  precision:  87.76%  [  192/  250]
loss: 0.0094012029  recall:   4.96%  precision:  85.83%  [  193/  250]
loss: 0.0093394824  recall:   4.86%  precision:  87.64%  [  194/  250]
loss: 0.0093110316  recall:   4.87%  precision:  82.99%  [  195/  250]
loss: 0.0092890305  recall:   5.09%  precision:  85.98%  [  196/  250]
loss: 0.0092916371  recall:   5.23%  precision:  85.97%  [  197/  250]
loss: 0.0092941529  recall:   4.84%  precision:  86.08%  [  198/  250]
loss: 0.0092487460  recall:   4.80%  precision:  88.08%  [  199/  250]
loss: 0.0092334363  recall:   4.64%  precision:  86.55%  [  200/  250]
####  eval set loss: 0.0091533440  recall:   1.82%  precision:  54.24%
loss: 0.0092283388  recall:   5.24%  precision:  87.20%  [  201/  250]
loss: 0.0092353365  recall:   5.27%  precision:  89.21%  [  202/  250]
loss: 0.0092405985  recall:   4.73%  precision:  87.72%  [  203/  250]
loss: 0.0092038768  recall:   4.88%  precision:  85.84%  [  204/  250]
loss: 0.0092085367  recall:   4.59%  precision:  88.02%  [  205/  250]
loss: 0.0101029785  recall:   4.16%  precision:  67.84%  [  206/  250]
loss: 0.0107079584  recall:   2.77%  precision:  79.58%  [  207/  250]
loss: 0.0099844959  recall:   3.48%  precision:  87.84%  [  208/  250]
loss: 0.0096896532  recall:   4.05%  precision:  87.08%  [  209/  250]
loss: 0.0096745074  recall:   3.37%  precision:  82.60%  [  210/  250]
loss: 0.0101533250  recall:   3.70%  precision:  71.63%  [  211/  250]
loss: 0.0096687969  recall:   3.73%  precision:  85.64%  [  212/  250]
loss: 0.0095149022  recall:   4.03%  precision:  84.81%  [  213/  250]
loss: 0.0094246226  recall:   4.21%  precision:  81.97%  [  214/  250]
loss: 0.0093703822  recall:   4.65%  precision:  84.50%  [  215/  250]
loss: 0.0093099198  recall:   4.37%  precision:  86.22%  [  216/  250]
loss: 0.0092821799  recall:   4.75%  precision:  84.04%  [  217/  250]
loss: 0.0092556217  recall:   5.22%  precision:  85.10%  [  218/  250]
loss: 0.0092437591  recall:   5.11%  precision:  83.66%  [  219/  250]
loss: 0.0092199400  recall:   5.42%  precision:  86.56%  [  220/  250]
loss: 0.0092020987  recall:   5.16%  precision:  86.32%  [  221/  250]
loss: 0.0091857882  recall:   5.45%  precision:  87.12%  [  222/  250]
loss: 0.0091653956  recall:   5.67%  precision:  83.81%  [  223/  250]
loss: 0.0091657174  recall:   5.38%  precision:  87.30%  [  224/  250]
loss: 0.0091311630  recall:   5.00%  precision:  87.39%  [  225/  250]
loss: 0.0091395875  recall:   5.34%  precision:  83.62%  [  226/  250]
loss: 0.0091096318  recall:   4.84%  precision:  86.27%  [  227/  250]
loss: 0.0090776246  recall:   5.28%  precision:  84.75%  [  228/  250]
loss: 0.0091049125  recall:   5.12%  precision:  87.30%  [  229/  250]
loss: 0.0090656720  recall:   5.30%  precision:  85.96%  [  230/  250]
loss: 0.0090524034  recall:   5.27%  precision:  86.90%  [  231/  250]
loss: 0.0090704458  recall:   4.96%  precision:  84.60%  [  232/  250]
loss: 0.0090830434  recall:   4.99%  precision:  85.92%  [  233/  250]
loss: 0.0090343700  recall:   4.87%  precision:  88.24%  [  234/  250]
loss: 0.0090060607  recall:   5.27%  precision:  87.25%  [  235/  250]
loss: 0.0089994204  recall:   5.06%  precision:  87.89%  [  236/  250]
loss: 0.0089984203  recall:   5.05%  precision:  85.89%  [  237/  250]
loss: 0.0089806379  recall:   5.23%  precision:  87.00%  [  238/  250]
loss: 0.0090085810  recall:   5.41%  precision:  86.71%  [  239/  250]
loss: 0.0089593864  recall:   4.68%  precision:  85.87%  [  240/  250]
loss: 0.0089834809  recall:   5.02%  precision:  87.79%  [  241/  250]
loss: 0.0089467042  recall:   5.16%  precision:  86.32%  [  242/  250]
loss: 0.0089817408  recall:   5.26%  precision:  87.58%  [  243/  250]
loss: 0.0089998378  recall:   4.84%  precision:  87.01%  [  244/  250]
loss: 0.0089454327  recall:   4.97%  precision:  81.94%  [  245/  250]
loss: 0.0089186077  recall:   4.69%  precision:  86.28%  [  246/  250]
loss: 0.0089361778  recall:   4.50%  precision:  85.00%  [  247/  250]
loss: 0.0089658517  recall:   5.14%  precision:  88.41%  [  248/  250]
loss: 0.0089110348  recall:   4.81%  precision:  85.29%  [  249/  250]
loss: 0.0088989584  recall:   5.16%  precision:  87.02%  [  250/  250]
####  eval set loss: 0.0088623684  recall:   1.93%  precision:  55.74%

-------------------------------------------------------------

@@@@  saltbridges
Model weights: 31985
loss: 0.6801645324  recall:   0.00%  precision:   0.00%  [    1/  250]
loss: 0.5148365359  recall:   0.00%  precision:   0.00%  [    2/  250]
loss: 0.2098083775  recall:   0.00%  precision:   0.00%  [    3/  250]
loss: 0.0853650710  recall:   0.00%  precision:   0.00%  [    4/  250]
loss: 0.0460410787  recall:   0.00%  precision:   0.00%  [    5/  250]
loss: 0.0351113878  recall:   0.00%  precision:   0.00%  [    6/  250]
loss: 0.0298107474  recall:   0.00%  precision:   0.00%  [    7/  250]
loss: 0.0264776071  recall:   0.00%  precision:   0.00%  [    8/  250]
loss: 0.0240719520  recall:   0.00%  precision:   0.00%  [    9/  250]
loss: 0.0223509091  recall:   0.00%  precision:   0.00%  [   10/  250]
loss: 0.0211017807  recall:   0.00%  precision:   0.00%  [   11/  250]
loss: 0.0201581530  recall:   0.00%  precision:   0.00%  [   12/  250]
loss: 0.0194712403  recall:   0.00%  precision:   0.00%  [   13/  250]
loss: 0.0189052198  recall:   0.00%  precision:   0.00%  [   14/  250]
loss: 0.0184368500  recall:   0.01%  precision:   0.56%  [   15/  250]
loss: 0.0179542635  recall:   0.01%  precision:   1.54%  [   16/  250]
loss: 0.0176239734  recall:   0.02%  precision:   2.86%  [   17/  250]
loss: 0.0172012750  recall:   0.07%  precision:   9.93%  [   18/  250]
loss: 0.0167988897  recall:   0.26%  precision:  28.16%  [   19/  250]
loss: 0.0165002410  recall:   0.35%  precision:  47.86%  [   20/  250]
loss: 0.0161727008  recall:   1.14%  precision:  61.89%  [   21/  250]
loss: 0.0158711344  recall:   1.77%  precision:  71.86%  [   22/  250]
loss: 0.0154742356  recall:   1.86%  precision:  74.53%  [   23/  250]
loss: 0.0151813260  recall:   2.12%  precision:  75.33%  [   24/  250]
loss: 0.0149208610  recall:   3.49%  precision:  78.30%  [   25/  250]
loss: 0.0146269154  recall:   2.91%  precision:  79.68%  [   26/  250]
loss: 0.0144896890  recall:   2.87%  precision:  73.88%  [   27/  250]
loss: 0.0142223674  recall:   3.00%  precision:  78.11%  [   28/  250]
loss: 0.0141246084  recall:   3.24%  precision:  76.46%  [   29/  250]
loss: 0.0140003208  recall:   3.30%  precision:  74.47%  [   30/  250]
loss: 0.0137667333  recall:   3.11%  precision:  76.06%  [   31/  250]
loss: 0.0137333518  recall:   3.30%  precision:  73.16%  [   32/  250]
loss: 0.0135029119  recall:   3.84%  precision:  75.03%  [   33/  250]
loss: 0.0134927919  recall:   4.03%  precision:  75.02%  [   34/  250]
loss: 0.0133159103  recall:   3.54%  precision:  78.53%  [   35/  250]
loss: 0.0131818192  recall:   4.13%  precision:  76.51%  [   36/  250]
loss: 0.0131288449  recall:   4.20%  precision:  77.27%  [   37/  250]
loss: 0.0130092648  recall:   4.62%  precision:  78.04%  [   38/  250]
loss: 0.0128957614  recall:   4.67%  precision:  78.51%  [   39/  250]
loss: 0.0129519517  recall:   5.93%  precision:  75.96%  [   40/  250]
loss: 0.0130579377  recall:   4.63%  precision:  75.80%  [   41/  250]
loss: 0.0127919222  recall:   7.40%  precision:  80.86%  [   42/  250]
loss: 0.0125863727  recall:   6.90%  precision:  83.74%  [   43/  250]
loss: 0.0125365057  recall:   7.07%  precision:  81.71%  [   44/  250]
loss: 0.0124689032  recall:   7.64%  precision:  84.68%  [   45/  250]
loss: 0.0123701631  recall:   8.95%  precision:  84.93%  [   46/  250]
loss: 0.0124617741  recall:   8.93%  precision:  83.28%  [   47/  250]
loss: 0.0122680950  recall:   8.25%  precision:  84.96%  [   48/  250]
loss: 0.0121691853  recall:  10.80%  precision:  86.78%  [   49/  250]
loss: 0.0121314780  recall:  11.26%  precision:  86.62%  [   50/  250]
####  eval set loss: 0.0178906932  recall:   7.33%  precision:  36.62%
loss: 0.0120364672  recall:  11.43%  precision:  87.77%  [   51/  250]
loss: 0.0119679905  recall:  12.05%  precision:  88.84%  [   52/  250]
loss: 0.0120176891  recall:  11.45%  precision:  88.25%  [   53/  250]
loss: 0.0118622616  recall:  13.14%  precision:  89.57%  [   54/  250]
loss: 0.0117947454  recall:  13.59%  precision:  89.23%  [   55/  250]
loss: 0.0117425731  recall:  15.03%  precision:  90.16%  [   56/  250]
loss: 0.0116865645  recall:  14.61%  precision:  90.85%  [   57/  250]
loss: 0.0119181839  recall:  16.04%  precision:  88.69%  [   58/  250]
loss: 0.0123281396  recall:   9.96%  precision:  83.52%  [   59/  250]
loss: 0.0117440434  recall:  14.13%  precision:  89.96%  [   60/  250]
loss: 0.0115674562  recall:  16.42%  precision:  93.23%  [   61/  250]
loss: 0.0115014291  recall:  17.15%  precision:  92.83%  [   62/  250]
loss: 0.0114409121  recall:  18.31%  precision:  92.59%  [   63/  250]
loss: 0.0114132330  recall:  18.04%  precision:  92.38%  [   64/  250]
loss: 0.0114432406  recall:  17.64%  precision:  92.60%  [   65/  250]
loss: 0.0113083842  recall:  18.48%  precision:  92.28%  [   66/  250]
loss: 0.0112488870  recall:  19.82%  precision:  93.39%  [   67/  250]
loss: 0.0111807687  recall:  19.81%  precision:  93.08%  [   68/  250]
loss: 0.0114234094  recall:  19.53%  precision:  90.63%  [   69/  250]
loss: 0.0111199625  recall:  19.86%  precision:  92.87%  [   70/  250]
loss: 0.0110519939  recall:  21.10%  precision:  93.29%  [   71/  250]
loss: 0.0110371020  recall:  22.34%  precision:  93.49%  [   72/  250]
loss: 0.0110045237  recall:  22.20%  precision:  92.79%  [   73/  250]
loss: 0.0109337352  recall:  22.80%  precision:  94.10%  [   74/  250]
loss: 0.0109083219  recall:  23.14%  precision:  93.22%  [   75/  250]
loss: 0.0108283942  recall:  23.78%  precision:  93.70%  [   76/  250]
loss: 0.0107542825  recall:  25.04%  precision:  94.00%  [   77/  250]
loss: 0.0107895744  recall:  24.91%  precision:  93.97%  [   78/  250]
loss: 0.0106814445  recall:  24.80%  precision:  93.76%  [   79/  250]
loss: 0.0106571332  recall:  25.34%  precision:  94.57%  [   80/  250]
loss: 0.0108222943  recall:  26.31%  precision:  93.13%  [   81/  250]
loss: 0.0106249527  recall:  27.13%  precision:  93.97%  [   82/  250]
loss: 0.0105556361  recall:  27.18%  precision:  94.15%  [   83/  250]
loss: 0.0105855170  recall:  27.74%  precision:  92.55%  [   84/  250]
loss: 0.0104822310  recall:  28.01%  precision:  94.33%  [   85/  250]
loss: 0.0104467639  recall:  27.75%  precision:  93.85%  [   86/  250]
loss: 0.0103880227  recall:  28.25%  precision:  94.75%  [   87/  250]
loss: 0.0107812336  recall:  27.82%  precision:  92.47%  [   88/  250]
loss: 0.0103869735  recall:  26.32%  precision:  94.72%  [   89/  250]
loss: 0.0103102238  recall:  29.33%  precision:  94.91%  [   90/  250]
loss: 0.0102975885  recall:  29.71%  precision:  94.61%  [   91/  250]
loss: 0.0102504462  recall:  28.82%  precision:  94.94%  [   92/  250]
loss: 0.0102301805  recall:  30.59%  precision:  94.45%  [   93/  250]
loss: 0.0102048535  recall:  30.19%  precision:  95.00%  [   94/  250]
loss: 0.0101739368  recall:  29.89%  precision:  94.16%  [   95/  250]
loss: 0.0102837380  recall:  29.97%  precision:  93.95%  [   96/  250]
loss: 0.0100792283  recall:  30.40%  precision:  95.22%  [   97/  250]
loss: 0.0100633183  recall:  30.71%  precision:  95.05%  [   98/  250]
loss: 0.0100417995  recall:  31.94%  precision:  94.70%  [   99/  250]
loss: 0.0107849584  recall:  32.40%  precision:  89.68%  [  100/  250]
####  eval set loss: 0.0840579797  recall:   7.46%  precision:  15.03%
loss: 0.0138386977  recall:  23.49%  precision:  77.76%  [  101/  250]
loss: 0.0106937222  recall:  21.00%  precision:  93.74%  [  102/  250]
loss: 0.0105225631  recall:  25.30%  precision:  95.72%  [  103/  250]
loss: 0.0103169519  recall:  27.52%  precision:  95.18%  [  104/  250]
loss: 0.0102542103  recall:  29.46%  precision:  95.53%  [  105/  250]
loss: 0.0102112047  recall:  29.36%  precision:  95.31%  [  106/  250]
loss: 0.0101061591  recall:  30.92%  precision:  95.78%  [  107/  250]
loss: 0.0101493462  recall:  31.40%  precision:  94.85%  [  108/  250]
loss: 0.0102305890  recall:  29.34%  precision:  94.85%  [  109/  250]
loss: 0.0099984864  recall:  31.14%  precision:  95.36%  [  110/  250]
loss: 0.0099538440  recall:  31.56%  precision:  95.57%  [  111/  250]
loss: 0.0099179476  recall:  32.81%  precision:  95.09%  [  112/  250]
loss: 0.0099002333  recall:  32.54%  precision:  95.20%  [  113/  250]
loss: 0.0098671561  recall:  32.76%  precision:  95.64%  [  114/  250]
loss: 0.0098206702  recall:  33.02%  precision:  95.38%  [  115/  250]
loss: 0.0098031739  recall:  34.26%  precision:  95.57%  [  116/  250]
loss: 0.0097871418  recall:  33.75%  precision:  95.45%  [  117/  250]
loss: 0.0097299080  recall:  34.01%  precision:  95.61%  [  118/  250]
loss: 0.0097356197  recall:  33.76%  precision:  95.45%  [  119/  250]
loss: 0.0096986165  recall:  34.14%  precision:  95.18%  [  120/  250]
loss: 0.0096987668  recall:  34.88%  precision:  95.67%  [  121/  250]
loss: 0.0096679684  recall:  35.04%  precision:  95.19%  [  122/  250]
loss: 0.0096345164  recall:  34.73%  precision:  95.05%  [  123/  250]
loss: 0.0095953804  recall:  35.40%  precision:  95.67%  [  124/  250]
loss: 0.0095925951  recall:  35.00%  precision:  95.26%  [  125/  250]
loss: 0.0095597221  recall:  35.74%  precision:  95.71%  [  126/  250]
loss: 0.0096574933  recall:  35.58%  precision:  94.84%  [  127/  250]
loss: 0.0095547943  recall:  36.22%  precision:  95.35%  [  128/  250]
loss: 0.0095118082  recall:  36.29%  precision:  95.54%  [  129/  250]
loss: 0.0095316121  recall:  35.62%  precision:  95.25%  [  130/  250]
loss: 0.0094896571  recall:  37.15%  precision:  95.26%  [  131/  250]
loss: 0.0094507723  recall:  36.55%  precision:  95.68%  [  132/  250]
loss: 0.0094381115  recall:  36.64%  precision:  95.37%  [  133/  250]
loss: 0.0094988205  recall:  37.15%  precision:  95.03%  [  134/  250]
loss: 0.0094274889  recall:  36.56%  precision:  95.34%  [  135/  250]
loss: 0.0093913317  recall:  37.54%  precision:  95.57%  [  136/  250]
loss: 0.0093908191  recall:  37.43%  precision:  95.54%  [  137/  250]
loss: 0.0093750133  recall:  37.52%  precision:  95.24%  [  138/  250]
loss: 0.0093702129  recall:  37.10%  precision:  95.17%  [  139/  250]
loss: 0.0093461966  recall:  38.33%  precision:  95.38%  [  140/  250]
loss: 0.0093014120  recall:  38.09%  precision:  95.59%  [  141/  250]
loss: 0.0093058772  recall:  37.74%  precision:  95.54%  [  142/  250]
loss: 0.0093225757  recall:  38.18%  precision:  95.37%  [  143/  250]
loss: 0.0092874222  recall:  38.23%  precision:  95.23%  [  144/  250]
loss: 0.0092495898  recall:  38.63%  precision:  95.39%  [  145/  250]
loss: 0.0092417327  recall:  38.61%  precision:  95.23%  [  146/  250]
loss: 0.0092487210  recall:  39.22%  precision:  95.30%  [  147/  250]
loss: 0.0092133069  recall:  38.15%  precision:  95.49%  [  148/  250]
loss: 0.0091940344  recall:  39.15%  precision:  95.41%  [  149/  250]
loss: 0.0091858654  recall:  39.05%  precision:  95.21%  [  150/  250]
####  eval set loss: 0.0152655023  recall:  42.28%  precision:  83.70%
loss: 0.0091645566  recall:  38.91%  precision:  95.55%  [  151/  250]
loss: 0.0091569936  recall:  38.93%  precision:  95.57%  [  152/  250]
loss: 0.0091241382  recall:  39.58%  precision:  95.79%  [  153/  250]
loss: 0.0091120836  recall:  39.67%  precision:  95.51%  [  154/  250]
loss: 0.0091011553  recall:  39.59%  precision:  95.61%  [  155/  250]
loss: 0.0091048371  recall:  39.61%  precision:  95.59%  [  156/  250]
loss: 0.0090728671  recall:  39.79%  precision:  95.74%  [  157/  250]
loss: 0.0090636411  recall:  39.86%  precision:  95.30%  [  158/  250]
loss: 0.0090777934  recall:  40.32%  precision:  95.28%  [  159/  250]
loss: 0.0125281545  recall:  35.41%  precision:  84.61%  [  160/  250]
loss: 0.0108628255  recall:  24.97%  precision:  91.86%  [  161/  250]
loss: 0.0099234205  recall:  27.48%  precision:  95.80%  [  162/  250]
loss: 0.0094462596  recall:  28.84%  precision:  96.53%  [  163/  250]
loss: 0.0092991338  recall:  32.57%  precision:  96.27%  [  164/  250]
loss: 0.0092108908  recall:  35.69%  precision:  96.08%  [  165/  250]
loss: 0.0091573238  recall:  37.11%  precision:  96.13%  [  166/  250]
loss: 0.0091269725  recall:  37.40%  precision:  95.89%  [  167/  250]
loss: 0.0091125250  recall:  37.92%  precision:  95.41%  [  168/  250]
loss: 0.0090641358  recall:  39.34%  precision:  96.00%  [  169/  250]
loss: 0.0090298415  recall:  39.10%  precision:  95.93%  [  170/  250]
loss: 0.0090001116  recall:  39.73%  precision:  95.88%  [  171/  250]
loss: 0.0089870817  recall:  40.12%  precision:  95.82%  [  172/  250]
loss: 0.0089712548  recall:  40.76%  precision:  95.75%  [  173/  250]
loss: 0.0089680264  recall:  40.23%  precision:  95.75%  [  174/  250]
loss: 0.0089457154  recall:  40.79%  precision:  95.54%  [  175/  250]
loss: 0.0089420707  recall:  40.58%  precision:  96.07%  [  176/  250]
loss: 0.0089291355  recall:  41.33%  precision:  95.81%  [  177/  250]
loss: 0.0088999789  recall:  40.60%  precision:  96.06%  [  178/  250]
loss: 0.0089146038  recall:  41.52%  precision:  95.21%  [  179/  250]
loss: 0.0088730420  recall:  41.57%  precision:  95.77%  [  180/  250]
loss: 0.0088624060  recall:  41.61%  precision:  95.83%  [  181/  250]
loss: 0.0088612542  recall:  41.55%  precision:  95.86%  [  182/  250]
loss: 0.0088341594  recall:  41.72%  precision:  95.97%  [  183/  250]
loss: 0.0088168825  recall:  42.33%  precision:  95.73%  [  184/  250]
loss: 0.0088136123  recall:  41.85%  precision:  96.07%  [  185/  250]
loss: 0.0088026833  recall:  42.39%  precision:  95.69%  [  186/  250]
loss: 0.0088006247  recall:  42.56%  precision:  95.98%  [  187/  250]
loss: 0.0087676777  recall:  42.20%  precision:  95.99%  [  188/  250]
loss: 0.0087603100  recall:  42.40%  precision:  95.86%  [  189/  250]
loss: 0.0087427499  recall:  43.62%  precision:  95.69%  [  190/  250]
loss: 0.0087290802  recall:  42.72%  precision:  95.86%  [  191/  250]
loss: 0.0087143758  recall:  43.27%  precision:  95.97%  [  192/  250]
loss: 0.0087145628  recall:  42.71%  precision:  96.05%  [  193/  250]
loss: 0.0087031176  recall:  43.56%  precision:  95.60%  [  194/  250]
loss: 0.0086896759  recall:  43.64%  precision:  96.05%  [  195/  250]
loss: 0.0086691463  recall:  43.36%  precision:  96.03%  [  196/  250]
loss: 0.0086735302  recall:  43.33%  precision:  95.70%  [  197/  250]
loss: 0.0086618494  recall:  43.68%  precision:  95.98%  [  198/  250]
loss: 0.0086502899  recall:  43.64%  precision:  95.73%  [  199/  250]
loss: 0.0086582445  recall:  43.38%  precision:  95.89%  [  200/  250]
####  eval set loss: 0.0353598620  recall:  49.27%  precision:  76.96%
loss: 0.0086189070  recall:  44.17%  precision:  96.11%  [  201/  250]
loss: 0.0086203710  recall:  43.70%  precision:  96.02%  [  202/  250]
loss: 0.0086154335  recall:  44.08%  precision:  95.82%  [  203/  250]
loss: 0.0085853368  recall:  44.21%  precision:  96.02%  [  204/  250]
loss: 0.0085708668  recall:  44.58%  precision:  95.95%  [  205/  250]
loss: 0.0085666279  recall:  44.29%  precision:  95.85%  [  206/  250]
loss: 0.0085688028  recall:  44.45%  precision:  96.16%  [  207/  250]
loss: 0.0085504174  recall:  44.50%  precision:  95.83%  [  208/  250]
loss: 0.0085310721  recall:  44.76%  precision:  96.07%  [  209/  250]
loss: 0.0085371455  recall:  45.04%  precision:  95.97%  [  210/  250]
loss: 0.0085107131  recall:  44.24%  precision:  96.05%  [  211/  250]
loss: 0.0085150576  recall:  45.22%  precision:  96.07%  [  212/  250]
loss: 0.0085214439  recall:  45.03%  precision:  95.62%  [  213/  250]
loss: 0.0085100742  recall:  44.92%  precision:  95.75%  [  214/  250]
loss: 0.0085085838  recall:  45.00%  precision:  95.68%  [  215/  250]
loss: 0.0084732505  recall:  45.22%  precision:  95.92%  [  216/  250]
loss: 0.0085034141  recall:  45.34%  precision:  95.57%  [  217/  250]
loss: 0.0084446428  recall:  45.43%  precision:  96.25%  [  218/  250]
loss: 0.0084468465  recall:  45.57%  precision:  95.99%  [  219/  250]
loss: 0.0084481001  recall:  45.45%  precision:  96.02%  [  220/  250]
loss: 0.0084219337  recall:  46.11%  precision:  95.99%  [  221/  250]
loss: 0.0084376538  recall:  45.97%  precision:  95.99%  [  222/  250]
loss: 0.0084344603  recall:  45.97%  precision:  95.96%  [  223/  250]
loss: 0.0083976880  recall:  45.74%  precision:  96.15%  [  224/  250]
loss: 0.0084197079  recall:  45.92%  precision:  95.48%  [  225/  250]
loss: 0.0083802851  recall:  46.37%  precision:  96.24%  [  226/  250]
loss: 0.0083980414  recall:  45.79%  precision:  95.95%  [  227/  250]
loss: 0.0083910576  recall:  46.46%  precision:  95.82%  [  228/  250]
loss: 0.0083466942  recall:  46.68%  precision:  96.15%  [  229/  250]
loss: 0.0083460138  recall:  47.40%  precision:  96.39%  [  230/  250]
loss: 0.0083444000  recall:  46.07%  precision:  95.90%  [  231/  250]
loss: 0.0083650205  recall:  46.89%  precision:  95.76%  [  232/  250]
loss: 0.0083508379  recall:  46.75%  precision:  95.81%  [  233/  250]
loss: 0.0083334359  recall:  46.27%  precision:  96.23%  [  234/  250]
loss: 0.0083398394  recall:  47.32%  precision:  95.86%  [  235/  250]
loss: 0.0083293850  recall:  46.38%  precision:  95.64%  [  236/  250]
loss: 0.0083110922  recall:  46.83%  precision:  96.02%  [  237/  250]
loss: 0.0083184193  recall:  46.89%  precision:  95.76%  [  238/  250]
loss: 0.0083053955  recall:  46.98%  precision:  95.77%  [  239/  250]
loss: 0.0082772838  recall:  47.27%  precision:  96.25%  [  240/  250]
loss: 0.0082990250  recall:  47.96%  precision:  95.74%  [  241/  250]
loss: 0.0082829880  recall:  47.22%  precision:  96.07%  [  242/  250]
loss: 0.0082494097  recall:  47.01%  precision:  96.31%  [  243/  250]
loss: 0.0082738141  recall:  47.71%  precision:  95.78%  [  244/  250]
loss: 0.0082409636  recall:  47.14%  precision:  96.25%  [  245/  250]
loss: 0.0082397682  recall:  47.58%  precision:  96.10%  [  246/  250]
loss: 0.0082177372  recall:  47.82%  precision:  96.39%  [  247/  250]
loss: 0.0082045345  recall:  47.95%  precision:  96.18%  [  248/  250]
loss: 0.0081948600  recall:  48.50%  precision:  96.26%  [  249/  250]
loss: 0.0082493285  recall:  47.74%  precision:  95.62%  [  250/  250]
####  eval set loss: 0.0488517518  recall:  54.12%  precision:  77.95%

-------------------------------------------------------------

@@@@  halogenbond
Model weights: 31985
loss: 0.6816968712  recall:   0.00%  precision:   0.00%  [    1/  250]
loss: 0.5674528860  recall:   0.00%  precision:   0.00%  [    2/  250]
loss: 0.2474249329  recall:   0.00%  precision:   0.00%  [    3/  250]
loss: 0.1420068381  recall:   0.00%  precision:   0.00%  [    4/  250]
loss: 0.0621981084  recall:   0.00%  precision:   0.00%  [    5/  250]
loss: 0.0284880714  recall:   0.00%  precision:   0.00%  [    6/  250]
loss: 0.0201839366  recall:   0.00%  precision:   0.00%  [    7/  250]
loss: 0.0170641431  recall:   0.00%  precision:   0.00%  [    8/  250]
loss: 0.0154598690  recall:   0.00%  precision:   0.00%  [    9/  250]
loss: 0.0144296131  recall:   0.00%  precision:   0.00%  [   10/  250]
loss: 0.0137347578  recall:   0.00%  precision:   0.00%  [   11/  250]
loss: 0.0131798185  recall:   0.00%  precision:   0.00%  [   12/  250]
loss: 0.0127290022  recall:   0.00%  precision:   0.00%  [   13/  250]
loss: 0.0123599279  recall:   0.00%  precision:   0.00%  [   14/  250]
loss: 0.0120106189  recall:   0.00%  precision:   0.00%  [   15/  250]
loss: 0.0117059693  recall:   0.00%  precision:   0.00%  [   16/  250]
loss: 0.0114556691  recall:   0.00%  precision:   0.00%  [   17/  250]
loss: 0.0112138850  recall:   0.00%  precision:   0.00%  [   18/  250]
loss: 0.0110102522  recall:   0.00%  precision:   0.00%  [   19/  250]
loss: 0.0108347073  recall:   0.00%  precision:   0.00%  [   20/  250]
loss: 0.0106811658  recall:   0.00%  precision:   0.00%  [   21/  250]
loss: 0.0105261967  recall:   0.00%  precision:   0.00%  [   22/  250]
loss: 0.0103825755  recall:   0.00%  precision:   0.00%  [   23/  250]
loss: 0.0102542457  recall:   0.00%  precision:   0.00%  [   24/  250]
loss: 0.0101142039  recall:   0.00%  precision:   0.00%  [   25/  250]
loss: 0.0100058926  recall:   0.00%  precision:   0.00%  [   26/  250]
loss: 0.0098875859  recall:   0.00%  precision:   0.00%  [   27/  250]
loss: 0.0097843706  recall:   0.00%  precision:   0.00%  [   28/  250]
loss: 0.0096922395  recall:   0.00%  precision:   0.00%  [   29/  250]
loss: 0.0096222647  recall:   0.00%  precision:   0.00%  [   30/  250]
loss: 0.0095257230  recall:   0.00%  precision:   0.00%  [   31/  250]
loss: 0.0094100479  recall:   0.00%  precision:   0.00%  [   32/  250]
loss: 0.0093121972  recall:   0.00%  precision:   0.00%  [   33/  250]
loss: 0.0092657325  recall:   0.00%  precision:   0.00%  [   34/  250]
loss: 0.0094863013  recall:   0.00%  precision:   0.00%  [   35/  250]
loss: 0.0091977794  recall:   0.00%  precision:   0.00%  [   36/  250]
loss: 0.0090962454  recall:   0.00%  precision:   0.00%  [   37/  250]
loss: 0.0090009868  recall:   0.00%  precision:   0.00%  [   38/  250]
loss: 0.0089141779  recall:   0.00%  precision:   0.00%  [   39/  250]
loss: 0.0088382666  recall:   0.00%  precision:   0.00%  [   40/  250]
loss: 0.0087737008  recall:   0.00%  precision:   0.00%  [   41/  250]
loss: 0.0087021165  recall:   0.00%  precision:   0.00%  [   42/  250]
loss: 0.0086357560  recall:   0.00%  precision:   0.00%  [   43/  250]
loss: 0.0085741589  recall:   0.00%  precision:   0.00%  [   44/  250]
loss: 0.0085184661  recall:   0.00%  precision:   0.00%  [   45/  250]
loss: 0.0084643212  recall:   0.00%  precision:   0.00%  [   46/  250]
loss: 0.0084117598  recall:   0.00%  precision:   0.00%  [   47/  250]
loss: 0.0083606723  recall:   0.00%  precision:   0.00%  [   48/  250]
loss: 0.0083068881  recall:   0.00%  precision:   0.00%  [   49/  250]
loss: 0.0083514553  recall:   0.00%  precision:   0.00%  [   50/  250]
####  eval set loss: 0.0088962711  recall:   0.00%  precision:   0.00%
loss: 0.0090149868  recall:   0.00%  precision:   0.00%  [   51/  250]
loss: 0.0083671606  recall:   0.00%  precision:   0.00%  [   52/  250]
loss: 0.0082610027  recall:   0.00%  precision:   0.00%  [   53/  250]
loss: 0.0081851867  recall:   0.00%  precision:   0.00%  [   54/  250]
loss: 0.0081361279  recall:   0.00%  precision:   0.00%  [   55/  250]
loss: 0.0080736593  recall:   0.00%  precision:   0.00%  [   56/  250]
loss: 0.0080371275  recall:   0.00%  precision:   0.00%  [   57/  250]
loss: 0.0079901841  recall:   0.00%  precision:   0.00%  [   58/  250]
loss: 0.0079311661  recall:   0.00%  precision:   0.00%  [   59/  250]
loss: 0.0078911386  recall:   0.00%  precision:   0.00%  [   60/  250]
loss: 0.0078446409  recall:   0.00%  precision:   0.00%  [   61/  250]
loss: 0.0078071436  recall:   0.00%  precision:   0.00%  [   62/  250]
loss: 0.0077913008  recall:   0.00%  precision:   0.00%  [   63/  250]
loss: 0.0077525079  recall:   0.00%  precision:   0.00%  [   64/  250]
loss: 0.0076997395  recall:   0.00%  precision:   0.00%  [   65/  250]
loss: 0.0076762040  recall:   0.00%  precision:   0.00%  [   66/  250]
loss: 0.0076314010  recall:   0.00%  precision:   0.00%  [   67/  250]
loss: 0.0076084847  recall:   0.00%  precision:   0.00%  [   68/  250]
loss: 0.0075806813  recall:   0.00%  precision:   0.00%  [   69/  250]
loss: 0.0075278638  recall:   0.00%  precision:   0.00%  [   70/  250]
loss: 0.0074911180  recall:   0.00%  precision:   0.00%  [   71/  250]
loss: 0.0074648695  recall:   0.18%  precision:  12.50%  [   72/  250]
loss: 0.0074530746  recall:   0.00%  precision:   0.00%  [   73/  250]
loss: 0.0074044827  recall:   0.18%  precision:  14.29%  [   74/  250]
loss: 0.0073687900  recall:   0.18%  precision:  14.29%  [   75/  250]
loss: 0.0073445576  recall:   0.18%  precision:  10.00%  [   76/  250]
loss: 0.0073291944  recall:   0.00%  precision:   0.00%  [   77/  250]
loss: 0.0072903575  recall:   0.18%  precision:  25.00%  [   78/  250]
loss: 0.0072604970  recall:   0.18%  precision:  11.11%  [   79/  250]
loss: 0.0072326472  recall:   0.18%  precision:  14.29%  [   80/  250]
loss: 0.0072042716  recall:   0.18%  precision:  11.11%  [   81/  250]
loss: 0.0071870965  recall:   0.18%  precision:  11.11%  [   82/  250]
loss: 0.0071587519  recall:   0.18%  precision:  12.50%  [   83/  250]
loss: 0.0071236013  recall:   0.18%  precision:  11.11%  [   84/  250]
loss: 0.0071043733  recall:   0.18%  precision:  12.50%  [   85/  250]
loss: 0.0070807318  recall:   0.18%  precision:  11.11%  [   86/  250]
loss: 0.0071120753  recall:   0.00%  precision:   0.00%  [   87/  250]
loss: 0.0070388079  recall:   0.18%  precision:   9.09%  [   88/  250]
loss: 0.0070113395  recall:   0.18%  precision:  14.29%  [   89/  250]
loss: 0.0069917037  recall:   0.18%  precision:  14.29%  [   90/  250]
loss: 0.0069656956  recall:   0.55%  precision:  23.08%  [   91/  250]
loss: 0.0069421435  recall:   0.37%  precision:  18.18%  [   92/  250]
loss: 0.0069219773  recall:   0.18%  precision:  10.00%  [   93/  250]
loss: 0.0069279913  recall:   0.18%  precision:   7.69%  [   94/  250]
loss: 0.0069191539  recall:   0.37%  precision:  33.33%  [   95/  250]
loss: 0.0068781091  recall:   0.92%  precision:  41.67%  [   96/  250]
loss: 0.0068516840  recall:   1.29%  precision:  35.00%  [   97/  250]
loss: 0.0068277956  recall:   1.10%  precision:  37.50%  [   98/  250]
loss: 0.0068145456  recall:   1.10%  precision:  33.33%  [   99/  250]
loss: 0.0067912059  recall:   1.10%  precision:  27.27%  [  100/  250]
####  eval set loss: 0.0075264250  recall:   0.52%  precision:   1.14%
loss: 0.0067725304  recall:   0.92%  precision:  29.41%  [  101/  250]
loss: 0.0067511065  recall:   1.65%  precision:  40.91%  [  102/  250]
loss: 0.0127186302  recall:   0.74%  precision:   0.69%  [  103/  250]
loss: 0.0071190264  recall:   0.00%  precision:   0.00%  [  104/  250]
loss: 0.0070260577  recall:   0.00%  precision:   0.00%  [  105/  250]
loss: 0.0070065038  recall:   0.18%  precision:  20.00%  [  106/  250]
loss: 0.0069827332  recall:   0.37%  precision:  33.33%  [  107/  250]
loss: 0.0069661317  recall:   0.55%  precision:  37.50%  [  108/  250]
loss: 0.0069421228  recall:   0.92%  precision:  45.45%  [  109/  250]
loss: 0.0069237483  recall:   1.29%  precision:  53.85%  [  110/  250]
loss: 0.0069127943  recall:   1.47%  precision:  53.33%  [  111/  250]
loss: 0.0068907791  recall:   1.47%  precision:  50.00%  [  112/  250]
loss: 0.0068753916  recall:   1.47%  precision:  53.33%  [  113/  250]
loss: 0.0068618735  recall:   1.47%  precision:  53.33%  [  114/  250]
loss: 0.0068405614  recall:   1.47%  precision:  50.00%  [  115/  250]
loss: 0.0068275342  recall:   1.47%  precision:  50.00%  [  116/  250]
loss: 0.0068127464  recall:   1.47%  precision:  50.00%  [  117/  250]
loss: 0.0067948338  recall:   1.47%  precision:  44.44%  [  118/  250]
loss: 0.0067790955  recall:   1.47%  precision:  47.06%  [  119/  250]
loss: 0.0067670762  recall:   1.47%  precision:  50.00%  [  120/  250]
loss: 0.0067527175  recall:   1.84%  precision:  47.62%  [  121/  250]
loss: 0.0067359412  recall:   1.65%  precision:  52.94%  [  122/  250]
loss: 0.0067246685  recall:   1.47%  precision:  44.44%  [  123/  250]
loss: 0.0067094921  recall:   2.02%  precision:  55.00%  [  124/  250]
loss: 0.0066920504  recall:   2.02%  precision:  55.00%  [  125/  250]
loss: 0.0066824395  recall:   2.21%  precision:  60.00%  [  126/  250]
loss: 0.0066673457  recall:   2.57%  precision:  58.33%  [  127/  250]
loss: 0.0066584791  recall:   2.57%  precision:  60.87%  [  128/  250]
loss: 0.0066473841  recall:   2.57%  precision:  58.33%  [  129/  250]
loss: 0.0066297286  recall:   2.76%  precision:  65.22%  [  130/  250]
loss: 0.0066155486  recall:   2.76%  precision:  65.22%  [  131/  250]
loss: 0.0066043437  recall:   2.76%  precision:  57.69%  [  132/  250]
loss: 0.0065949949  recall:   3.12%  precision:  60.71%  [  133/  250]
loss: 0.0065810328  recall:   2.76%  precision:  62.50%  [  134/  250]
loss: 0.0065703288  recall:   3.31%  precision:  64.29%  [  135/  250]
loss: 0.0065582710  recall:   2.94%  precision:  64.00%  [  136/  250]
loss: 0.0065510011  recall:   3.12%  precision:  68.00%  [  137/  250]
loss: 0.0065343101  recall:   3.12%  precision:  60.71%  [  138/  250]
loss: 0.0065208951  recall:   3.31%  precision:  60.00%  [  139/  250]
loss: 0.0065121303  recall:   3.12%  precision:  60.71%  [  140/  250]
loss: 0.0065007970  recall:   3.12%  precision:  60.71%  [  141/  250]
loss: 0.0064880837  recall:   3.12%  precision:  60.71%  [  142/  250]
loss: 0.0064795733  recall:   3.12%  precision:  60.71%  [  143/  250]
loss: 0.0064686444  recall:   3.31%  precision:  64.29%  [  144/  250]
loss: 0.0064582671  recall:   3.12%  precision:  62.96%  [  145/  250]
loss: 0.0064430405  recall:   3.31%  precision:  62.07%  [  146/  250]
loss: 0.0064369725  recall:   3.12%  precision:  60.71%  [  147/  250]
loss: 0.0064279961  recall:   3.49%  precision:  65.52%  [  148/  250]
loss: 0.0064146555  recall:   3.12%  precision:  62.96%  [  149/  250]
loss: 0.0064069893  recall:   3.31%  precision:  64.29%  [  150/  250]
####  eval set loss: 0.0076052870  recall:   0.52%  precision:   2.04%
loss: 0.0063955715  recall:   3.49%  precision:  63.33%  [  151/  250]
loss: 0.0063877225  recall:   2.94%  precision:  64.00%  [  152/  250]
loss: 0.0063738587  recall:   3.31%  precision:  64.29%  [  153/  250]
loss: 0.0063688678  recall:   2.76%  precision:  60.00%  [  154/  250]
loss: 0.0063579656  recall:   3.12%  precision:  62.96%  [  155/  250]
loss: 0.0063467313  recall:   2.94%  precision:  61.54%  [  156/  250]
loss: 0.0063364102  recall:   3.31%  precision:  62.07%  [  157/  250]
loss: 0.0063302869  recall:   2.94%  precision:  64.00%  [  158/  250]
loss: 0.0063206013  recall:   2.76%  precision:  60.00%  [  159/  250]
loss: 0.0063112172  recall:   2.94%  precision:  61.54%  [  160/  250]
loss: 0.0063014038  recall:   3.12%  precision:  65.38%  [  161/  250]
loss: 0.0062979364  recall:   3.12%  precision:  65.38%  [  162/  250]
loss: 0.0062821774  recall:   3.31%  precision:  62.07%  [  163/  250]
loss: 0.0062748282  recall:   3.49%  precision:  67.86%  [  164/  250]
loss: 0.0062636171  recall:   2.94%  precision:  69.57%  [  165/  250]
loss: 0.0062581234  recall:   3.31%  precision:  60.00%  [  166/  250]
loss: 0.0062467464  recall:   3.12%  precision:  68.00%  [  167/  250]
loss: 0.0062361979  recall:   3.12%  precision:  73.91%  [  168/  250]
loss: 0.0062291252  recall:   3.12%  precision:  62.96%  [  169/  250]
loss: 0.0062214419  recall:   3.12%  precision:  62.96%  [  170/  250]
loss: 0.0062120125  recall:   2.76%  precision:  71.43%  [  171/  250]
loss: 0.0062076435  recall:   3.12%  precision:  68.00%  [  172/  250]
loss: 0.0061936891  recall:   3.31%  precision:  75.00%  [  173/  250]
loss: 0.0061889354  recall:   2.94%  precision:  66.67%  [  174/  250]
loss: 0.0061797035  recall:   3.12%  precision:  68.00%  [  175/  250]
loss: 0.0061705392  recall:   2.94%  precision:  72.73%  [  176/  250]
loss: 0.0061594062  recall:   3.49%  precision:  70.37%  [  177/  250]
loss: 0.0061594539  recall:   3.31%  precision:  75.00%  [  178/  250]
loss: 0.0061524258  recall:   3.49%  precision:  76.00%  [  179/  250]
loss: 0.0061392781  recall:   3.31%  precision:  85.71%  [  180/  250]
loss: 0.0061360005  recall:   3.86%  precision:  77.78%  [  181/  250]
loss: 0.0061230073  recall:   3.86%  precision:  75.00%  [  182/  250]
loss: 0.0061200400  recall:   3.86%  precision:  77.78%  [  183/  250]
loss: 0.0061129580  recall:   3.86%  precision:  72.41%  [  184/  250]
loss: 0.0061065476  recall:   3.86%  precision:  77.78%  [  185/  250]
loss: 0.0060956181  recall:   3.68%  precision:  83.33%  [  186/  250]
loss: 0.0060851112  recall:   3.49%  precision:  76.00%  [  187/  250]
loss: 0.0060804613  recall:   3.68%  precision:  80.00%  [  188/  250]
loss: 0.0060753445  recall:   3.86%  precision:  80.77%  [  189/  250]
loss: 0.0060636963  recall:   3.68%  precision:  83.33%  [  190/  250]
loss: 0.0060610094  recall:   3.86%  precision:  77.78%  [  191/  250]
loss: 0.0060511579  recall:   3.68%  precision:  83.33%  [  192/  250]
loss: 0.0060460192  recall:   3.68%  precision:  76.92%  [  193/  250]
loss: 0.0060400835  recall:   3.68%  precision:  76.92%  [  194/  250]
loss: 0.0060328335  recall:   3.68%  precision:  66.67%  [  195/  250]
loss: 0.0060251390  recall:   4.04%  precision:  81.48%  [  196/  250]
loss: 0.0060161253  recall:   3.49%  precision:  76.00%  [  197/  250]
loss: 0.0060094277  recall:   3.86%  precision:  87.50%  [  198/  250]
loss: 0.0060023894  recall:   3.86%  precision:  77.78%  [  199/  250]
loss: 0.0059979427  recall:   3.86%  precision:  72.41%  [  200/  250]
####  eval set loss: 0.0079703982  recall:   0.52%  precision:   2.94%
loss: 0.0059899949  recall:   4.04%  precision:  84.62%  [  201/  250]
loss: 0.0059835050  recall:   4.04%  precision:  73.33%  [  202/  250]
loss: 0.0059760530  recall:   4.04%  precision:  78.57%  [  203/  250]
loss: 0.0059688770  recall:   3.68%  precision:  74.07%  [  204/  250]
loss: 0.0059647169  recall:   3.86%  precision:  77.78%  [  205/  250]
loss: 0.0059587074  recall:   4.04%  precision:  78.57%  [  206/  250]
loss: 0.0059540347  recall:   4.04%  precision:  70.97%  [  207/  250]
loss: 0.0059455984  recall:   4.04%  precision:  78.57%  [  208/  250]
loss: 0.0059352999  recall:   4.23%  precision:  92.00%  [  209/  250]
loss: 0.0059327108  recall:   3.86%  precision:  72.41%  [  210/  250]
loss: 0.0059267536  recall:   4.04%  precision:  81.48%  [  211/  250]
loss: 0.0059162106  recall:   4.04%  precision:  73.33%  [  212/  250]
loss: 0.0059135103  recall:   4.41%  precision:  77.42%  [  213/  250]
loss: 0.0059098463  recall:   4.41%  precision:  80.00%  [  214/  250]
loss: 0.0058997626  recall:   4.41%  precision:  80.00%  [  215/  250]
loss: 0.0058948267  recall:   4.78%  precision:  76.47%  [  216/  250]
loss: 0.0058895095  recall:   4.41%  precision:  85.71%  [  217/  250]
loss: 0.0058849358  recall:   4.60%  precision:  78.12%  [  218/  250]
loss: 0.0058743652  recall:   4.41%  precision:  80.00%  [  219/  250]
loss: 0.0058717901  recall:   4.41%  precision:  82.76%  [  220/  250]
loss: 0.0058661457  recall:   4.96%  precision:  79.41%  [  221/  250]
loss: 0.0540968068  recall:   1.10%  precision:   0.30%  [  222/  250]
loss: 0.0081609714  recall:   0.74%  precision:   1.27%  [  223/  250]
loss: 0.0071361120  recall:   2.02%  precision:   6.92%  [  224/  250]
loss: 0.0067715950  recall:   2.21%  precision:  10.71%  [  225/  250]
loss: 0.0065800878  recall:   2.94%  precision:  16.33%  [  226/  250]
loss: 0.0064354966  recall:   3.31%  precision:  23.38%  [  227/  250]
loss: 0.0063261287  recall:   3.49%  precision:  22.62%  [  228/  250]
loss: 0.0062071647  recall:   3.68%  precision:  38.46%  [  229/  250]
loss: 0.0061739742  recall:   3.86%  precision:  72.41%  [  230/  250]
loss: 0.0061710969  recall:   3.68%  precision:  64.52%  [  231/  250]
loss: 0.0061662237  recall:   3.68%  precision:  62.50%  [  232/  250]
loss: 0.0061599625  recall:   3.86%  precision:  75.00%  [  233/  250]
loss: 0.0061539160  recall:   3.86%  precision:  72.41%  [  234/  250]
loss: 0.0061475830  recall:   4.23%  precision:  67.65%  [  235/  250]
loss: 0.0061460055  recall:   4.23%  precision:  69.70%  [  236/  250]
loss: 0.0061410004  recall:   4.23%  precision:  69.70%  [  237/  250]
loss: 0.0061353043  recall:   4.41%  precision:  63.16%  [  238/  250]
loss: 0.0061353257  recall:   4.23%  precision:  58.97%  [  239/  250]
loss: 0.0061323208  recall:   4.04%  precision:  66.67%  [  240/  250]
loss: 0.0061269352  recall:   4.78%  precision:  70.27%  [  241/  250]
loss: 0.0061230478  recall:   4.41%  precision:  63.16%  [  242/  250]
loss: 0.0061257193  recall:   4.23%  precision:  60.53%  [  243/  250]
loss: 0.0061177925  recall:   4.78%  precision:  66.67%  [  244/  250]
loss: 0.0061187778  recall:   4.96%  precision:  65.85%  [  245/  250]
loss: 0.0061099268  recall:   5.15%  precision:  63.64%  [  246/  250]
loss: 0.0061080875  recall:   5.33%  precision:  64.44%  [  247/  250]
loss: 0.0061072145  recall:   5.15%  precision:  63.64%  [  248/  250]
loss: 0.0061000131  recall:   5.15%  precision:  63.64%  [  249/  250]
loss: 0.0060967505  recall:   5.33%  precision:  67.44%  [  250/  250]
####  eval set loss: 0.0063529241  recall:   0.00%  precision:   0.00%

-------------------------------------------------------------

RESULT: 0.058132 

