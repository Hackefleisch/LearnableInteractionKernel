

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------
~/Projects/LearnableInteractionKernel »
                     iwe20@iwe20
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
---------------------------------
~/Projects/LearnableInteractionKernel (main*) » python train_conv.py --epochs 250 --node_emb_hidden_layers 0 --node_embedding_size 8 --tp_weights_hidden_layers 24 --tp_weights_act ReLU --spherical_harmonics_l 2
--batch_normalize 1 --irreps_node_scalars 8 --irreps_node_vectors 1 --irreps_node_tensors 1 --basis_density_per_A 5 --inter_tp_weights_hidden_layers 5 --radius 7.5 --batch_size 128 --lr 0.001 --abs_path ~/Projec
ts/LearnableInteractionKernel/ --data_dir pdbbind2020_reduced/ --split_file LP_PDBBind.csv --storage_path model_weights/ --num_workers 6 --conv_radius 4.5 --inter_tp_weights_act ReLU

Using cuda device.
Training size: 7366 Validation size: 1885 Test size: 4240

@@@@  hydrophobic
Model weights: 4119
loss: 0.5898291386  recall:   0.00%  precision:   0.00%  [    1/  250]
loss: 0.3503126523  recall:   0.00%  precision:   0.00%  [    2/  250]
loss: 0.1873081992  recall:   0.00%  precision:   0.00%  [    3/  250]
loss: 0.1221695930  recall:   0.00%  precision:   0.00%  [    4/  250]
loss: 0.0841819558  recall:   0.00%  precision:   0.00%  [    5/  250]
loss: 0.0635597960  recall:   0.00%  precision:   0.00%  [    6/  250]
loss: 0.0526179262  recall:   0.00%  precision:   0.00%  [    7/  250]
loss: 0.0461298588  recall:   0.00%  precision:   0.00%  [    8/  250]
loss: 0.0422837435  recall:   0.00%  precision:   0.00%  [    9/  250]
loss: 0.0399037104  recall:   0.00%  precision:   0.00%  [   10/  250]
loss: 0.0382288821  recall:   0.00%  precision:   0.00%  [   11/  250]
loss: 0.0369201947  recall:   0.00%  precision:   0.00%  [   12/  250]
loss: 0.0359086177  recall:   0.00%  precision:   0.00%  [   13/  250]
loss: 0.0349790452  recall:   0.00%  precision:   0.00%  [   14/  250]
loss: 0.0341374744  recall:   0.00%  precision:   0.00%  [   15/  250]
loss: 0.0333601215  recall:   0.00%  precision:   0.00%  [   16/  250]
loss: 0.0326399965  recall:   0.00%  precision:  33.33%  [   17/  250]
loss: 0.0319235729  recall:   0.00%  precision:  27.27%  [   18/  250]
loss: 0.0312324430  recall:   0.01%  precision:  43.75%  [   19/  250]
loss: 0.0305265132  recall:   0.03%  precision:  62.50%  [   20/  250]
loss: 0.0298317397  recall:   0.04%  precision:  57.45%  [   21/  250]
loss: 0.0291258111  recall:   0.06%  precision:  62.69%  [   22/  250]
loss: 0.0284099545  recall:   0.09%  precision:  62.89%  [   23/  250]
loss: 0.0277072555  recall:   0.12%  precision:  62.22%  [   24/  250]
loss: 0.0270017670  recall:   0.21%  precision:  66.52%  [   25/  250]
loss: 0.0263089862  recall:   0.32%  precision:  70.79%  [   26/  250]
loss: 0.0256261495  recall:   0.45%  precision:  73.23%  [   27/  250]
loss: 0.0249295551  recall:   0.61%  precision:  75.04%  [   28/  250]
loss: 0.0242637345  recall:   0.86%  precision:  78.91%  [   29/  250]
loss: 0.0236021412  recall:   1.15%  precision:  81.95%  [   30/  250]
loss: 0.0229331426  recall:   1.42%  precision:  83.36%  [   31/  250]
loss: 0.0222542583  recall:   1.89%  precision:  85.94%  [   32/  250]
loss: 0.0216214941  recall:   2.71%  precision:  88.69%  [   33/  250]
loss: 0.0209763103  recall:   3.60%  precision:  90.31%  [   34/  250]
loss: 0.0203494290  recall:   4.98%  precision:  91.71%  [   35/  250]
loss: 0.0196900132  recall:   6.84%  precision:  93.01%  [   36/  250]
loss: 0.0190079617  recall:   9.51%  precision:  93.83%  [   37/  250]
loss: 0.0184022828  recall:  12.41%  precision:  94.82%  [   38/  250]
loss: 0.0177971873  recall:  16.24%  precision:  95.46%  [   39/  250]
loss: 0.0172019813  recall:  20.51%  precision:  95.98%  [   40/  250]
loss: 0.0166573059  recall:  25.00%  precision:  96.54%  [   41/  250]
loss: 0.0161269554  recall:  29.89%  precision:  96.95%  [   42/  250]
loss: 0.0155969982  recall:  34.93%  precision:  97.37%  [   43/  250]
loss: 0.0150760176  recall:  39.86%  precision:  97.76%  [   44/  250]
loss: 0.0145696840  recall:  44.59%  precision:  98.10%  [   45/  250]
loss: 0.0140824188  recall:  49.62%  precision:  98.36%  [   46/  250]
loss: 0.0136228854  recall:  54.55%  precision:  98.62%  [   47/  250]
loss: 0.0131819960  recall:  59.02%  precision:  98.79%  [   48/  250]
loss: 0.0127521985  recall:  63.27%  precision:  98.99%  [   49/  250]
loss: 0.0123556640  recall:  67.61%  precision:  99.12%  [   50/  250]
####  eval set loss: 0.0125897169  recall:  66.03%  precision:  98.81%
loss: 0.0119878396  recall:  71.48%  precision:  99.20%  [   51/  250]
loss: 0.0116579160  recall:  74.33%  precision:  99.31%  [   52/  250]
loss: 0.0113506078  recall:  77.86%  precision:  99.37%  [   53/  250]
loss: 0.0110795183  recall:  80.46%  precision:  99.44%  [   54/  250]
loss: 0.0108203357  recall:  82.93%  precision:  99.49%  [   55/  250]
loss: 0.0106050765  recall:  84.96%  precision:  99.53%  [   56/  250]
loss: 0.0104060003  recall:  86.91%  precision:  99.54%  [   57/  250]
loss: 0.0102146797  recall:  88.51%  precision:  99.59%  [   58/  250]
loss: 0.0100637982  recall:  89.79%  precision:  99.62%  [   59/  250]
loss: 0.0099095501  recall:  91.12%  precision:  99.63%  [   60/  250]
loss: 0.0097836644  recall:  92.02%  precision:  99.65%  [   61/  250]
loss: 0.0096688530  recall:  92.87%  precision:  99.66%  [   62/  250]
loss: 0.0095546460  recall:  93.64%  precision:  99.68%  [   63/  250]
loss: 0.0094609780  recall:  94.21%  precision:  99.70%  [   64/  250]
loss: 0.0093693256  recall:  94.80%  precision:  99.71%  [   65/  250]
loss: 0.0092877246  recall:  95.24%  precision:  99.73%  [   66/  250]
loss: 0.0092151918  recall:  95.63%  precision:  99.73%  [   67/  250]
loss: 0.0091502280  recall:  95.89%  precision:  99.74%  [   68/  250]
loss: 0.0090811618  recall:  96.24%  precision:  99.74%  [   69/  250]
loss: 0.0090211306  recall:  96.52%  precision:  99.75%  [   70/  250]
loss: 0.0089658284  recall:  96.68%  precision:  99.75%  [   71/  250]
loss: 0.0089127238  recall:  96.93%  precision:  99.75%  [   72/  250]
loss: 0.0088597341  recall:  97.11%  precision:  99.75%  [   73/  250]
loss: 0.0088184513  recall:  97.28%  precision:  99.75%  [   74/  250]
loss: 0.0087733865  recall:  97.33%  precision:  99.77%  [   75/  250]
loss: 0.0087277062  recall:  97.50%  precision:  99.76%  [   76/  250]
loss: 0.0086907837  recall:  97.58%  precision:  99.77%  [   77/  250]
loss: 0.0086492420  recall:  97.72%  precision:  99.78%  [   78/  250]
loss: 0.0086089953  recall:  97.77%  precision:  99.79%  [   79/  250]
loss: 0.0085776879  recall:  97.90%  precision:  99.78%  [   80/  250]
loss: 0.0085414247  recall:  97.91%  precision:  99.79%  [   81/  250]
loss: 0.0085154140  recall:  98.03%  precision:  99.79%  [   82/  250]
loss: 0.0084825325  recall:  98.04%  precision:  99.80%  [   83/  250]
loss: 0.0084494168  recall:  98.17%  precision:  99.79%  [   84/  250]
loss: 0.0084251405  recall:  98.20%  precision:  99.80%  [   85/  250]
loss: 0.0083909892  recall:  98.30%  precision:  99.80%  [   86/  250]
loss: 0.0083619096  recall:  98.30%  precision:  99.80%  [   87/  250]
loss: 0.0083395891  recall:  98.40%  precision:  99.80%  [   88/  250]
loss: 0.0083089508  recall:  98.39%  precision:  99.80%  [   89/  250]
loss: 0.0082862223  recall:  98.46%  precision:  99.80%  [   90/  250]
loss: 0.0082599146  recall:  98.52%  precision:  99.80%  [   91/  250]
loss: 0.0082366627  recall:  98.52%  precision:  99.81%  [   92/  250]
loss: 0.0082146044  recall:  98.57%  precision:  99.81%  [   93/  250]
loss: 0.0081866807  recall:  98.55%  precision:  99.81%  [   94/  250]
loss: 0.0081708435  recall:  98.64%  precision:  99.80%  [   95/  250]
loss: 0.0081486672  recall:  98.65%  precision:  99.81%  [   96/  250]
loss: 0.0081270573  recall:  98.68%  precision:  99.81%  [   97/  250]
loss: 0.0081054678  recall:  98.72%  precision:  99.81%  [   98/  250]
loss: 0.0080834973  recall:  98.71%  precision:  99.81%  [   99/  250]
loss: 0.0080651862  recall:  98.80%  precision:  99.81%  [  100/  250]
####  eval set loss: 0.0083765291  recall:  98.49%  precision:  99.78%
loss: 0.0080440227  recall:  98.79%  precision:  99.82%  [  101/  250]
loss: 0.0080274800  recall:  98.81%  precision:  99.81%  [  102/  250]
loss: 0.0080064290  recall:  98.82%  precision:  99.81%  [  103/  250]
loss: 0.0079901779  recall:  98.86%  precision:  99.81%  [  104/  250]
loss: 0.0079696860  recall:  98.85%  precision:  99.82%  [  105/  250]
loss: 0.0079510206  recall:  98.88%  precision:  99.82%  [  106/  250]
loss: 0.0079360357  recall:  98.94%  precision:  99.82%  [  107/  250]
loss: 0.0079131993  recall:  98.89%  precision:  99.82%  [  108/  250]
loss: 0.0078995014  recall:  98.94%  precision:  99.81%  [  109/  250]
loss: 0.0078828213  recall:  98.95%  precision:  99.82%  [  110/  250]
loss: 0.0078624107  recall:  98.96%  precision:  99.82%  [  111/  250]
loss: 0.0078492940  recall:  98.98%  precision:  99.82%  [  112/  250]
loss: 0.0078308678  recall:  99.00%  precision:  99.82%  [  113/  250]
loss: 0.0078136266  recall:  99.03%  precision:  99.82%  [  114/  250]
loss: 0.0078027267  recall:  99.03%  precision:  99.82%  [  115/  250]
loss: 0.0077944080  recall:  99.03%  precision:  99.82%  [  116/  250]
loss: 0.0077714699  recall:  99.04%  precision:  99.82%  [  117/  250]
loss: 0.0077590838  recall:  99.05%  precision:  99.82%  [  118/  250]
loss: 0.0077377003  recall:  99.06%  precision:  99.82%  [  119/  250]
loss: 0.0077298088  recall:  99.05%  precision:  99.83%  [  120/  250]
loss: 0.0077138322  recall:  99.12%  precision:  99.82%  [  121/  250]
loss: 0.0076977276  recall:  99.12%  precision:  99.82%  [  122/  250]
loss: 0.0076853526  recall:  99.10%  precision:  99.82%  [  123/  250]
loss: 0.0076695669  recall:  99.14%  precision:  99.82%  [  124/  250]
loss: 0.0076592040  recall:  99.13%  precision:  99.82%  [  125/  250]
loss: 0.0076447775  recall:  99.16%  precision:  99.82%  [  126/  250]
loss: 0.0076299310  recall:  99.16%  precision:  99.83%  [  127/  250]
loss: 0.0076127510  recall:  99.19%  precision:  99.83%  [  128/  250]
loss: 0.0076041028  recall:  99.19%  precision:  99.83%  [  129/  250]
loss: 0.0075884937  recall:  99.20%  precision:  99.81%  [  130/  250]
loss: 0.0075770918  recall:  99.19%  precision:  99.83%  [  131/  250]
loss: 0.0075625760  recall:  99.22%  precision:  99.83%  [  132/  250]
loss: 0.0075556790  recall:  99.18%  precision:  99.83%  [  133/  250]
loss: 0.0075433106  recall:  99.23%  precision:  99.82%  [  134/  250]
loss: 0.0075265515  recall:  99.23%  precision:  99.83%  [  135/  250]
loss: 0.0075177568  recall:  99.22%  precision:  99.82%  [  136/  250]
loss: 0.0075056220  recall:  99.26%  precision:  99.82%  [  137/  250]
loss: 0.0074881631  recall:  99.27%  precision:  99.82%  [  138/  250]
loss: 0.0074839159  recall:  99.25%  precision:  99.83%  [  139/  250]
loss: 0.0074666849  recall:  99.27%  precision:  99.83%  [  140/  250]
loss: 0.0074653142  recall:  99.28%  precision:  99.83%  [  141/  250]
loss: 0.0074436711  recall:  99.29%  precision:  99.83%  [  142/  250]
loss: 0.0074315636  recall:  99.32%  precision:  99.82%  [  143/  250]
loss: 0.0074251661  recall:  99.28%  precision:  99.82%  [  144/  250]
loss: 0.0074138414  recall:  99.33%  precision:  99.83%  [  145/  250]
loss: 0.0074031528  recall:  99.33%  precision:  99.83%  [  146/  250]
loss: 0.0073957561  recall:  99.30%  precision:  99.82%  [  147/  250]
loss: 0.0073795048  recall:  99.31%  precision:  99.82%  [  148/  250]
loss: 0.0073700462  recall:  99.37%  precision:  99.83%  [  149/  250]
loss: 0.0073592188  recall:  99.33%  precision:  99.82%  [  150/  250]
####  eval set loss: 0.0076838986  recall:  99.24%  precision:  99.79%
loss: 0.0073402131  recall:  99.34%  precision:  99.83%  [  151/  250]
loss: 0.0073419397  recall:  99.37%  precision:  99.83%  [  152/  250]
loss: 0.0073301925  recall:  99.36%  precision:  99.83%  [  153/  250]
loss: 0.0073144617  recall:  99.37%  precision:  99.83%  [  154/  250]
loss: 0.0073089387  recall:  99.39%  precision:  99.83%  [  155/  250]
loss: 0.0072948470  recall:  99.39%  precision:  99.84%  [  156/  250]
loss: 0.0072871963  recall:  99.39%  precision:  99.83%  [  157/  250]
loss: 0.0072792829  recall:  99.39%  precision:  99.83%  [  158/  250]
loss: 0.0072701303  recall:  99.41%  precision:  99.83%  [  159/  250]
loss: 0.0072569960  recall:  99.40%  precision:  99.83%  [  160/  250]
loss: 0.0072479465  recall:  99.42%  precision:  99.83%  [  161/  250]
loss: 0.0072387526  recall:  99.41%  precision:  99.84%  [  162/  250]
loss: 0.0072354731  recall:  99.43%  precision:  99.82%  [  163/  250]
loss: 0.0072208057  recall:  99.40%  precision:  99.84%  [  164/  250]
loss: 0.0072090829  recall:  99.45%  precision:  99.83%  [  165/  250]
loss: 0.0072075451  recall:  99.42%  precision:  99.83%  [  166/  250]
loss: 0.0071941237  recall:  99.45%  precision:  99.83%  [  167/  250]
loss: 0.0071841936  recall:  99.42%  precision:  99.84%  [  168/  250]
loss: 0.0071729423  recall:  99.44%  precision:  99.83%  [  169/  250]
loss: 0.0071653225  recall:  99.45%  precision:  99.84%  [  170/  250]
loss: 0.0071638867  recall:  99.42%  precision:  99.83%  [  171/  250]
loss: 0.0071494092  recall:  99.46%  precision:  99.84%  [  172/  250]
loss: 0.0071392855  recall:  99.43%  precision:  99.83%  [  173/  250]
loss: 0.0071267654  recall:  99.45%  precision:  99.83%  [  174/  250]
loss: 0.0071225080  recall:  99.46%  precision:  99.83%  [  175/  250]
loss: 0.0071106446  recall:  99.47%  precision:  99.84%  [  176/  250]
loss: 0.0071042641  recall:  99.47%  precision:  99.84%  [  177/  250]
loss: 0.0071003277  recall:  99.46%  precision:  99.84%  [  178/  250]
loss: 0.0070846263  recall:  99.48%  precision:  99.84%  [  179/  250]
loss: 0.0070784299  recall:  99.49%  precision:  99.83%  [  180/  250]
loss: 0.0070775181  recall:  99.44%  precision:  99.84%  [  181/  250]
loss: 0.0070715725  recall:  99.45%  precision:  99.82%  [  182/  250]
loss: 0.0070524313  recall:  99.48%  precision:  99.83%  [  183/  250]
loss: 0.0070423210  recall:  99.47%  precision:  99.84%  [  184/  250]
loss: 0.0070367790  recall:  99.50%  precision:  99.84%  [  185/  250]
loss: 0.0070288363  recall:  99.49%  precision:  99.84%  [  186/  250]
loss: 0.0070221900  recall:  99.51%  precision:  99.84%  [  187/  250]
loss: 0.0070146550  recall:  99.48%  precision:  99.83%  [  188/  250]
loss: 0.0070085771  recall:  99.49%  precision:  99.84%  [  189/  250]
loss: 0.0070015778  recall:  99.51%  precision:  99.83%  [  190/  250]
loss: 0.0069944268  recall:  99.50%  precision:  99.83%  [  191/  250]
loss: 0.0069851516  recall:  99.50%  precision:  99.83%  [  192/  250]
loss: 0.0069815319  recall:  99.49%  precision:  99.84%  [  193/  250]
loss: 0.0069712531  recall:  99.50%  precision:  99.83%  [  194/  250]
loss: 0.0069563034  recall:  99.50%  precision:  99.85%  [  195/  250]
loss: 0.0069532586  recall:  99.53%  precision:  99.84%  [  196/  250]
loss: 0.0069436790  recall:  99.51%  precision:  99.84%  [  197/  250]
loss: 0.0069424709  recall:  99.52%  precision:  99.84%  [  198/  250]
loss: 0.0069318037  recall:  99.55%  precision:  99.84%  [  199/  250]
loss: 0.0069281138  recall:  99.54%  precision:  99.85%  [  200/  250]
####  eval set loss: 0.0072454372  recall:  99.59%  precision:  99.78%
loss: 0.0069207836  recall:  99.54%  precision:  99.84%  [  201/  250]
loss: 0.0069087080  recall:  99.55%  precision:  99.84%  [  202/  250]
loss: 0.0069053671  recall:  99.55%  precision:  99.84%  [  203/  250]
loss: 0.0068995191  recall:  99.53%  precision:  99.84%  [  204/  250]
loss: 0.0068904916  recall:  99.52%  precision:  99.84%  [  205/  250]
loss: 0.0068816924  recall:  99.55%  precision:  99.85%  [  206/  250]
loss: 0.0068714620  recall:  99.55%  precision:  99.84%  [  207/  250]
loss: 0.0068739631  recall:  99.55%  precision:  99.85%  [  208/  250]
loss: 0.0068737787  recall:  99.52%  precision:  99.83%  [  209/  250]
loss: 0.0068553129  recall:  99.53%  precision:  99.85%  [  210/  250]
loss: 0.0068465347  recall:  99.57%  precision:  99.85%  [  211/  250]
loss: 0.0068507492  recall:  99.50%  precision:  99.82%  [  212/  250]
loss: 0.0068351083  recall:  99.59%  precision:  99.84%  [  213/  250]
loss: 0.0068356217  recall:  99.55%  precision:  99.83%  [  214/  250]
loss: 0.0068255875  recall:  99.55%  precision:  99.83%  [  215/  250]
loss: 0.0068189071  recall:  99.58%  precision:  99.84%  [  216/  250]
loss: 0.0068103222  recall:  99.56%  precision:  99.85%  [  217/  250]
loss: 0.0068035748  recall:  99.57%  precision:  99.85%  [  218/  250]
loss: 0.0068091435  recall:  99.50%  precision:  99.84%  [  219/  250]
loss: 0.0067954622  recall:  99.55%  precision:  99.84%  [  220/  250]
loss: 0.0067813964  recall:  99.59%  precision:  99.84%  [  221/  250]
loss: 0.0067790679  recall:  99.56%  precision:  99.85%  [  222/  250]
loss: 0.0067702854  recall:  99.58%  precision:  99.84%  [  223/  250]
loss: 0.0067641728  recall:  99.56%  precision:  99.85%  [  224/  250]
loss: 0.0067559419  recall:  99.59%  precision:  99.85%  [  225/  250]
loss: 0.0067530800  recall:  99.59%  precision:  99.85%  [  226/  250]
loss: 0.0067461995  recall:  99.58%  precision:  99.84%  [  227/  250]
loss: 0.0067446607  recall:  99.59%  precision:  99.84%  [  228/  250]
loss: 0.0067327628  recall:  99.56%  precision:  99.85%  [  229/  250]
loss: 0.0067251242  recall:  99.60%  precision:  99.85%  [  230/  250]
loss: 0.0067251213  recall:  99.61%  precision:  99.85%  [  231/  250]
loss: 0.0067260199  recall:  99.55%  precision:  99.83%  [  232/  250]
loss: 0.0067110412  recall:  99.60%  precision:  99.84%  [  233/  250]
loss: 0.0067049838  recall:  99.62%  precision:  99.84%  [  234/  250]
loss: 0.0067015685  recall:  99.58%  precision:  99.85%  [  235/  250]
loss: 0.0067075814  recall:  99.63%  precision:  99.84%  [  236/  250]
loss: 0.0066986297  recall:  99.57%  precision:  99.84%  [  237/  250]
loss: 0.0066836490  recall:  99.59%  precision:  99.85%  [  238/  250]
loss: 0.0066812888  recall:  99.59%  precision:  99.84%  [  239/  250]
loss: 0.0066701355  recall:  99.62%  precision:  99.85%  [  240/  250]
loss: 0.0066696391  recall:  99.58%  precision:  99.85%  [  241/  250]
loss: 0.0066602533  recall:  99.60%  precision:  99.85%  [  242/  250]
loss: 0.0066570017  recall:  99.62%  precision:  99.85%  [  243/  250]
loss: 0.0066578304  recall:  99.60%  precision:  99.82%  [  244/  250]
loss: 0.0066449641  recall:  99.62%  precision:  99.84%  [  245/  250]
loss: 0.0066409857  recall:  99.56%  precision:  99.84%  [  246/  250]
loss: 0.0066333583  recall:  99.64%  precision:  99.85%  [  247/  250]
loss: 0.0066336973  recall:  99.60%  precision:  99.84%  [  248/  250]
loss: 0.0066222422  recall:  99.63%  precision:  99.85%  [  249/  250]
loss: 0.0066137877  recall:  99.64%  precision:  99.85%  [  250/  250]
####  eval set loss: 0.0069336312  recall:  99.67%  precision:  99.78%
@@@@  test set loss: 0.0069835979  recall:  99.68%  precision:  99.86%

-------------------------------------------------------------

@@@@  hbond
Model weights: 4119
loss: 0.6575137613  recall:   0.00%  precision:   1.92%  [    1/  250]
loss: 0.4017076207  recall:   0.01%  precision: 100.00%  [    2/  250]
loss: 0.1812254036  recall:   0.00%  precision:  16.67%  [    3/  250]
loss: 0.1194898649  recall:   0.00%  precision:  16.67%  [    4/  250]
loss: 0.0929162552  recall:   0.00%  precision:  11.11%  [    5/  250]
loss: 0.0775728878  recall:   0.00%  precision:   8.33%  [    6/  250]
loss: 0.0663775397  recall:   0.00%  precision:   7.69%  [    7/  250]
loss: 0.0572937169  recall:   0.00%  precision:   0.00%  [    8/  250]
loss: 0.0497096296  recall:   0.00%  precision:   0.00%  [    9/  250]
loss: 0.0442431365  recall:   0.00%  precision:   0.00%  [   10/  250]
loss: 0.0407317705  recall:   0.00%  precision:   0.00%  [   11/  250]
loss: 0.0380866217  recall:   0.00%  precision:   0.00%  [   12/  250]
loss: 0.0358922527  recall:   0.00%  precision:   0.00%  [   13/  250]
loss: 0.0340410556  recall:   0.00%  precision:   0.00%  [   14/  250]
loss: 0.0324415740  recall:   0.00%  precision:   0.00%  [   15/  250]
loss: 0.0310178599  recall:   0.00%  precision:   0.00%  [   16/  250]
loss: 0.0297456577  recall:   0.01%  precision:  20.00%  [   17/  250]
loss: 0.0285517829  recall:   0.01%  precision:  21.43%  [   18/  250]
loss: 0.0274390124  recall:   0.02%  precision:  38.10%  [   19/  250]
loss: 0.0264526824  recall:   0.03%  precision:  39.47%  [   20/  250]
loss: 0.0255335953  recall:   0.05%  precision:  47.92%  [   21/  250]
loss: 0.0246887069  recall:   0.07%  precision:  46.38%  [   22/  250]
loss: 0.0238654172  recall:   0.11%  precision:  51.61%  [   23/  250]
loss: 0.0231232312  recall:   0.16%  precision:  52.31%  [   24/  250]
loss: 0.0224199675  recall:   0.23%  precision:  53.23%  [   25/  250]
loss: 0.0217839616  recall:   0.30%  precision:  54.29%  [   26/  250]
loss: 0.0212138856  recall:   0.38%  precision:  56.23%  [   27/  250]
loss: 0.0207495025  recall:   0.53%  precision:  58.08%  [   28/  250]
loss: 0.0203028982  recall:   0.66%  precision:  58.86%  [   29/  250]
loss: 0.0198971563  recall:   0.83%  precision:  58.71%  [   30/  250]
loss: 0.0195382946  recall:   1.04%  precision:  59.74%  [   31/  250]
loss: 0.0191869571  recall:   1.28%  precision:  60.13%  [   32/  250]
loss: 0.0188761934  recall:   1.49%  precision:  60.63%  [   33/  250]
loss: 0.0185881886  recall:   1.78%  precision:  61.49%  [   34/  250]
loss: 0.0182565228  recall:   2.15%  precision:  61.01%  [   35/  250]
loss: 0.0179779889  recall:   2.52%  precision:  61.73%  [   36/  250]
loss: 0.0177527923  recall:   2.91%  precision:  62.53%  [   37/  250]
loss: 0.0175645249  recall:   3.10%  precision:  62.85%  [   38/  250]
loss: 0.0173568063  recall:   3.62%  precision:  63.85%  [   39/  250]
loss: 0.0171878930  recall:   3.93%  precision:  64.73%  [   40/  250]
loss: 0.0170167777  recall:   4.26%  precision:  65.09%  [   41/  250]
loss: 0.0168627504  recall:   4.49%  precision:  66.60%  [   42/  250]
loss: 0.0167164102  recall:   4.84%  precision:  66.95%  [   43/  250]
loss: 0.0165705897  recall:   5.16%  precision:  67.87%  [   44/  250]
loss: 0.0164356865  recall:   5.36%  precision:  68.53%  [   45/  250]
loss: 0.0163107254  recall:   5.67%  precision:  69.44%  [   46/  250]
loss: 0.0161896960  recall:   6.03%  precision:  69.82%  [   47/  250]
loss: 0.0160732455  recall:   6.43%  precision:  70.79%  [   48/  250]
loss: 0.0159736343  recall:   6.64%  precision:  71.55%  [   49/  250]
loss: 0.0158665659  recall:   6.86%  precision:  71.98%  [   50/  250]
####  eval set loss: 0.0143599252  recall:  10.09%  precision:  78.66%
loss: 0.0157704320  recall:   7.30%  precision:  72.67%  [   51/  250]
loss: 0.0156598369  recall:   7.57%  precision:  73.53%  [   52/  250]
loss: 0.0155699431  recall:   7.83%  precision:  73.78%  [   53/  250]
loss: 0.0154851272  recall:   8.11%  precision:  74.50%  [   54/  250]
loss: 0.0153906783  recall:   8.40%  precision:  74.96%  [   55/  250]
loss: 0.0153056152  recall:   8.80%  precision:  76.00%  [   56/  250]
loss: 0.0152211907  recall:   9.00%  precision:  76.01%  [   57/  250]
loss: 0.0151354303  recall:   9.36%  precision:  76.73%  [   58/  250]
loss: 0.0150562599  recall:   9.58%  precision:  77.26%  [   59/  250]
loss: 0.0149681609  recall:  10.08%  precision:  78.25%  [   60/  250]
loss: 0.0148822561  recall:  10.24%  precision:  78.42%  [   61/  250]
loss: 0.0148134052  recall:  10.52%  precision:  78.80%  [   62/  250]
loss: 0.0147282717  recall:  10.87%  precision:  79.44%  [   63/  250]
loss: 0.0146567141  recall:  11.35%  precision:  79.64%  [   64/  250]
loss: 0.0145854476  recall:  11.33%  precision:  80.02%  [   65/  250]
loss: 0.0145112493  recall:  11.80%  precision:  80.66%  [   66/  250]
loss: 0.0144368756  recall:  11.95%  precision:  81.01%  [   67/  250]
loss: 0.0143689833  recall:  12.36%  precision:  81.50%  [   68/  250]
loss: 0.0143097638  recall:  12.95%  precision:  81.76%  [   69/  250]
loss: 0.0142410009  recall:  12.90%  precision:  81.94%  [   70/  250]
loss: 0.0141790837  recall:  13.32%  precision:  82.02%  [   71/  250]
loss: 0.0141298108  recall:  13.53%  precision:  82.49%  [   72/  250]
loss: 0.0140664389  recall:  13.98%  precision:  83.07%  [   73/  250]
loss: 0.0140164195  recall:  14.28%  precision:  83.19%  [   74/  250]
loss: 0.0139534076  recall:  14.69%  precision:  83.46%  [   75/  250]
loss: 0.0138942910  recall:  14.76%  precision:  83.47%  [   76/  250]
loss: 0.0138510004  recall:  15.24%  precision:  84.21%  [   77/  250]
loss: 0.0137918390  recall:  15.66%  precision:  84.35%  [   78/  250]
loss: 0.0137382880  recall:  15.83%  precision:  84.39%  [   79/  250]
loss: 0.0136900122  recall:  16.00%  precision:  84.76%  [   80/  250]
loss: 0.0136450175  recall:  16.58%  precision:  84.92%  [   81/  250]
loss: 0.0135927393  recall:  16.85%  precision:  85.20%  [   82/  250]
loss: 0.0135527152  recall:  17.23%  precision:  85.46%  [   83/  250]
loss: 0.0135066528  recall:  17.47%  precision:  85.61%  [   84/  250]
loss: 0.0134556521  recall:  17.84%  precision:  85.79%  [   85/  250]
loss: 0.0134180812  recall:  18.19%  precision:  85.82%  [   86/  250]
loss: 0.0133659808  recall:  18.50%  precision:  86.06%  [   87/  250]
loss: 0.0133379446  recall:  19.16%  precision:  86.08%  [   88/  250]
loss: 0.0132815097  recall:  19.27%  precision:  86.27%  [   89/  250]
loss: 0.0132307825  recall:  19.59%  precision:  86.55%  [   90/  250]
loss: 0.0131905515  recall:  19.61%  precision:  86.65%  [   91/  250]
loss: 0.0131496100  recall:  20.09%  precision:  86.68%  [   92/  250]
loss: 0.0131135807  recall:  20.71%  precision:  86.97%  [   93/  250]
loss: 0.0130781254  recall:  20.78%  precision:  87.09%  [   94/  250]
loss: 0.0130407373  recall:  21.13%  precision:  86.97%  [   95/  250]
loss: 0.0130090681  recall:  21.48%  precision:  87.37%  [   96/  250]
loss: 0.0129771766  recall:  22.05%  precision:  87.41%  [   97/  250]
loss: 0.0129459425  recall:  22.12%  precision:  87.30%  [   98/  250]
loss: 0.0129062950  recall:  22.21%  precision:  87.60%  [   99/  250]
loss: 0.0128765368  recall:  22.79%  precision:  87.71%  [  100/  250]
####  eval set loss: 0.0117172468  recall:  26.76%  precision:  89.24%
loss: 0.0128363066  recall:  23.10%  precision:  87.66%  [  101/  250]
loss: 0.0127978640  recall:  23.24%  precision:  87.93%  [  102/  250]
loss: 0.0127663177  recall:  23.86%  precision:  87.81%  [  103/  250]
loss: 0.0127293683  recall:  24.01%  precision:  88.15%  [  104/  250]
loss: 0.0127035759  recall:  24.07%  precision:  88.25%  [  105/  250]
loss: 0.0126835503  recall:  24.99%  precision:  88.32%  [  106/  250]
loss: 0.0126558215  recall:  25.12%  precision:  88.53%  [  107/  250]
loss: 0.0126037692  recall:  25.24%  precision:  88.61%  [  108/  250]
loss: 0.0125878192  recall:  25.67%  precision:  88.45%  [  109/  250]
loss: 0.0125719796  recall:  25.86%  precision:  88.57%  [  110/  250]
loss: 0.0125291597  recall:  26.01%  precision:  88.75%  [  111/  250]
loss: 0.0124914816  recall:  26.39%  precision:  88.96%  [  112/  250]
loss: 0.0124782474  recall:  26.86%  precision:  89.05%  [  113/  250]
loss: 0.0124615924  recall:  26.98%  precision:  88.88%  [  114/  250]
loss: 0.0124283235  recall:  27.28%  precision:  89.26%  [  115/  250]
loss: 0.0124126968  recall:  27.28%  precision:  89.23%  [  116/  250]
loss: 0.0123721071  recall:  27.68%  precision:  89.32%  [  117/  250]
loss: 0.0123449398  recall:  28.18%  precision:  89.32%  [  118/  250]
loss: 0.0123296173  recall:  28.02%  precision:  89.33%  [  119/  250]
loss: 0.0123012299  recall:  28.69%  precision:  89.72%  [  120/  250]
loss: 0.0122954639  recall:  28.92%  precision:  89.65%  [  121/  250]
loss: 0.0122483002  recall:  29.16%  precision:  89.75%  [  122/  250]
loss: 0.0122210343  recall:  29.25%  precision:  89.83%  [  123/  250]
loss: 0.0122127270  recall:  29.64%  precision:  89.86%  [  124/  250]
loss: 0.0121876888  recall:  29.75%  precision:  89.88%  [  125/  250]
loss: 0.0121621693  recall:  30.03%  precision:  90.02%  [  126/  250]
loss: 0.0121468049  recall:  29.90%  precision:  90.05%  [  127/  250]
loss: 0.0121172914  recall:  30.33%  precision:  89.90%  [  128/  250]
loss: 0.0120997287  recall:  30.79%  precision:  90.09%  [  129/  250]
loss: 0.0120834813  recall:  30.91%  precision:  90.26%  [  130/  250]
loss: 0.0120605414  recall:  31.15%  precision:  90.25%  [  131/  250]
loss: 0.0120553429  recall:  31.81%  precision:  90.11%  [  132/  250]
loss: 0.0120325379  recall:  31.42%  precision:  90.18%  [  133/  250]
loss: 0.0119916273  recall:  31.83%  precision:  90.34%  [  134/  250]
loss: 0.0119784600  recall:  32.14%  precision:  90.47%  [  135/  250]
loss: 0.0119645634  recall:  32.12%  precision:  90.41%  [  136/  250]
loss: 0.0119344023  recall:  32.48%  precision:  90.47%  [  137/  250]
loss: 0.0119263225  recall:  32.69%  precision:  90.63%  [  138/  250]
loss: 0.0119061830  recall:  32.74%  precision:  90.52%  [  139/  250]
loss: 0.0118865386  recall:  32.94%  precision:  90.51%  [  140/  250]
loss: 0.0118609009  recall:  33.18%  precision:  90.59%  [  141/  250]
loss: 0.0118368612  recall:  33.43%  precision:  90.70%  [  142/  250]
loss: 0.0118275061  recall:  33.57%  precision:  90.81%  [  143/  250]
loss: 0.0118116265  recall:  33.58%  precision:  90.82%  [  144/  250]
loss: 0.0117949015  recall:  33.86%  precision:  90.72%  [  145/  250]
loss: 0.0117770306  recall:  33.99%  precision:  90.81%  [  146/  250]
loss: 0.0117541019  recall:  34.28%  precision:  90.93%  [  147/  250]
loss: 0.0117456332  recall:  34.47%  precision:  90.96%  [  148/  250]
loss: 0.0117242193  recall:  34.57%  precision:  90.94%  [  149/  250]
loss: 0.0117120088  recall:  34.78%  precision:  90.97%  [  150/  250]
####  eval set loss: 0.0106530585  recall:  41.90%  precision:  90.89%
loss: 0.0116821235  recall:  34.72%  precision:  91.02%  [  151/  250]
loss: 0.0116763560  recall:  35.25%  precision:  91.01%  [  152/  250]
loss: 0.0116538970  recall:  35.40%  precision:  90.92%  [  153/  250]
loss: 0.0116614958  recall:  35.58%  precision:  90.94%  [  154/  250]
loss: 0.0116219856  recall:  35.61%  precision:  91.17%  [  155/  250]
loss: 0.0116202587  recall:  35.86%  precision:  91.17%  [  156/  250]
loss: 0.0115898046  recall:  36.10%  precision:  91.19%  [  157/  250]
loss: 0.0115787960  recall:  35.88%  precision:  91.23%  [  158/  250]
loss: 0.0115687727  recall:  36.30%  precision:  91.14%  [  159/  250]
loss: 0.0115570564  recall:  36.58%  precision:  91.14%  [  160/  250]
loss: 0.0115366482  recall:  36.48%  precision:  91.36%  [  161/  250]
loss: 0.0115187308  recall:  36.51%  precision:  91.29%  [  162/  250]
loss: 0.0114952668  recall:  36.80%  precision:  91.32%  [  163/  250]
loss: 0.0115045179  recall:  37.02%  precision:  91.28%  [  164/  250]
loss: 0.0114794276  recall:  36.97%  precision:  91.44%  [  165/  250]
loss: 0.0114631716  recall:  37.14%  precision:  91.39%  [  166/  250]
loss: 0.0114466563  recall:  37.28%  precision:  91.46%  [  167/  250]
loss: 0.0114332235  recall:  37.40%  precision:  91.52%  [  168/  250]
loss: 0.0114323309  recall:  37.44%  precision:  91.40%  [  169/  250]
loss: 0.0114121879  recall:  37.59%  precision:  91.42%  [  170/  250]
loss: 0.0114074294  recall:  37.57%  precision:  91.42%  [  171/  250]
loss: 0.0113912576  recall:  37.97%  precision:  91.47%  [  172/  250]
loss: 0.0113777173  recall:  37.94%  precision:  91.51%  [  173/  250]
loss: 0.0113689175  recall:  38.19%  precision:  91.46%  [  174/  250]
loss: 0.0113448694  recall:  38.49%  precision:  91.50%  [  175/  250]
loss: 0.0113360328  recall:  38.21%  precision:  91.52%  [  176/  250]
loss: 0.0113301096  recall:  38.65%  precision:  91.59%  [  177/  250]
loss: 0.0113065623  recall:  38.32%  precision:  91.70%  [  178/  250]
loss: 0.0112966066  recall:  38.77%  precision:  91.56%  [  179/  250]
loss: 0.0112859676  recall:  39.03%  precision:  91.54%  [  180/  250]
loss: 0.0112766048  recall:  38.74%  precision:  91.69%  [  181/  250]
loss: 0.0112734766  recall:  38.91%  precision:  91.59%  [  182/  250]
loss: 0.0112696467  recall:  39.11%  precision:  91.62%  [  183/  250]
loss: 0.0112400823  recall:  39.03%  precision:  91.66%  [  184/  250]
loss: 0.0112231928  recall:  39.33%  precision:  91.64%  [  185/  250]
loss: 0.0112273002  recall:  39.54%  precision:  91.62%  [  186/  250]
loss: 0.0112103416  recall:  39.45%  precision:  91.67%  [  187/  250]
loss: 0.0111932521  recall:  39.70%  precision:  91.63%  [  188/  250]
loss: 0.0111806373  recall:  39.74%  precision:  91.79%  [  189/  250]
loss: 0.0111793522  recall:  39.61%  precision:  91.88%  [  190/  250]
loss: 0.0111617258  recall:  39.96%  precision:  91.72%  [  191/  250]
loss: 0.0111592007  recall:  39.62%  precision:  91.71%  [  192/  250]
loss: 0.0111575862  recall:  40.26%  precision:  91.80%  [  193/  250]
loss: 0.0111326567  recall:  40.20%  precision:  91.75%  [  194/  250]
loss: 0.0111273982  recall:  40.08%  precision:  91.79%  [  195/  250]
loss: 0.0111054985  recall:  40.12%  precision:  91.81%  [  196/  250]
loss: 0.0110959582  recall:  40.23%  precision:  91.84%  [  197/  250]
loss: 0.0110817924  recall:  40.49%  precision:  91.91%  [  198/  250]
loss: 0.0110754387  recall:  40.38%  precision:  91.77%  [  199/  250]
loss: 0.0110653574  recall:  40.49%  precision:  91.82%  [  200/  250]
####  eval set loss: 0.0100537851  recall:  49.62%  precision:  91.97%
loss: 0.0110624445  recall:  40.54%  precision:  91.85%  [  201/  250]
loss: 0.0110483821  recall:  40.62%  precision:  91.97%  [  202/  250]
loss: 0.0110387411  recall:  40.81%  precision:  91.92%  [  203/  250]
loss: 0.0110287063  recall:  40.71%  precision:  91.84%  [  204/  250]
loss: 0.0110253677  recall:  40.81%  precision:  91.85%  [  205/  250]
loss: 0.0110184910  recall:  41.37%  precision:  91.85%  [  206/  250]
loss: 0.0110002917  recall:  40.87%  precision:  92.02%  [  207/  250]
loss: 0.0109950816  recall:  41.15%  precision:  91.92%  [  208/  250]
loss: 0.0109776925  recall:  41.33%  precision:  91.88%  [  209/  250]
loss: 0.0109694344  recall:  41.19%  precision:  91.96%  [  210/  250]
loss: 0.0109710425  recall:  41.47%  precision:  92.03%  [  211/  250]
loss: 0.0109510571  recall:  41.20%  precision:  92.16%  [  212/  250]
loss: 0.0109451556  recall:  41.56%  precision:  91.92%  [  213/  250]
loss: 0.0109389275  recall:  41.44%  precision:  92.07%  [  214/  250]
loss: 0.0109480165  recall:  41.59%  precision:  91.94%  [  215/  250]
loss: 0.0109155574  recall:  41.64%  precision:  91.99%  [  216/  250]
loss: 0.0109268062  recall:  41.83%  precision:  91.91%  [  217/  250]
loss: 0.0109026814  recall:  41.65%  precision:  91.93%  [  218/  250]
loss: 0.0108960381  recall:  41.90%  precision:  92.11%  [  219/  250]
loss: 0.0108812334  recall:  41.78%  precision:  92.04%  [  220/  250]
loss: 0.0108630735  recall:  41.96%  precision:  92.09%  [  221/  250]
loss: 0.0108843013  recall:  42.07%  precision:  92.09%  [  222/  250]
loss: 0.0108545806  recall:  41.95%  precision:  92.05%  [  223/  250]
loss: 0.0108571251  recall:  42.08%  precision:  92.07%  [  224/  250]
loss: 0.0108397258  recall:  42.25%  precision:  92.14%  [  225/  250]
loss: 0.0108527464  recall:  42.32%  precision:  92.09%  [  226/  250]
loss: 0.0108206082  recall:  42.34%  precision:  92.18%  [  227/  250]
loss: 0.0108178118  recall:  42.45%  precision:  92.22%  [  228/  250]
loss: 0.0108094881  recall:  42.37%  precision:  92.18%  [  229/  250]
loss: 0.0108001968  recall:  42.59%  precision:  92.10%  [  230/  250]
loss: 0.0107865971  recall:  42.36%  precision:  92.14%  [  231/  250]
loss: 0.0107765002  recall:  42.53%  precision:  92.19%  [  232/  250]
loss: 0.0107730244  recall:  42.69%  precision:  92.19%  [  233/  250]
loss: 0.0107941008  recall:  42.77%  precision:  92.17%  [  234/  250]
loss: 0.0107607894  recall:  42.66%  precision:  92.16%  [  235/  250]
loss: 0.0107563160  recall:  42.62%  precision:  92.24%  [  236/  250]
loss: 0.0107616817  recall:  43.30%  precision:  92.12%  [  237/  250]
loss: 0.0107388732  recall:  42.87%  precision:  92.29%  [  238/  250]
loss: 0.0107350007  recall:  43.03%  precision:  92.25%  [  239/  250]
loss: 0.0107418423  recall:  42.86%  precision:  92.10%  [  240/  250]
loss: 0.0107069763  recall:  42.81%  precision:  92.36%  [  241/  250]
loss: 0.0107231642  recall:  43.12%  precision:  92.25%  [  242/  250]
loss: 0.0106979113  recall:  42.74%  precision:  92.39%  [  243/  250]
loss: 0.0107099913  recall:  43.18%  precision:  92.25%  [  244/  250]
loss: 0.0106929656  recall:  43.13%  precision:  92.24%  [  245/  250]
loss: 0.0106787742  recall:  43.16%  precision:  92.36%  [  246/  250]
loss: 0.0106654500  recall:  43.21%  precision:  92.19%  [  247/  250]
loss: 0.0106805695  recall:  43.08%  precision:  92.28%  [  248/  250]
loss: 0.0106487552  recall:  43.51%  precision:  92.32%  [  249/  250]
loss: 0.0106462031  recall:  43.35%  precision:  92.33%  [  250/  250]
####  eval set loss: 0.0096442448  recall:  48.90%  precision:  92.99%
@@@@  test set loss: 0.0103512251  recall:  49.97%  precision:  91.58%

-------------------------------------------------------------

@@@@  pistacking
Model weights: 4119
loss: 0.6921951822  recall:   0.00%  precision:   0.00%  [    1/  250]
loss: 0.4230721547  recall:   0.00%  precision:   0.00%  [    2/  250]
loss: 0.1892842889  recall:   0.00%  precision:   0.00%  [    3/  250]
loss: 0.1390163297  recall:   0.00%  precision:   0.00%  [    4/  250]
loss: 0.1264665495  recall:   0.00%  precision:   0.00%  [    5/  250]
loss: 0.1182399219  recall:   0.00%  precision:   0.00%  [    6/  250]
loss: 0.1117466994  recall:   0.00%  precision:   0.00%  [    7/  250]
loss: 0.1068957059  recall:   0.00%  precision:   0.00%  [    8/  250]
loss: 0.1024758303  recall:   0.00%  precision:   0.00%  [    9/  250]
loss: 0.0987271420  recall:   0.00%  precision:   0.00%  [   10/  250]
loss: 0.0956295294  recall:   0.00%  precision:   0.00%  [   11/  250]
loss: 0.0929597757  recall:   0.00%  precision:   0.00%  [   12/  250]
loss: 0.0902764828  recall:   0.00%  precision:   0.00%  [   13/  250]
loss: 0.0880683197  recall:   0.00%  precision:   0.00%  [   14/  250]
loss: 0.0858506747  recall:   0.00%  precision:   0.00%  [   15/  250]
loss: 0.0839668805  recall:   0.00%  precision:   0.00%  [   16/  250]
loss: 0.0819928816  recall:   0.00%  precision:   0.00%  [   17/  250]
loss: 0.0802346306  recall:   0.00%  precision:   0.00%  [   18/  250]
loss: 0.0784984375  recall:   0.00%  precision:   0.00%  [   19/  250]
loss: 0.0770248392  recall:   0.00%  precision:   0.00%  [   20/  250]
loss: 0.0753197321  recall:   0.00%  precision:   0.00%  [   21/  250]
loss: 0.0741257283  recall:   0.00%  precision:   0.00%  [   22/  250]
loss: 0.0724956120  recall:   0.00%  precision:  33.33%  [   23/  250]
loss: 0.0711862232  recall:   0.00%  precision:  60.00%  [   24/  250]
loss: 0.0698397186  recall:   0.01%  precision:  77.27%  [   25/  250]
loss: 0.0686854913  recall:   0.02%  precision:  79.49%  [   26/  250]
loss: 0.0674743710  recall:   0.04%  precision:  80.00%  [   27/  250]
loss: 0.0662249320  recall:   0.07%  precision:  80.95%  [   28/  250]
loss: 0.0650145738  recall:   0.10%  precision:  82.24%  [   29/  250]
loss: 0.0637078282  recall:   0.17%  precision:  84.40%  [   30/  250]
loss: 0.0625491956  recall:   0.22%  precision:  85.23%  [   31/  250]
loss: 0.0614463636  recall:   0.30%  precision:  82.97%  [   32/  250]
loss: 0.0601379598  recall:   0.38%  precision:  84.30%  [   33/  250]
loss: 0.0590932104  recall:   0.47%  precision:  84.65%  [   34/  250]
loss: 0.0579523759  recall:   0.61%  precision:  84.68%  [   35/  250]
loss: 0.0568536280  recall:   0.76%  precision:  83.26%  [   36/  250]
loss: 0.0557179166  recall:   0.89%  precision:  83.78%  [   37/  250]
loss: 0.0548558170  recall:   1.04%  precision:  84.93%  [   38/  250]
loss: 0.0537906937  recall:   1.32%  precision:  84.61%  [   39/  250]
loss: 0.0528805038  recall:   1.46%  precision:  85.72%  [   40/  250]
loss: 0.0519743523  recall:   1.79%  precision:  85.90%  [   41/  250]
loss: 0.0511554615  recall:   1.93%  precision:  86.60%  [   42/  250]
loss: 0.0503742357  recall:   2.23%  precision:  86.50%  [   43/  250]
loss: 0.0495264693  recall:   2.41%  precision:  87.05%  [   44/  250]
loss: 0.0488521948  recall:   2.69%  precision:  87.28%  [   45/  250]
loss: 0.0481824786  recall:   2.99%  precision:  87.56%  [   46/  250]
loss: 0.0475160441  recall:   3.21%  precision:  87.29%  [   47/  250]
loss: 0.0470631210  recall:   3.45%  precision:  87.34%  [   48/  250]
loss: 0.0464395755  recall:   3.76%  precision:  87.06%  [   49/  250]
loss: 0.0459561814  recall:   3.80%  precision:  87.71%  [   50/  250]
####  eval set loss: 0.0399369779  recall:   6.56%  precision:  87.67%
loss: 0.0454747685  recall:   4.24%  precision:  87.03%  [   51/  250]
loss: 0.0450975042  recall:   4.45%  precision:  86.98%  [   52/  250]
loss: 0.0446765904  recall:   4.66%  precision:  87.06%  [   53/  250]
loss: 0.0443715020  recall:   4.79%  precision:  87.38%  [   54/  250]
loss: 0.0440332909  recall:   5.21%  precision:  86.72%  [   55/  250]
loss: 0.0437045129  recall:   5.32%  precision:  87.04%  [   56/  250]
loss: 0.0433767930  recall:   5.69%  precision:  86.65%  [   57/  250]
loss: 0.0430808708  recall:   5.81%  precision:  86.75%  [   58/  250]
loss: 0.0427825162  recall:   6.03%  precision:  86.39%  [   59/  250]
loss: 0.0426866110  recall:   6.11%  precision:  86.55%  [   60/  250]
loss: 0.0423814270  recall:   6.33%  precision:  86.45%  [   61/  250]
loss: 0.0422493950  recall:   6.40%  precision:  86.49%  [   62/  250]
loss: 0.0420440981  recall:   6.49%  precision:  86.40%  [   63/  250]
loss: 0.0417951565  recall:   6.93%  precision:  86.06%  [   64/  250]
loss: 0.0416377542  recall:   7.09%  precision:  86.00%  [   65/  250]
loss: 0.0414096843  recall:   6.96%  precision:  86.30%  [   66/  250]
loss: 0.0412583167  recall:   7.26%  precision:  86.06%  [   67/  250]
loss: 0.0410838114  recall:   7.35%  precision:  86.01%  [   68/  250]
loss: 0.0409874106  recall:   7.43%  precision:  86.22%  [   69/  250]
loss: 0.0408500186  recall:   7.49%  precision:  85.85%  [   70/  250]
loss: 0.0406673975  recall:   7.56%  precision:  86.03%  [   71/  250]
loss: 0.0405433615  recall:   7.54%  precision:  86.36%  [   72/  250]
loss: 0.0404274433  recall:   7.73%  precision:  86.16%  [   73/  250]
loss: 0.0402856046  recall:   7.77%  precision:  86.14%  [   74/  250]
loss: 0.0402040554  recall:   7.80%  precision:  86.34%  [   75/  250]
loss: 0.0401555342  recall:   7.84%  precision:  86.26%  [   76/  250]
loss: 0.0400076221  recall:   8.08%  precision:  85.93%  [   77/  250]
loss: 0.0398996966  recall:   7.98%  precision:  85.90%  [   78/  250]
loss: 0.0397446904  recall:   7.77%  precision:  86.73%  [   79/  250]
loss: 0.0396954467  recall:   8.10%  precision:  86.23%  [   80/  250]
loss: 0.0395996478  recall:   8.18%  precision:  86.43%  [   81/  250]
loss: 0.0395685301  recall:   8.06%  precision:  86.42%  [   82/  250]
loss: 0.0394987574  recall:   7.97%  precision:  86.56%  [   83/  250]
loss: 0.0392818383  recall:   8.17%  precision:  86.44%  [   84/  250]
loss: 0.0393287583  recall:   8.04%  precision:  86.54%  [   85/  250]
loss: 0.0392288791  recall:   8.23%  precision:  86.45%  [   86/  250]
loss: 0.0392050886  recall:   8.27%  precision:  86.16%  [   87/  250]
loss: 0.0390722648  recall:   8.15%  precision:  86.48%  [   88/  250]
loss: 0.0390078044  recall:   8.14%  precision:  86.51%  [   89/  250]
loss: 0.0389866284  recall:   8.32%  precision:  86.27%  [   90/  250]
loss: 0.0388345831  recall:   8.16%  precision:  86.40%  [   91/  250]
loss: 0.0388049938  recall:   8.40%  precision:  86.67%  [   92/  250]
loss: 0.0386163340  recall:   8.27%  precision:  86.62%  [   93/  250]
loss: 0.0386515896  recall:   8.40%  precision:  86.57%  [   94/  250]
loss: 0.0385444961  recall:   8.35%  precision:  86.53%  [   95/  250]
loss: 0.0385613314  recall:   8.46%  precision:  86.42%  [   96/  250]
loss: 0.0384146474  recall:   8.43%  precision:  86.64%  [   97/  250]
loss: 0.0384314983  recall:   8.35%  precision:  86.81%  [   98/  250]
loss: 0.0383420479  recall:   8.38%  precision:  86.79%  [   99/  250]
loss: 0.0382919220  recall:   8.48%  precision:  86.92%  [  100/  250]
####  eval set loss: 0.0330781803  recall:  12.50%  precision:  86.23%
loss: 0.0382417047  recall:   8.43%  precision:  86.74%  [  101/  250]
loss: 0.0381679934  recall:   8.47%  precision:  86.55%  [  102/  250]
loss: 0.0381858483  recall:   8.51%  precision:  86.79%  [  103/  250]
loss: 0.0381216461  recall:   8.45%  precision:  86.58%  [  104/  250]
loss: 0.0380567001  recall:   8.57%  precision:  86.98%  [  105/  250]
loss: 0.0379419489  recall:   8.72%  precision:  86.83%  [  106/  250]
loss: 0.0379910958  recall:   8.40%  precision:  86.83%  [  107/  250]
loss: 0.0378232727  recall:   8.72%  precision:  86.66%  [  108/  250]
loss: 0.0378796131  recall:   8.51%  precision:  86.97%  [  109/  250]
loss: 0.0378011357  recall:   8.55%  precision:  86.88%  [  110/  250]
loss: 0.0376789281  recall:   8.65%  precision:  86.91%  [  111/  250]
loss: 0.0376743471  recall:   8.64%  precision:  86.47%  [  112/  250]
loss: 0.0377080040  recall:   8.42%  precision:  87.07%  [  113/  250]
loss: 0.0375521695  recall:   8.66%  precision:  86.84%  [  114/  250]
loss: 0.0375468714  recall:   8.56%  precision:  86.58%  [  115/  250]
loss: 0.0374063815  recall:   8.70%  precision:  86.96%  [  116/  250]
loss: 0.0374231845  recall:   8.71%  precision:  86.87%  [  117/  250]
loss: 0.0373704044  recall:   8.60%  precision:  86.89%  [  118/  250]
loss: 0.0374507492  recall:   8.78%  precision:  86.50%  [  119/  250]
loss: 0.0372730363  recall:   8.50%  precision:  86.98%  [  120/  250]
loss: 0.0372210025  recall:   8.80%  precision:  86.77%  [  121/  250]
loss: 0.0372682217  recall:   8.62%  precision:  86.98%  [  122/  250]
loss: 0.0371538579  recall:   8.69%  precision:  87.07%  [  123/  250]
loss: 0.0370980997  recall:   8.59%  precision:  86.81%  [  124/  250]
loss: 0.0371318373  recall:   8.86%  precision:  86.49%  [  125/  250]
loss: 0.0371356728  recall:   8.60%  precision:  87.00%  [  126/  250]
loss: 0.0370835142  recall:   8.66%  precision:  87.01%  [  127/  250]
loss: 0.0370358309  recall:   8.74%  precision:  86.51%  [  128/  250]
loss: 0.0369663305  recall:   8.81%  precision:  86.55%  [  129/  250]
loss: 0.0369102393  recall:   8.66%  precision:  86.79%  [  130/  250]
loss: 0.0368811993  recall:   8.80%  precision:  86.65%  [  131/  250]
loss: 0.0368377142  recall:   8.72%  precision:  86.88%  [  132/  250]
loss: 0.0368286483  recall:   9.02%  precision:  86.39%  [  133/  250]
loss: 0.0368302012  recall:   8.53%  precision:  86.94%  [  134/  250]
loss: 0.0368041216  recall:   8.74%  precision:  86.41%  [  135/  250]
loss: 0.0368088899  recall:   8.89%  precision:  86.56%  [  136/  250]
loss: 0.0367227896  recall:   8.72%  precision:  86.74%  [  137/  250]
loss: 0.0367539091  recall:   8.74%  precision:  86.75%  [  138/  250]
loss: 0.0366788679  recall:   8.94%  precision:  86.33%  [  139/  250]
loss: 0.0366725914  recall:   8.72%  precision:  86.79%  [  140/  250]
loss: 0.0366234917  recall:   8.94%  precision:  86.59%  [  141/  250]
loss: 0.0366391363  recall:   8.78%  precision:  86.54%  [  142/  250]
loss: 0.0365689090  recall:   8.79%  precision:  86.59%  [  143/  250]
loss: 0.0364804036  recall:   8.82%  precision:  86.61%  [  144/  250]
loss: 0.0365101331  recall:   8.67%  precision:  86.88%  [  145/  250]
loss: 0.0364455181  recall:   8.88%  precision:  86.52%  [  146/  250]
loss: 0.0364164799  recall:   8.95%  precision:  86.66%  [  147/  250]
loss: 0.0364434041  recall:   8.62%  precision:  86.71%  [  148/  250]
loss: 0.0363846505  recall:   8.69%  precision:  86.78%  [  149/  250]
loss: 0.0363565783  recall:   8.92%  precision:  86.67%  [  150/  250]
####  eval set loss: 0.0311523971  recall:  12.17%  precision:  86.83%
loss: 0.0363339107  recall:   8.98%  precision:  86.65%  [  151/  250]
loss: 0.0363478651  recall:   8.89%  precision:  86.68%  [  152/  250]
loss: 0.0363572093  recall:   8.75%  precision:  86.63%  [  153/  250]
loss: 0.0363134340  recall:   8.98%  precision:  86.51%  [  154/  250]
loss: 0.0363989880  recall:   8.85%  precision:  86.43%  [  155/  250]
loss: 0.0362893500  recall:   8.72%  precision:  86.69%  [  156/  250]
loss: 0.0361974090  recall:   8.84%  precision:  86.65%  [  157/  250]
loss: 0.0362251242  recall:   8.65%  precision:  86.84%  [  158/  250]
loss: 0.0362838880  recall:   9.13%  precision:  85.59%  [  159/  250]
loss: 0.0361577051  recall:   8.86%  precision:  86.68%  [  160/  250]
loss: 0.0361915253  recall:   8.81%  precision:  86.69%  [  161/  250]
loss: 0.0361388045  recall:   8.94%  precision:  86.43%  [  162/  250]
loss: 0.0360844648  recall:   8.66%  precision:  86.79%  [  163/  250]
loss: 0.0360526367  recall:   8.91%  precision:  86.50%  [  164/  250]
loss: 0.0360466019  recall:   8.79%  precision:  86.53%  [  165/  250]
loss: 0.0359731776  recall:   8.95%  precision:  86.38%  [  166/  250]
loss: 0.0360416310  recall:   8.80%  precision:  86.45%  [  167/  250]
loss: 0.0359925347  recall:   9.03%  precision:  86.16%  [  168/  250]
loss: 0.0359756045  recall:   8.81%  precision:  86.52%  [  169/  250]
loss: 0.0359232937  recall:   8.85%  precision:  86.47%  [  170/  250]
loss: 0.0359732364  recall:   9.18%  precision:  86.39%  [  171/  250]
loss: 0.0359723726  recall:   8.86%  precision:  86.50%  [  172/  250]
loss: 0.0358627055  recall:   8.79%  precision:  86.59%  [  173/  250]
loss: 0.0358254974  recall:   8.99%  precision:  86.68%  [  174/  250]
loss: 0.0358770253  recall:   8.89%  precision:  86.43%  [  175/  250]
loss: 0.0358765857  recall:   8.84%  precision:  86.53%  [  176/  250]
loss: 0.0358266267  recall:   8.96%  precision:  86.62%  [  177/  250]
loss: 0.0357717450  recall:   8.95%  precision:  86.57%  [  178/  250]
loss: 0.0358397517  recall:   9.08%  precision:  86.16%  [  179/  250]
loss: 0.0358450718  recall:   8.67%  precision:  86.31%  [  180/  250]
loss: 0.0358101775  recall:   8.88%  precision:  86.66%  [  181/  250]
loss: 0.0357440059  recall:   9.05%  precision:  86.59%  [  182/  250]
loss: 0.0357000716  recall:   9.02%  precision:  86.37%  [  183/  250]
loss: 0.0357383094  recall:   8.84%  precision:  86.61%  [  184/  250]
loss: 0.0357296186  recall:   8.94%  precision:  86.31%  [  185/  250]
loss: 0.0357102338  recall:   8.94%  precision:  86.51%  [  186/  250]
loss: 0.0356180568  recall:   8.90%  precision:  86.72%  [  187/  250]
loss: 0.0356940386  recall:   8.80%  precision:  86.61%  [  188/  250]
loss: 0.0356258138  recall:   9.02%  precision:  86.23%  [  189/  250]
loss: 0.0356252442  recall:   8.81%  precision:  86.28%  [  190/  250]
loss: 0.0355956700  recall:   9.01%  precision:  86.30%  [  191/  250]
loss: 0.0356598559  recall:   8.90%  precision:  86.24%  [  192/  250]
loss: 0.0356098191  recall:   8.93%  precision:  86.39%  [  193/  250]
loss: 0.0356032375  recall:   8.78%  precision:  86.43%  [  194/  250]
loss: 0.0356518214  recall:   8.93%  precision:  86.27%  [  195/  250]
loss: 0.0355877987  recall:   8.92%  precision:  86.31%  [  196/  250]
loss: 0.0355900574  recall:   8.73%  precision:  86.51%  [  197/  250]
loss: 0.0356159542  recall:   8.81%  precision:  86.07%  [  198/  250]
loss: 0.0356099709  recall:   8.78%  precision:  86.27%  [  199/  250]
loss: 0.0355513489  recall:   9.05%  precision:  86.19%  [  200/  250]
####  eval set loss: 0.0304163655  recall:  12.02%  precision:  86.43%
loss: 0.0355614150  recall:   8.86%  precision:  86.36%  [  201/  250]
loss: 0.0354979721  recall:   8.74%  precision:  86.43%  [  202/  250]
loss: 0.0355043875  recall:   8.89%  precision:  86.21%  [  203/  250]
loss: 0.0354971451  recall:   8.87%  precision:  86.20%  [  204/  250]
loss: 0.0355062459  recall:   8.89%  precision:  85.96%  [  205/  250]
loss: 0.0354556799  recall:   8.80%  precision:  86.29%  [  206/  250]
loss: 0.0354723540  recall:   8.79%  precision:  86.37%  [  207/  250]
loss: 0.0354575356  recall:   8.83%  precision:  86.31%  [  208/  250]
loss: 0.0354314706  recall:   8.85%  precision:  86.24%  [  209/  250]
loss: 0.0354654403  recall:   8.83%  precision:  86.21%  [  210/  250]
loss: 0.0354036228  recall:   8.91%  precision:  86.37%  [  211/  250]
loss: 0.0353425213  recall:   8.92%  precision:  86.06%  [  212/  250]
loss: 0.0354179936  recall:   8.86%  precision:  86.01%  [  213/  250]
loss: 0.0353524456  recall:   8.73%  precision:  86.20%  [  214/  250]
loss: 0.0353677031  recall:   8.82%  precision:  86.42%  [  215/  250]
loss: 0.0353962047  recall:   8.94%  precision:  86.28%  [  216/  250]
loss: 0.0353505766  recall:   8.73%  precision:  86.32%  [  217/  250]
loss: 0.0353977310  recall:   8.94%  precision:  85.81%  [  218/  250]
loss: 0.0353013218  recall:   8.88%  precision:  86.13%  [  219/  250]
loss: 0.0353751790  recall:   8.76%  precision:  85.88%  [  220/  250]
loss: 0.0353315622  recall:   8.71%  precision:  85.87%  [  221/  250]
loss: 0.0352543836  recall:   9.01%  precision:  85.89%  [  222/  250]
loss: 0.0352737134  recall:   8.70%  precision:  86.33%  [  223/  250]
loss: 0.0353408016  recall:   8.92%  precision:  86.16%  [  224/  250]
loss: 0.0353150577  recall:   8.80%  precision:  85.80%  [  225/  250]
loss: 0.0352331355  recall:   8.70%  precision:  86.22%  [  226/  250]
loss: 0.0352730705  recall:   8.82%  precision:  86.27%  [  227/  250]
loss: 0.0353186591  recall:   8.91%  precision:  85.82%  [  228/  250]
loss: 0.0352558664  recall:   8.98%  precision:  86.02%  [  229/  250]
loss: 0.0352521084  recall:   8.71%  precision:  86.17%  [  230/  250]
loss: 0.0352643369  recall:   8.93%  precision:  85.55%  [  231/  250]
loss: 0.0352290857  recall:   8.71%  precision:  86.01%  [  232/  250]
loss: 0.0351966148  recall:   8.94%  precision:  85.88%  [  233/  250]
loss: 0.0351354729  recall:   8.80%  precision:  86.08%  [  234/  250]
loss: 0.0351524553  recall:   8.62%  precision:  86.33%  [  235/  250]
loss: 0.0352694192  recall:   8.92%  precision:  85.93%  [  236/  250]
loss: 0.0353126947  recall:   8.69%  precision:  86.14%  [  237/  250]
loss: 0.0352853701  recall:   8.84%  precision:  85.57%  [  238/  250]
loss: 0.0351495382  recall:   8.74%  precision:  85.99%  [  239/  250]
loss: 0.0351800016  recall:   8.88%  precision:  85.84%  [  240/  250]
loss: 0.0351474794  recall:   8.62%  precision:  86.40%  [  241/  250]
loss: 0.0351857141  recall:   8.80%  precision:  85.85%  [  242/  250]
loss: 0.0351559196  recall:   8.63%  precision:  85.74%  [  243/  250]
loss: 0.0351453362  recall:   9.02%  precision:  86.02%  [  244/  250]
loss: 0.0351784170  recall:   8.89%  precision:  85.91%  [  245/  250]
loss: 0.0351129238  recall:   8.61%  precision:  86.33%  [  246/  250]
loss: 0.0350617982  recall:   8.80%  precision:  86.06%  [  247/  250]
loss: 0.0350632375  recall:   8.66%  precision:  86.13%  [  248/  250]
loss: 0.0350450808  recall:   8.89%  precision:  85.90%  [  249/  250]
loss: 0.0351084100  recall:   8.80%  precision:  86.15%  [  250/  250]
####  eval set loss: 0.0300299704  recall:  12.43%  precision:  86.28%
@@@@  test set loss: 0.0297981363  recall:   9.19%  precision:  80.27%

-------------------------------------------------------------

@@@@  pication
Model weights: 4119
loss: 0.6040076732  recall:   0.00%  precision:   0.00%  [    1/  250]
loss: 0.3006653650  recall:   0.00%  precision:   0.00%  [    2/  250]
loss: 0.0860034377  recall:   0.00%  precision:   0.00%  [    3/  250]
loss: 0.0408815045  recall:   0.00%  precision:   0.00%  [    4/  250]
loss: 0.0303399041  recall:   0.00%  precision:   0.00%  [    5/  250]
loss: 0.0265951432  recall:   0.00%  precision:   0.00%  [    6/  250]
loss: 0.0248678722  recall:   0.00%  precision:   0.00%  [    7/  250]
loss: 0.0240493979  recall:   0.00%  precision:   0.00%  [    8/  250]
loss: 0.0234360899  recall:   0.00%  precision:   0.00%  [    9/  250]
loss: 0.0230313056  recall:   0.00%  precision:   0.00%  [   10/  250]
loss: 0.0227733487  recall:   0.00%  precision:   0.00%  [   11/  250]
loss: 0.0224839438  recall:   0.00%  precision:   0.00%  [   12/  250]
loss: 0.0222263081  recall:   0.00%  precision:   0.00%  [   13/  250]
loss: 0.0220364435  recall:   0.00%  precision:   0.00%  [   14/  250]
loss: 0.0218322456  recall:   0.00%  precision:   0.00%  [   15/  250]
loss: 0.0216547883  recall:   0.00%  precision:   0.00%  [   16/  250]
loss: 0.0214686848  recall:   0.00%  precision:   0.00%  [   17/  250]
loss: 0.0212721512  recall:   0.00%  precision:   0.00%  [   18/  250]
loss: 0.0210850081  recall:   0.00%  precision:   0.00%  [   19/  250]
loss: 0.0209069456  recall:   0.00%  precision:   0.00%  [   20/  250]
loss: 0.0207463610  recall:   0.00%  precision:   0.00%  [   21/  250]
loss: 0.0205978775  recall:   0.00%  precision:   0.00%  [   22/  250]
loss: 0.0204823052  recall:   0.00%  precision:   0.00%  [   23/  250]
loss: 0.0203422769  recall:   0.00%  precision:   0.00%  [   24/  250]
loss: 0.0201619791  recall:   0.00%  precision:   0.00%  [   25/  250]
loss: 0.0200162463  recall:   0.00%  precision:   0.00%  [   26/  250]
loss: 0.0199292152  recall:   0.00%  precision:   0.00%  [   27/  250]
loss: 0.0197936232  recall:   0.00%  precision:   0.00%  [   28/  250]
loss: 0.0196779243  recall:   0.00%  precision:   0.00%  [   29/  250]
loss: 0.0195485388  recall:   0.00%  precision:   0.00%  [   30/  250]
loss: 0.0194211840  recall:   0.00%  precision:   0.00%  [   31/  250]
loss: 0.0193356216  recall:   0.00%  precision:   0.00%  [   32/  250]
loss: 0.0192089376  recall:   0.00%  precision:   0.00%  [   33/  250]
loss: 0.0191416364  recall:   0.00%  precision:   0.00%  [   34/  250]
loss: 0.0190142617  recall:   0.00%  precision:   0.00%  [   35/  250]
loss: 0.0189721792  recall:   0.00%  precision:   0.00%  [   36/  250]
loss: 0.0188453758  recall:   0.00%  precision:   0.00%  [   37/  250]
loss: 0.0187541255  recall:   0.00%  precision:   0.00%  [   38/  250]
loss: 0.0186438160  recall:   0.00%  precision:   0.00%  [   39/  250]
loss: 0.0185895330  recall:   0.00%  precision:   0.00%  [   40/  250]
loss: 0.0185452428  recall:   0.00%  precision:   0.00%  [   41/  250]
loss: 0.0184731597  recall:   0.00%  precision:   0.00%  [   42/  250]
loss: 0.0183933126  recall:   0.00%  precision:   0.00%  [   43/  250]
loss: 0.0182830835  recall:   0.00%  precision:   0.00%  [   44/  250]
loss: 0.0182492174  recall:   0.00%  precision:   0.00%  [   45/  250]
loss: 0.0181636533  recall:   0.00%  precision:   0.00%  [   46/  250]
loss: 0.0181138034  recall:   0.00%  precision:   0.00%  [   47/  250]
loss: 0.0180151323  recall:   0.00%  precision:   0.00%  [   48/  250]
loss: 0.0179724810  recall:   0.00%  precision:   0.00%  [   49/  250]
loss: 0.0179345702  recall:   0.00%  precision:   0.00%  [   50/  250]
####  eval set loss: 0.0167205435  recall:   0.00%  precision:   0.00%
loss: 0.0178405798  recall:   0.00%  precision:   0.00%  [   51/  250]
loss: 0.0177916037  recall:   0.00%  precision:   0.00%  [   52/  250]
loss: 0.0177779320  recall:   0.00%  precision:   0.00%  [   53/  250]
loss: 0.0176999944  recall:   0.00%  precision:   0.00%  [   54/  250]
loss: 0.0176484600  recall:   0.00%  precision:   0.00%  [   55/  250]
loss: 0.0176214339  recall:   0.00%  precision:   0.00%  [   56/  250]
loss: 0.0175421830  recall:   0.00%  precision:   0.00%  [   57/  250]
loss: 0.0175242140  recall:   0.00%  precision:   0.00%  [   58/  250]
loss: 0.0174944750  recall:   0.00%  precision:   0.00%  [   59/  250]
loss: 0.0174224177  recall:   0.00%  precision:   0.00%  [   60/  250]
loss: 0.0173693298  recall:   0.00%  precision:   0.00%  [   61/  250]
loss: 0.0173534607  recall:   0.00%  precision:   0.00%  [   62/  250]
loss: 0.0173149150  recall:   0.00%  precision:   0.00%  [   63/  250]
loss: 0.0172684213  recall:   0.00%  precision:   0.00%  [   64/  250]
loss: 0.0172423864  recall:   0.00%  precision:   0.00%  [   65/  250]
loss: 0.0172018187  recall:   0.00%  precision:   0.00%  [   66/  250]
loss: 0.0171391107  recall:   0.00%  precision:   0.00%  [   67/  250]
loss: 0.0170877752  recall:   0.00%  precision:   0.00%  [   68/  250]
loss: 0.0170824556  recall:   0.00%  precision:   0.00%  [   69/  250]
loss: 0.0170681281  recall:   0.00%  precision:   0.00%  [   70/  250]
loss: 0.0170041893  recall:   0.00%  precision:   0.00%  [   71/  250]
loss: 0.0169627370  recall:   0.00%  precision:   0.00%  [   72/  250]
loss: 0.0169442351  recall:   0.00%  precision:   0.00%  [   73/  250]
loss: 0.0169070631  recall:   0.00%  precision:   0.00%  [   74/  250]
loss: 0.0168573656  recall:   0.00%  precision:   0.00%  [   75/  250]
loss: 0.0168468753  recall:   0.00%  precision:   0.00%  [   76/  250]
loss: 0.0168110997  recall:   0.00%  precision:   0.00%  [   77/  250]
loss: 0.0167720208  recall:   0.00%  precision:   0.00%  [   78/  250]
loss: 0.0167385793  recall:   0.00%  precision:   0.00%  [   79/  250]
loss: 0.0167214530  recall:   0.00%  precision:   0.00%  [   80/  250]
loss: 0.0166798185  recall:   0.00%  precision:   0.00%  [   81/  250]
loss: 0.0166583699  recall:   0.00%  precision:   0.00%  [   82/  250]
loss: 0.0165905500  recall:   0.00%  precision:   0.00%  [   83/  250]
loss: 0.0165865222  recall:   0.00%  precision:   0.00%  [   84/  250]
loss: 0.0165379806  recall:   0.00%  precision:   0.00%  [   85/  250]
loss: 0.0165160605  recall:   0.00%  precision:   0.00%  [   86/  250]
loss: 0.0165109658  recall:   0.00%  precision:   0.00%  [   87/  250]
loss: 0.0164904416  recall:   0.00%  precision:   0.00%  [   88/  250]
loss: 0.0164352686  recall:   0.00%  precision:   0.00%  [   89/  250]
loss: 0.0163998832  recall:   0.00%  precision:   0.00%  [   90/  250]
loss: 0.0164131410  recall:   0.00%  precision:   0.00%  [   91/  250]
loss: 0.0163387859  recall:   0.00%  precision:   0.00%  [   92/  250]
loss: 0.0163610779  recall:   0.00%  precision:   0.00%  [   93/  250]
loss: 0.0162892780  recall:   0.00%  precision:   0.00%  [   94/  250]
loss: 0.0163308853  recall:   0.00%  precision:   0.00%  [   95/  250]
loss: 0.0162589361  recall:   0.00%  precision:   0.00%  [   96/  250]
loss: 0.0162622093  recall:   0.00%  precision:   0.00%  [   97/  250]
loss: 0.0162062049  recall:   0.00%  precision:   0.00%  [   98/  250]
loss: 0.0162119801  recall:   0.00%  precision:   0.00%  [   99/  250]
loss: 0.0161458186  recall:   0.00%  precision:   0.00%  [  100/  250]
####  eval set loss: 0.0151990613  recall:   0.00%  precision:   0.00%
loss: 0.0161535915  recall:   0.00%  precision:   0.00%  [  101/  250]
loss: 0.0161206434  recall:   0.00%  precision:   0.00%  [  102/  250]
loss: 0.0160985003  recall:   0.00%  precision:   0.00%  [  103/  250]
loss: 0.0160405871  recall:   0.00%  precision:   0.00%  [  104/  250]
loss: 0.0160233157  recall:   0.00%  precision:   0.00%  [  105/  250]
loss: 0.0160160498  recall:   0.00%  precision:   0.00%  [  106/  250]
loss: 0.0159694258  recall:   0.00%  precision:   0.00%  [  107/  250]
loss: 0.0159594419  recall:   0.00%  precision:   0.00%  [  108/  250]
loss: 0.0159277382  recall:   0.00%  precision:   0.00%  [  109/  250]
loss: 0.0159270787  recall:   0.00%  precision:   0.00%  [  110/  250]
loss: 0.0159065090  recall:   0.00%  precision:   0.00%  [  111/  250]
loss: 0.0158724341  recall:   0.00%  precision:   0.00%  [  112/  250]
loss: 0.0158436415  recall:   0.00%  precision:   0.00%  [  113/  250]
loss: 0.0158068402  recall:   0.00%  precision:   0.00%  [  114/  250]
loss: 0.0158104339  recall:   0.00%  precision:   0.00%  [  115/  250]
loss: 0.0157702690  recall:   0.00%  precision:   0.00%  [  116/  250]
loss: 0.0157567851  recall:   0.00%  precision:   0.00%  [  117/  250]
loss: 0.0157305527  recall:   0.00%  precision:   0.00%  [  118/  250]
loss: 0.0157126051  recall:   0.00%  precision:   0.00%  [  119/  250]
loss: 0.0156592295  recall:   0.00%  precision:   0.00%  [  120/  250]
loss: 0.0156511960  recall:   0.00%  precision:   0.00%  [  121/  250]
loss: 0.0156386009  recall:   0.00%  precision:   0.00%  [  122/  250]
loss: 0.0156060610  recall:   0.00%  precision:   0.00%  [  123/  250]
loss: 0.0155937203  recall:   0.00%  precision:   0.00%  [  124/  250]
loss: 0.0155757676  recall:   0.00%  precision:   0.00%  [  125/  250]
loss: 0.0155551970  recall:   0.00%  precision:   0.00%  [  126/  250]
loss: 0.0155334847  recall:   0.00%  precision:   0.00%  [  127/  250]
loss: 0.0154916325  recall:   0.00%  precision:   0.00%  [  128/  250]
loss: 0.0154762541  recall:   0.00%  precision:   0.00%  [  129/  250]
loss: 0.0154411901  recall:   0.00%  precision:   0.00%  [  130/  250]
loss: 0.0154286580  recall:   0.00%  precision:   0.00%  [  131/  250]
loss: 0.0154200925  recall:   0.00%  precision:   0.00%  [  132/  250]
loss: 0.0153897799  recall:   0.00%  precision:   0.00%  [  133/  250]
loss: 0.0153761354  recall:   0.00%  precision:   0.00%  [  134/  250]
loss: 0.0153615525  recall:   0.00%  precision:   0.00%  [  135/  250]
loss: 0.0153378622  recall:   0.00%  precision:   0.00%  [  136/  250]
loss: 0.0153061947  recall:   0.00%  precision:   0.00%  [  137/  250]
loss: 0.0152961996  recall:   0.00%  precision:   0.00%  [  138/  250]
loss: 0.0152679271  recall:   0.00%  precision:   0.00%  [  139/  250]
loss: 0.0152813754  recall:   0.00%  precision:   0.00%  [  140/  250]
loss: 0.0152371485  recall:   0.00%  precision:   0.00%  [  141/  250]
loss: 0.0152252064  recall:   0.00%  precision:   0.00%  [  142/  250]
loss: 0.0152005219  recall:   0.00%  precision:   0.00%  [  143/  250]
loss: 0.0151615879  recall:   0.00%  precision:   0.00%  [  144/  250]
loss: 0.0151398118  recall:   0.00%  precision:   0.00%  [  145/  250]
loss: 0.0151179298  recall:   0.00%  precision:   0.00%  [  146/  250]
loss: 0.0151292549  recall:   0.00%  precision:   0.00%  [  147/  250]
loss: 0.0150973118  recall:   0.00%  precision:   0.00%  [  148/  250]
loss: 0.0150683967  recall:   0.00%  precision:   0.00%  [  149/  250]
loss: 0.0150343168  recall:   0.00%  precision:   0.00%  [  150/  250]
####  eval set loss: 0.0142335501  recall:   0.00%  precision:   0.00%
loss: 0.0150108925  recall:   0.00%  precision:   0.00%  [  151/  250]
loss: 0.0150164825  recall:   0.00%  precision:   0.00%  [  152/  250]
loss: 0.0149806973  recall:   0.00%  precision:   0.00%  [  153/  250]
loss: 0.0149447437  recall:   0.00%  precision:   0.00%  [  154/  250]
loss: 0.0149314011  recall:   0.00%  precision:   0.00%  [  155/  250]
loss: 0.0149003713  recall:   0.00%  precision:   0.00%  [  156/  250]
loss: 0.0148991492  recall:   0.00%  precision:   0.00%  [  157/  250]
loss: 0.0148580516  recall:   0.00%  precision:   0.00%  [  158/  250]
loss: 0.0148582186  recall:   0.00%  precision:   0.00%  [  159/  250]
loss: 0.0148370962  recall:   0.00%  precision:   0.00%  [  160/  250]
loss: 0.0148229930  recall:   0.00%  precision:   0.00%  [  161/  250]
loss: 0.0147964537  recall:   0.00%  precision:   0.00%  [  162/  250]
loss: 0.0147812559  recall:   0.00%  precision:   0.00%  [  163/  250]
loss: 0.0147472910  recall:   0.00%  precision:   0.00%  [  164/  250]
loss: 0.0147300633  recall:   0.00%  precision:   0.00%  [  165/  250]
loss: 0.0147470722  recall:   0.00%  precision:   0.00%  [  166/  250]
loss: 0.0146999006  recall:   0.00%  precision:   0.00%  [  167/  250]
loss: 0.0146775417  recall:   0.00%  precision:   0.00%  [  168/  250]
loss: 0.0146565926  recall:   0.00%  precision:   0.00%  [  169/  250]
loss: 0.0146394856  recall:   0.00%  precision:   0.00%  [  170/  250]
loss: 0.0146126043  recall:   0.00%  precision:   0.00%  [  171/  250]
loss: 0.0146190343  recall:   0.00%  precision:   0.00%  [  172/  250]
loss: 0.0146270955  recall:   0.00%  precision:   0.00%  [  173/  250]
loss: 0.0145639107  recall:   0.00%  precision:   0.00%  [  174/  250]
loss: 0.0145395592  recall:   0.00%  precision:   0.00%  [  175/  250]
loss: 0.0145256936  recall:   0.00%  precision:   0.00%  [  176/  250]
loss: 0.0144995003  recall:   0.00%  precision:   0.00%  [  177/  250]
loss: 0.0144839064  recall:   0.00%  precision:   0.00%  [  178/  250]
loss: 0.0144541413  recall:   0.00%  precision:   0.00%  [  179/  250]
loss: 0.0144438509  recall:   0.00%  precision:   0.00%  [  180/  250]
loss: 0.0144156055  recall:   0.00%  precision:   0.00%  [  181/  250]
loss: 0.0144111299  recall:   0.00%  precision:   0.00%  [  182/  250]
loss: 0.0143878537  recall:   0.00%  precision:   0.00%  [  183/  250]
loss: 0.0144178927  recall:   0.00%  precision:   0.00%  [  184/  250]
loss: 0.0143472008  recall:   0.00%  precision:   0.00%  [  185/  250]
loss: 0.0143508954  recall:   0.00%  precision:   0.00%  [  186/  250]
loss: 0.0143174322  recall:   0.00%  precision:   0.00%  [  187/  250]
loss: 0.0142764109  recall:   0.00%  precision:   0.00%  [  188/  250]
loss: 0.0142830247  recall:   0.00%  precision:   0.00%  [  189/  250]
loss: 0.0142733909  recall:   0.00%  precision:   0.00%  [  190/  250]
loss: 0.0142803159  recall:   0.00%  precision:   0.00%  [  191/  250]
loss: 0.0142372032  recall:   0.00%  precision:   0.00%  [  192/  250]
loss: 0.0142446449  recall:   0.00%  precision:   0.00%  [  193/  250]
loss: 0.0141885431  recall:   0.00%  precision:   0.00%  [  194/  250]
loss: 0.0141684636  recall:   0.00%  precision:   0.00%  [  195/  250]
loss: 0.0141650332  recall:   0.00%  precision:   0.00%  [  196/  250]
loss: 0.0141300705  recall:   0.00%  precision:   0.00%  [  197/  250]
loss: 0.0141172517  recall:   0.00%  precision:   0.00%  [  198/  250]
loss: 0.0141030895  recall:   0.00%  precision:   0.00%  [  199/  250]
loss: 0.0140833319  recall:   0.00%  precision:   0.00%  [  200/  250]
####  eval set loss: 0.0133937867  recall:   0.00%  precision:   0.00%
loss: 0.0140908760  recall:   0.00%  precision:   0.00%  [  201/  250]
loss: 0.0140600400  recall:   0.00%  precision:   0.00%  [  202/  250]
loss: 0.0140605608  recall:   0.00%  precision:   0.00%  [  203/  250]
loss: 0.0140186752  recall:   0.00%  precision:   0.00%  [  204/  250]
loss: 0.0140082299  recall:   0.00%  precision:   0.00%  [  205/  250]
loss: 0.0139862676  recall:   0.00%  precision:   0.00%  [  206/  250]
loss: 0.0139877503  recall:   0.00%  precision:   0.00%  [  207/  250]
loss: 0.0139509152  recall:   0.00%  precision:   0.00%  [  208/  250]
loss: 0.0139488595  recall:   0.01%  precision:  20.00%  [  209/  250]
loss: 0.0139051443  recall:   0.00%  precision:   0.00%  [  210/  250]
loss: 0.0139251959  recall:   0.00%  precision:   0.00%  [  211/  250]
loss: 0.0138678233  recall:   0.01%  precision:  16.67%  [  212/  250]
loss: 0.0138640001  recall:   0.01%  precision:  16.67%  [  213/  250]
loss: 0.0138432535  recall:   0.01%  precision:  20.00%  [  214/  250]
loss: 0.0139931748  recall:   0.01%  precision:  20.00%  [  215/  250]
loss: 0.0138175369  recall:   0.01%  precision:  16.67%  [  216/  250]
loss: 0.0138001498  recall:   0.00%  precision:   0.00%  [  217/  250]
loss: 0.0137941372  recall:   0.01%  precision:  20.00%  [  218/  250]
loss: 0.0137812355  recall:   0.00%  precision:   0.00%  [  219/  250]
loss: 0.0137508596  recall:   0.01%  precision:  20.00%  [  220/  250]
loss: 0.0137344359  recall:   0.01%  precision:  25.00%  [  221/  250]
loss: 0.0137351457  recall:   0.01%  precision:  20.00%  [  222/  250]
loss: 0.0136902351  recall:   0.01%  precision:  20.00%  [  223/  250]
loss: 0.0136919149  recall:   0.01%  precision:  20.00%  [  224/  250]
loss: 0.0136777377  recall:   0.01%  precision:  20.00%  [  225/  250]
loss: 0.0136559895  recall:   0.01%  precision:  25.00%  [  226/  250]
loss: 0.0136665110  recall:   0.02%  precision:  33.33%  [  227/  250]
loss: 0.0136255926  recall:   0.02%  precision:  33.33%  [  228/  250]
loss: 0.0136409026  recall:   0.02%  precision:  33.33%  [  229/  250]
loss: 0.0135956510  recall:   0.02%  precision:  33.33%  [  230/  250]
loss: 0.0135934057  recall:   0.02%  precision:  33.33%  [  231/  250]
loss: 0.0135836429  recall:   0.02%  precision:  33.33%  [  232/  250]
loss: 0.0135659580  recall:   0.02%  precision:  33.33%  [  233/  250]
loss: 0.0135288863  recall:   0.02%  precision:  40.00%  [  234/  250]
loss: 0.0135242579  recall:   0.02%  precision:  33.33%  [  235/  250]
loss: 0.0135010624  recall:   0.02%  precision:  33.33%  [  236/  250]
loss: 0.0135003002  recall:   0.02%  precision:  33.33%  [  237/  250]
loss: 0.0135376662  recall:   0.01%  precision:  20.00%  [  238/  250]
loss: 0.0134708305  recall:   0.02%  precision:  28.57%  [  239/  250]
loss: 0.0134753946  recall:   0.02%  precision:  28.57%  [  240/  250]
loss: 0.0134539441  recall:   0.02%  precision:  28.57%  [  241/  250]
loss: 0.0134423972  recall:   0.02%  precision:  33.33%  [  242/  250]
loss: 0.0134245971  recall:   0.01%  precision:  16.67%  [  243/  250]
loss: 0.0134743513  recall:   0.04%  precision:  42.86%  [  244/  250]
loss: 0.0133806483  recall:   0.04%  precision:  42.86%  [  245/  250]
loss: 0.0133706940  recall:   0.02%  precision:  33.33%  [  246/  250]
loss: 0.0133551965  recall:   0.04%  precision:  42.86%  [  247/  250]
loss: 0.0133459052  recall:   0.04%  precision:  42.86%  [  248/  250]
loss: 0.0133395538  recall:   0.04%  precision:  42.86%  [  249/  250]
loss: 0.0133146965  recall:   0.02%  precision:  33.33%  [  250/  250]
####  eval set loss: 0.0126258000  recall:   0.06%  precision:  33.33%
@@@@  test set loss: 0.0125897808  recall:   0.03%  precision:  20.00%

-------------------------------------------------------------

@@@@  saltbridges
Model weights: 4119
loss: 0.6528711401  recall:   0.00%  precision:   0.00%  [    1/  250]
loss: 0.4321653021  recall:   0.00%  precision:   0.00%  [    2/  250]
loss: 0.1751740992  recall:   0.00%  precision:   0.00%  [    3/  250]
loss: 0.0724298896  recall:   0.00%  precision:   0.00%  [    4/  250]
loss: 0.0436802127  recall:   0.00%  precision:   0.00%  [    5/  250]
loss: 0.0354888550  recall:   0.00%  precision:   0.00%  [    6/  250]
loss: 0.0327098088  recall:   0.00%  precision:   0.00%  [    7/  250]
loss: 0.0313627203  recall:   0.00%  precision:   0.00%  [    8/  250]
loss: 0.0305202993  recall:   0.00%  precision:   0.00%  [    9/  250]
loss: 0.0299838890  recall:   0.00%  precision:   0.00%  [   10/  250]
loss: 0.0295141748  recall:   0.00%  precision:   0.00%  [   11/  250]
loss: 0.0290767015  recall:   0.00%  precision:   0.00%  [   12/  250]
loss: 0.0287485295  recall:   0.00%  precision:   0.00%  [   13/  250]
loss: 0.0283767740  recall:   0.00%  precision:   0.00%  [   14/  250]
loss: 0.0281108631  recall:   0.00%  precision:   0.00%  [   15/  250]
loss: 0.0277595976  recall:   0.00%  precision:   0.00%  [   16/  250]
loss: 0.0274708083  recall:   0.00%  precision:   0.00%  [   17/  250]
loss: 0.0271404764  recall:   0.00%  precision:   0.00%  [   18/  250]
loss: 0.0268556583  recall:   0.00%  precision:   0.00%  [   19/  250]
loss: 0.0266126911  recall:   0.00%  precision:   0.00%  [   20/  250]
loss: 0.0263253139  recall:   0.00%  precision:   0.00%  [   21/  250]
loss: 0.0261042498  recall:   0.00%  precision:   0.00%  [   22/  250]
loss: 0.0258737361  recall:   0.00%  precision:   0.00%  [   23/  250]
loss: 0.0256933370  recall:   0.00%  precision:   0.00%  [   24/  250]
loss: 0.0254550195  recall:   0.00%  precision:   0.00%  [   25/  250]
loss: 0.0252846843  recall:   0.00%  precision:   0.00%  [   26/  250]
loss: 0.0251315424  recall:   0.00%  precision:   0.00%  [   27/  250]
loss: 0.0249340834  recall:   0.00%  precision:   0.00%  [   28/  250]
loss: 0.0247709917  recall:   0.00%  precision:   0.00%  [   29/  250]
loss: 0.0245991782  recall:   0.00%  precision:   0.00%  [   30/  250]
loss: 0.0244857270  recall:   0.00%  precision:   0.00%  [   31/  250]
loss: 0.0243400434  recall:   0.00%  precision:   0.00%  [   32/  250]
loss: 0.0242341426  recall:   0.00%  precision:   0.00%  [   33/  250]
loss: 0.0240954948  recall:   0.00%  precision:   0.00%  [   34/  250]
loss: 0.0239692733  recall:   0.00%  precision:   0.00%  [   35/  250]
loss: 0.0238579688  recall:   0.00%  precision:   0.00%  [   36/  250]
loss: 0.0237392998  recall:   0.00%  precision:   0.00%  [   37/  250]
loss: 0.0236358797  recall:   0.00%  precision:   0.00%  [   38/  250]
loss: 0.0235315853  recall:   0.00%  precision:   0.00%  [   39/  250]
loss: 0.0234460560  recall:   0.00%  precision:   0.00%  [   40/  250]
loss: 0.0233479938  recall:   0.00%  precision:   0.00%  [   41/  250]
loss: 0.0232244951  recall:   0.00%  precision:   0.00%  [   42/  250]
loss: 0.0231460538  recall:   0.00%  precision:   0.00%  [   43/  250]
loss: 0.0230618362  recall:   0.00%  precision:   0.00%  [   44/  250]
loss: 0.0229745052  recall:   0.00%  precision:   0.00%  [   45/  250]
loss: 0.0228717252  recall:   0.00%  precision:   0.00%  [   46/  250]
loss: 0.0228133742  recall:   0.00%  precision:   0.00%  [   47/  250]
loss: 0.0227137235  recall:   0.00%  precision:   0.00%  [   48/  250]
loss: 0.0226241972  recall:   0.00%  precision:   0.00%  [   49/  250]
loss: 0.0225658436  recall:   0.00%  precision:   0.00%  [   50/  250]
####  eval set loss: 0.0173258254  recall:   0.00%  precision:   0.00%
loss: 0.0224997054  recall:   0.00%  precision:   0.00%  [   51/  250]
loss: 0.0224149175  recall:   0.00%  precision:   0.00%  [   52/  250]
loss: 0.0223501213  recall:   0.00%  precision:   0.00%  [   53/  250]
loss: 0.0222506756  recall:   0.00%  precision:   0.00%  [   54/  250]
loss: 0.0221859018  recall:   0.00%  precision:   0.00%  [   55/  250]
loss: 0.0220862855  recall:   0.00%  precision:   0.00%  [   56/  250]
loss: 0.0220273458  recall:   0.00%  precision:   0.00%  [   57/  250]
loss: 0.0219603515  recall:   0.00%  precision:   0.00%  [   58/  250]
loss: 0.0218872309  recall:   0.00%  precision:   0.00%  [   59/  250]
loss: 0.0218339592  recall:   0.00%  precision:   0.00%  [   60/  250]
loss: 0.0217409862  recall:   0.00%  precision:   0.00%  [   61/  250]
loss: 0.0216888929  recall:   0.00%  precision:   0.00%  [   62/  250]
loss: 0.0216110425  recall:   0.00%  precision:   0.00%  [   63/  250]
loss: 0.0215286526  recall:   0.00%  precision:   0.00%  [   64/  250]
loss: 0.0214695902  recall:   0.00%  precision:   0.00%  [   65/  250]
loss: 0.0214226112  recall:   0.00%  precision:   0.00%  [   66/  250]
loss: 0.0213526556  recall:   0.00%  precision:   0.00%  [   67/  250]
loss: 0.0213148008  recall:   0.00%  precision:   0.00%  [   68/  250]
loss: 0.0211948535  recall:   0.00%  precision:   0.00%  [   69/  250]
loss: 0.0211613588  recall:   0.00%  precision:   0.00%  [   70/  250]
loss: 0.0211000828  recall:   0.00%  precision:   0.00%  [   71/  250]
loss: 0.0210215294  recall:   0.00%  precision:   0.00%  [   72/  250]
loss: 0.0209682842  recall:   0.00%  precision:   0.00%  [   73/  250]
loss: 0.0209385045  recall:   0.00%  precision:   0.00%  [   74/  250]
loss: 0.0208742128  recall:   0.00%  precision:   0.00%  [   75/  250]
loss: 0.0208048377  recall:   0.00%  precision:   0.00%  [   76/  250]
loss: 0.0207624059  recall:   0.00%  precision:   0.00%  [   77/  250]
loss: 0.0207042860  recall:   0.00%  precision:   0.00%  [   78/  250]
loss: 0.0206189749  recall:   0.00%  precision:   0.00%  [   79/  250]
loss: 0.0206095676  recall:   0.00%  precision:   0.00%  [   80/  250]
loss: 0.0205188443  recall:   0.00%  precision:   0.00%  [   81/  250]
loss: 0.0204763374  recall:   0.00%  precision:   0.00%  [   82/  250]
loss: 0.0204366238  recall:   0.00%  precision:   0.00%  [   83/  250]
loss: 0.0204272597  recall:   0.00%  precision:   0.00%  [   84/  250]
loss: 0.0203144291  recall:   0.01%  precision:  50.00%  [   85/  250]
loss: 0.0202772382  recall:   0.01%  precision:  66.67%  [   86/  250]
loss: 0.0202002535  recall:   0.01%  precision:  66.67%  [   87/  250]
loss: 0.0202126149  recall:   0.01%  precision:  66.67%  [   88/  250]
loss: 0.0201118718  recall:   0.01%  precision:  66.67%  [   89/  250]
loss: 0.0200888012  recall:   0.01%  precision:  66.67%  [   90/  250]
loss: 0.0200217012  recall:   0.01%  precision:  66.67%  [   91/  250]
loss: 0.0199834287  recall:   0.01%  precision:  66.67%  [   92/  250]
loss: 0.0199421422  recall:   0.01%  precision:  66.67%  [   93/  250]
loss: 0.0199314667  recall:   0.01%  precision:  66.67%  [   94/  250]
loss: 0.0198501631  recall:   0.01%  precision:  66.67%  [   95/  250]
loss: 0.0198100962  recall:   0.01%  precision:  66.67%  [   96/  250]
loss: 0.0197733444  recall:   0.01%  precision:  66.67%  [   97/  250]
loss: 0.0197267013  recall:   0.01%  precision:  66.67%  [   98/  250]
loss: 0.0196511906  recall:   0.01%  precision:  66.67%  [   99/  250]
loss: 0.0196170532  recall:   0.01%  precision:  66.67%  [  100/  250]
####  eval set loss: 0.0151516713  recall:   0.04%  precision: 100.00%
loss: 0.0195775060  recall:   0.01%  precision:  66.67%  [  101/  250]
loss: 0.0195477579  recall:   0.01%  precision:  66.67%  [  102/  250]
loss: 0.0195127071  recall:   0.01%  precision:  66.67%  [  103/  250]
loss: 0.0194558288  recall:   0.02%  precision:  75.00%  [  104/  250]
loss: 0.0194457801  recall:   0.02%  precision:  75.00%  [  105/  250]
loss: 0.0193927557  recall:   0.02%  precision:  75.00%  [  106/  250]
loss: 0.0193484771  recall:   0.02%  precision:  75.00%  [  107/  250]
loss: 0.0192996519  recall:   0.02%  precision:  75.00%  [  108/  250]
loss: 0.0192473431  recall:   0.02%  precision:  75.00%  [  109/  250]
loss: 0.0192001740  recall:   0.02%  precision:  66.67%  [  110/  250]
loss: 0.0191487594  recall:   0.02%  precision:  66.67%  [  111/  250]
loss: 0.0191134712  recall:   0.03%  precision:  71.43%  [  112/  250]
loss: 0.0190828587  recall:   0.03%  precision:  71.43%  [  113/  250]
loss: 0.0190796733  recall:   0.03%  precision:  71.43%  [  114/  250]
loss: 0.0190068436  recall:   0.03%  precision:  71.43%  [  115/  250]
loss: 0.0189361027  recall:   0.03%  precision:  71.43%  [  116/  250]
loss: 0.0189074135  recall:   0.03%  precision:  71.43%  [  117/  250]
loss: 0.0188679854  recall:   0.03%  precision:  71.43%  [  118/  250]
loss: 0.0188308373  recall:   0.03%  precision:  75.00%  [  119/  250]
loss: 0.0188178898  recall:   0.03%  precision:  62.50%  [  120/  250]
loss: 0.0187321612  recall:   0.03%  precision:  66.67%  [  121/  250]
loss: 0.0187362083  recall:   0.03%  precision:  66.67%  [  122/  250]
loss: 0.0186699405  recall:   0.04%  precision:  72.73%  [  123/  250]
loss: 0.0186549209  recall:   0.04%  precision:  70.00%  [  124/  250]
loss: 0.0185887411  recall:   0.04%  precision:  72.73%  [  125/  250]
loss: 0.0185155995  recall:   0.05%  precision:  75.00%  [  126/  250]
loss: 0.0184904919  recall:   0.05%  precision:  76.92%  [  127/  250]
loss: 0.0184589585  recall:   0.05%  precision:  76.92%  [  128/  250]
loss: 0.0183825565  recall:   0.06%  precision:  78.57%  [  129/  250]
loss: 0.0183464168  recall:   0.06%  precision:  80.00%  [  130/  250]
loss: 0.0183166601  recall:   0.07%  precision:  81.25%  [  131/  250]
loss: 0.0182516205  recall:   0.08%  precision:  78.95%  [  132/  250]
loss: 0.0182405317  recall:   0.08%  precision:  71.43%  [  133/  250]
loss: 0.0181933532  recall:   0.08%  precision:  75.00%  [  134/  250]
loss: 0.0181379034  recall:   0.08%  precision:  75.00%  [  135/  250]
loss: 0.0180749551  recall:   0.08%  precision:  76.19%  [  136/  250]
loss: 0.0180812381  recall:   0.10%  precision:  73.08%  [  137/  250]
loss: 0.0180131026  recall:   0.11%  precision:  68.97%  [  138/  250]
loss: 0.0179843430  recall:   0.11%  precision:  65.62%  [  139/  250]
loss: 0.0179371635  recall:   0.12%  precision:  65.71%  [  140/  250]
loss: 0.0179118266  recall:   0.13%  precision:  64.86%  [  141/  250]
loss: 0.0178562702  recall:   0.14%  precision:  63.41%  [  142/  250]
loss: 0.0178196853  recall:   0.15%  precision:  62.22%  [  143/  250]
loss: 0.0177819861  recall:   0.15%  precision:  63.64%  [  144/  250]
loss: 0.0177539550  recall:   0.16%  precision:  62.50%  [  145/  250]
loss: 0.0177496579  recall:   0.16%  precision:  62.00%  [  146/  250]
loss: 0.0176814651  recall:   0.19%  precision:  66.07%  [  147/  250]
loss: 0.0176527570  recall:   0.20%  precision:  65.52%  [  148/  250]
loss: 0.0176299959  recall:   0.20%  precision:  62.30%  [  149/  250]
loss: 0.0175738330  recall:   0.23%  precision:  63.24%  [  150/  250]
####  eval set loss: 0.0137450170  recall:   0.13%  precision:  60.00%
loss: 0.0175480846  recall:   0.22%  precision:  60.00%  [  151/  250]
loss: 0.0175067442  recall:   0.25%  precision:  59.26%  [  152/  250]
loss: 0.0174746433  recall:   0.27%  precision:  59.77%  [  153/  250]
loss: 0.0174420452  recall:   0.27%  precision:  59.09%  [  154/  250]
loss: 0.0174290291  recall:   0.32%  precision:  61.86%  [  155/  250]
loss: 0.0173854948  recall:   0.35%  precision:  62.62%  [  156/  250]
loss: 0.0173303121  recall:   0.40%  precision:  66.96%  [  157/  250]
loss: 0.0173007641  recall:   0.44%  precision:  65.87%  [  158/  250]
loss: 0.0172627703  recall:   0.45%  precision:  64.89%  [  159/  250]
loss: 0.0172546580  recall:   0.47%  precision:  64.75%  [  160/  250]
loss: 0.0172164856  recall:   0.55%  precision:  67.31%  [  161/  250]
loss: 0.0171744549  recall:   0.55%  precision:  66.46%  [  162/  250]
loss: 0.0171799107  recall:   0.57%  precision:  66.87%  [  163/  250]
loss: 0.0171143023  recall:   0.60%  precision:  69.09%  [  164/  250]
loss: 0.0171000920  recall:   0.66%  precision:  68.68%  [  165/  250]
loss: 0.0170787732  recall:   0.68%  precision:  68.62%  [  166/  250]
loss: 0.0170099832  recall:   0.74%  precision:  68.78%  [  167/  250]
loss: 0.0169893534  recall:   0.77%  precision:  69.01%  [  168/  250]
loss: 0.0169701524  recall:   0.87%  precision:  71.12%  [  169/  250]
loss: 0.0169210821  recall:   0.89%  precision:  71.13%  [  170/  250]
loss: 0.0168878600  recall:   0.97%  precision:  71.43%  [  171/  250]
loss: 0.0168796480  recall:   1.02%  precision:  71.59%  [  172/  250]
loss: 0.0168438375  recall:   1.07%  precision:  71.73%  [  173/  250]
loss: 0.0168222922  recall:   1.11%  precision:  71.62%  [  174/  250]
loss: 0.0167796160  recall:   1.18%  precision:  71.88%  [  175/  250]
loss: 0.0167283203  recall:   1.21%  precision:  72.33%  [  176/  250]
loss: 0.0167048797  recall:   1.26%  precision:  72.87%  [  177/  250]
loss: 0.0166777046  recall:   1.26%  precision:  73.17%  [  178/  250]
loss: 0.0166466187  recall:   1.41%  precision:  73.70%  [  179/  250]
loss: 0.0166354533  recall:   1.41%  precision:  74.10%  [  180/  250]
loss: 0.0166035588  recall:   1.52%  precision:  74.87%  [  181/  250]
loss: 0.0165639902  recall:   1.60%  precision:  74.69%  [  182/  250]
loss: 0.0165593630  recall:   1.55%  precision:  75.45%  [  183/  250]
loss: 0.0165124615  recall:   1.73%  precision:  74.66%  [  184/  250]
loss: 0.0165186812  recall:   1.63%  precision:  75.30%  [  185/  250]
loss: 0.0164614587  recall:   1.79%  precision:  74.95%  [  186/  250]
loss: 0.0164306120  recall:   1.76%  precision:  74.94%  [  187/  250]
loss: 0.0164021571  recall:   1.88%  precision:  75.48%  [  188/  250]
loss: 0.0163712312  recall:   1.94%  precision:  75.82%  [  189/  250]
loss: 0.0163477284  recall:   1.92%  precision:  75.88%  [  190/  250]
loss: 0.0163017226  recall:   2.05%  precision:  77.08%  [  191/  250]
loss: 0.0162757557  recall:   2.13%  precision:  77.44%  [  192/  250]
loss: 0.0162684256  recall:   2.16%  precision:  77.80%  [  193/  250]
loss: 0.0162197455  recall:   2.32%  precision:  78.09%  [  194/  250]
loss: 0.0161886224  recall:   2.26%  precision:  77.58%  [  195/  250]
loss: 0.0161624353  recall:   2.32%  precision:  77.78%  [  196/  250]
loss: 0.0161203669  recall:   2.44%  precision:  78.55%  [  197/  250]
loss: 0.0161147815  recall:   2.37%  precision:  78.53%  [  198/  250]
loss: 0.0160632508  recall:   2.72%  precision:  78.81%  [  199/  250]
loss: 0.0160270117  recall:   2.58%  precision:  79.07%  [  200/  250]
####  eval set loss: 0.0128361967  recall:   2.62%  precision:  79.22%
loss: 0.0159996320  recall:   2.66%  precision:  79.81%  [  201/  250]
loss: 0.0159636858  recall:   2.77%  precision:  80.34%  [  202/  250]
loss: 0.0159618314  recall:   2.99%  precision:  80.48%  [  203/  250]
loss: 0.0159192280  recall:   2.92%  precision:  80.67%  [  204/  250]
loss: 0.0158764862  recall:   3.21%  precision:  80.47%  [  205/  250]
loss: 0.0158566105  recall:   3.08%  precision:  81.25%  [  206/  250]
loss: 0.0158360551  recall:   3.33%  precision:  81.18%  [  207/  250]
loss: 0.0157912263  recall:   3.34%  precision:  81.64%  [  208/  250]
loss: 0.0157798362  recall:   3.45%  precision:  81.21%  [  209/  250]
loss: 0.0157578110  recall:   3.58%  precision:  81.58%  [  210/  250]
loss: 0.0156890021  recall:   3.68%  precision:  81.49%  [  211/  250]
loss: 0.0156472855  recall:   3.91%  precision:  82.39%  [  212/  250]
loss: 0.0156211617  recall:   3.92%  precision:  82.50%  [  213/  250]
loss: 0.0156078884  recall:   4.02%  precision:  82.70%  [  214/  250]
loss: 0.0155492771  recall:   4.31%  precision:  82.98%  [  215/  250]
loss: 0.0155037952  recall:   4.05%  precision:  83.33%  [  216/  250]
loss: 0.0154780025  recall:   4.49%  precision:  83.66%  [  217/  250]
loss: 0.0154374897  recall:   4.69%  precision:  83.93%  [  218/  250]
loss: 0.0153955526  recall:   4.61%  precision:  84.08%  [  219/  250]
loss: 0.0153650400  recall:   4.87%  precision:  83.89%  [  220/  250]
loss: 0.0153205512  recall:   4.99%  precision:  84.21%  [  221/  250]
loss: 0.0152937264  recall:   5.05%  precision:  84.89%  [  222/  250]
loss: 0.0152590293  recall:   5.04%  precision:  84.87%  [  223/  250]
loss: 0.0152560502  recall:   5.27%  precision:  85.13%  [  224/  250]
loss: 0.0151927529  recall:   5.51%  precision:  85.20%  [  225/  250]
loss: 0.0151737731  recall:   5.52%  precision:  85.52%  [  226/  250]
loss: 0.0151309708  recall:   5.75%  precision:  85.87%  [  227/  250]
loss: 0.0151019140  recall:   5.90%  precision:  85.99%  [  228/  250]
loss: 0.0150653696  recall:   5.93%  precision:  85.91%  [  229/  250]
loss: 0.0150351501  recall:   6.13%  precision:  86.24%  [  230/  250]
loss: 0.0150046747  recall:   6.62%  precision:  86.11%  [  231/  250]
loss: 0.0149657946  recall:   6.30%  precision:  86.81%  [  232/  250]
loss: 0.0149339006  recall:   6.60%  precision:  86.62%  [  233/  250]
loss: 0.0149086344  recall:   6.74%  precision:  86.50%  [  234/  250]
loss: 0.0148717241  recall:   6.85%  precision:  87.04%  [  235/  250]
loss: 0.0148389434  recall:   7.05%  precision:  86.86%  [  236/  250]
loss: 0.0148032208  recall:   6.99%  precision:  87.21%  [  237/  250]
loss: 0.0147775429  recall:   7.33%  precision:  87.24%  [  238/  250]
loss: 0.0147578470  recall:   7.28%  precision:  87.43%  [  239/  250]
loss: 0.0147259819  recall:   7.56%  precision:  87.58%  [  240/  250]
loss: 0.0146971065  recall:   7.80%  precision:  87.44%  [  241/  250]
loss: 0.0146544702  recall:   7.90%  precision:  87.53%  [  242/  250]
loss: 0.0146421962  recall:   8.06%  precision:  87.81%  [  243/  250]
loss: 0.0146092930  recall:   8.18%  precision:  88.07%  [  244/  250]
loss: 0.0145792503  recall:   8.37%  precision:  88.05%  [  245/  250]
loss: 0.0145440777  recall:   8.46%  precision:  88.12%  [  246/  250]
loss: 0.0145024417  recall:   8.48%  precision:  88.58%  [  247/  250]
loss: 0.0144833760  recall:   8.60%  precision:  88.53%  [  248/  250]
loss: 0.0144496242  recall:   8.87%  precision:  88.42%  [  249/  250]
loss: 0.0144487571  recall:   8.90%  precision:  88.41%  [  250/  250]
####  eval set loss: 0.0119085996  recall:   9.95%  precision:  89.92%
@@@@  test set loss: 0.0133909229  recall:   6.85%  precision:  90.04%

-------------------------------------------------------------

@@@@  halogenbond
Model weights: 4119
loss: 0.5382747563  recall:   0.00%  precision:   0.00%  [    1/  250]
loss: 0.3090702858  recall:   0.00%  precision:   0.00%  [    2/  250]
loss: 0.1523009480  recall:   0.00%  precision:   0.00%  [    3/  250]
loss: 0.0985230699  recall:   0.00%  precision:   0.00%  [    4/  250]
loss: 0.0678922476  recall:   0.00%  precision:   0.00%  [    5/  250]
loss: 0.0442938154  recall:   0.00%  precision:   0.00%  [    6/  250]
loss: 0.0300219835  recall:   0.00%  precision:   0.00%  [    7/  250]
loss: 0.0237241105  recall:   0.00%  precision:   0.00%  [    8/  250]
loss: 0.0208539428  recall:   0.00%  precision:   0.00%  [    9/  250]
loss: 0.0192406748  recall:   0.00%  precision:   0.00%  [   10/  250]
loss: 0.0181827945  recall:   0.00%  precision:   0.00%  [   11/  250]
loss: 0.0171767323  recall:   0.00%  precision:   0.00%  [   12/  250]
loss: 0.0153483481  recall:   0.00%  precision:   0.00%  [   13/  250]
loss: 0.0145737332  recall:   0.00%  precision:   0.00%  [   14/  250]
loss: 0.0141382485  recall:   0.00%  precision:   0.00%  [   15/  250]
loss: 0.0138153158  recall:   0.00%  precision:   0.00%  [   16/  250]
loss: 0.0135666966  recall:   0.00%  precision:   0.00%  [   17/  250]
loss: 0.0133616500  recall:   0.00%  precision:   0.00%  [   18/  250]
loss: 0.0131711281  recall:   0.00%  precision:   0.00%  [   19/  250]
loss: 0.0130097857  recall:   0.00%  precision:   0.00%  [   20/  250]
loss: 0.0128721812  recall:   0.00%  precision:   0.00%  [   21/  250]
loss: 0.0127348226  recall:   0.00%  precision:   0.00%  [   22/  250]
loss: 0.0126229394  recall:   0.00%  precision:   0.00%  [   23/  250]
loss: 0.0125067346  recall:   0.00%  precision:   0.00%  [   24/  250]
loss: 0.0123946075  recall:   0.00%  precision:   0.00%  [   25/  250]
loss: 0.0122845441  recall:   0.00%  precision:   0.00%  [   26/  250]
loss: 0.0121910550  recall:   0.00%  precision:   0.00%  [   27/  250]
loss: 0.0120901783  recall:   0.00%  precision:   0.00%  [   28/  250]
loss: 0.0119950397  recall:   0.00%  precision:   0.00%  [   29/  250]
loss: 0.0119043674  recall:   0.00%  precision:   0.00%  [   30/  250]
loss: 0.0118177063  recall:   0.00%  precision:   0.00%  [   31/  250]
loss: 0.0117520845  recall:   0.00%  precision:   0.00%  [   32/  250]
loss: 0.0116768631  recall:   0.00%  precision:   0.00%  [   33/  250]
loss: 0.0116024465  recall:   0.00%  precision:   0.00%  [   34/  250]
loss: 0.0115224760  recall:   0.00%  precision:   0.00%  [   35/  250]
loss: 0.0114486732  recall:   0.00%  precision:   0.00%  [   36/  250]
loss: 0.0113727993  recall:   0.00%  precision:   0.00%  [   37/  250]
loss: 0.0113088743  recall:   0.00%  precision:   0.00%  [   38/  250]
loss: 0.0112512395  recall:   0.00%  precision:   0.00%  [   39/  250]
loss: 0.0111851358  recall:   0.00%  precision:   0.00%  [   40/  250]
loss: 0.0111135658  recall:   0.00%  precision:   0.00%  [   41/  250]
loss: 0.0110487454  recall:   0.00%  precision:   0.00%  [   42/  250]
loss: 0.0109854115  recall:   0.00%  precision:   0.00%  [   43/  250]
loss: 0.0109298988  recall:   0.00%  precision:   0.00%  [   44/  250]
loss: 0.0108658327  recall:   0.00%  precision:   0.00%  [   45/  250]
loss: 0.0108071603  recall:   0.00%  precision:   0.00%  [   46/  250]
loss: 0.0107406576  recall:   0.00%  precision:   0.00%  [   47/  250]
loss: 0.0106833281  recall:   0.00%  precision:   0.00%  [   48/  250]
loss: 0.0106312413  recall:   0.00%  precision:   0.00%  [   49/  250]
loss: 0.0105721077  recall:   0.00%  precision:   0.00%  [   50/  250]
####  eval set loss: 0.0109590782  recall:   0.00%  precision:   0.00%
loss: 0.0105164131  recall:   0.00%  precision:   0.00%  [   51/  250]
loss: 0.0104612486  recall:   0.00%  precision:   0.00%  [   52/  250]
loss: 0.0104081344  recall:   0.00%  precision:   0.00%  [   53/  250]
loss: 0.0103544835  recall:   0.00%  precision:   0.00%  [   54/  250]
loss: 0.0103109999  recall:   0.00%  precision:   0.00%  [   55/  250]
loss: 0.0102487346  recall:   0.00%  precision:   0.00%  [   56/  250]
loss: 0.0101977417  recall:   0.00%  precision:   0.00%  [   57/  250]
loss: 0.0101486494  recall:   0.00%  precision:   0.00%  [   58/  250]
loss: 0.0100960520  recall:   0.00%  precision:   0.00%  [   59/  250]
loss: 0.0100465992  recall:   0.00%  precision:   0.00%  [   60/  250]
loss: 0.0100089699  recall:   0.00%  precision:   0.00%  [   61/  250]
loss: 0.0099521072  recall:   0.00%  precision:   0.00%  [   62/  250]
loss: 0.0099037033  recall:   0.00%  precision:   0.00%  [   63/  250]
loss: 0.0098575799  recall:   0.00%  precision:   0.00%  [   64/  250]
loss: 0.0098090221  recall:   0.00%  precision:   0.00%  [   65/  250]
loss: 0.0097655745  recall:   0.00%  precision:   0.00%  [   66/  250]
loss: 0.0097181487  recall:   0.00%  precision:   0.00%  [   67/  250]
loss: 0.0096707980  recall:   0.00%  precision:   0.00%  [   68/  250]
loss: 0.0096293865  recall:   0.00%  precision:   0.00%  [   69/  250]
loss: 0.0095822594  recall:   0.18%  precision: 100.00%  [   70/  250]
loss: 0.0095336166  recall:   0.00%  precision:   0.00%  [   71/  250]
loss: 0.0094949538  recall:   0.00%  precision:   0.00%  [   72/  250]
loss: 0.0094475076  recall:   0.00%  precision:   0.00%  [   73/  250]
loss: 0.0094021522  recall:   0.00%  precision:   0.00%  [   74/  250]
loss: 0.0093552580  recall:   0.00%  precision:   0.00%  [   75/  250]
loss: 0.0093114562  recall:   0.00%  precision:   0.00%  [   76/  250]
loss: 0.0092762970  recall:   0.00%  precision:   0.00%  [   77/  250]
loss: 0.0092321697  recall:   0.00%  precision:   0.00%  [   78/  250]
loss: 0.0092008320  recall:   0.00%  precision:   0.00%  [   79/  250]
loss: 0.0091621992  recall:   0.00%  precision:   0.00%  [   80/  250]
loss: 0.0091309847  recall:   0.00%  precision:   0.00%  [   81/  250]
loss: 0.0090948412  recall:   0.00%  precision:   0.00%  [   82/  250]
loss: 0.0090606683  recall:   0.00%  precision:   0.00%  [   83/  250]
loss: 0.0090192807  recall:   0.00%  precision:   0.00%  [   84/  250]
loss: 0.0089909090  recall:   0.00%  precision:   0.00%  [   85/  250]
loss: 0.0089545710  recall:   0.00%  precision:   0.00%  [   86/  250]
loss: 0.0089307557  recall:   0.00%  precision:   0.00%  [   87/  250]
loss: 0.0088969757  recall:   0.00%  precision:   0.00%  [   88/  250]
loss: 0.0088600785  recall:   0.00%  precision:   0.00%  [   89/  250]
loss: 0.0088287729  recall:   0.00%  precision:   0.00%  [   90/  250]
loss: 0.0087983606  recall:   0.00%  precision:   0.00%  [   91/  250]
loss: 0.0087593487  recall:   0.00%  precision:   0.00%  [   92/  250]
loss: 0.0087325630  recall:   0.00%  precision:   0.00%  [   93/  250]
loss: 0.0087052858  recall:   0.18%  precision: 100.00%  [   94/  250]
loss: 0.0086703960  recall:   0.18%  precision:  50.00%  [   95/  250]
loss: 0.0086417744  recall:   0.18%  precision:  50.00%  [   96/  250]
loss: 0.0086108773  recall:   0.00%  precision:   0.00%  [   97/  250]
loss: 0.0085821817  recall:   0.55%  precision:  60.00%  [   98/  250]
loss: 0.0085547833  recall:   0.18%  precision:  25.00%  [   99/  250]
loss: 0.0085203521  recall:   0.18%  precision: 100.00%  [  100/  250]
####  eval set loss: 0.0088471211  recall:   0.00%  precision:   0.00%
loss: 0.0084893280  recall:   0.37%  precision:  50.00%  [  101/  250]
loss: 0.0084553792  recall:   0.37%  precision: 100.00%  [  102/  250]
loss: 0.0084314503  recall:   0.37%  precision:  66.67%  [  103/  250]
loss: 0.0084003469  recall:   0.37%  precision:  66.67%  [  104/  250]
loss: 0.0083721579  recall:   0.18%  precision:  50.00%  [  105/  250]
loss: 0.0083453346  recall:   0.55%  precision: 100.00%  [  106/  250]
loss: 0.0083233332  recall:   0.18%  precision:  50.00%  [  107/  250]
loss: 0.0082938620  recall:   0.55%  precision:  75.00%  [  108/  250]
loss: 0.0082744062  recall:   0.37%  precision:  66.67%  [  109/  250]
loss: 0.0082483748  recall:   0.18%  precision:  50.00%  [  110/  250]
loss: 0.0082256982  recall:   0.18%  precision:  50.00%  [  111/  250]
loss: 0.0082016116  recall:   0.18%  precision:  50.00%  [  112/  250]
loss: 0.0081771279  recall:   0.37%  precision: 100.00%  [  113/  250]
loss: 0.0081504550  recall:   0.37%  precision:  66.67%  [  114/  250]
loss: 0.0081290113  recall:   0.55%  precision:  75.00%  [  115/  250]
loss: 0.0081098082  recall:   0.37%  precision: 100.00%  [  116/  250]
loss: 0.0080842223  recall:   0.18%  precision:  33.33%  [  117/  250]
loss: 0.0080603701  recall:   0.18%  precision: 100.00%  [  118/  250]
loss: 0.0080426238  recall:   0.00%  precision:   0.00%  [  119/  250]
loss: 0.0080176070  recall:   0.18%  precision:  50.00%  [  120/  250]
loss: 0.0079955973  recall:   0.00%  precision:   0.00%  [  121/  250]
loss: 0.0079742432  recall:   0.18%  precision: 100.00%  [  122/  250]
loss: 0.0079544956  recall:   0.37%  precision: 100.00%  [  123/  250]
loss: 0.0079342852  recall:   0.18%  precision: 100.00%  [  124/  250]
loss: 0.0079145439  recall:   0.55%  precision:  75.00%  [  125/  250]
loss: 0.0078923307  recall:   0.00%  precision:   0.00%  [  126/  250]
loss: 0.0078771377  recall:   0.37%  precision:  66.67%  [  127/  250]
loss: 0.0078573096  recall:   0.00%  precision:   0.00%  [  128/  250]
loss: 0.0078430219  recall:   0.37%  precision: 100.00%  [  129/  250]
loss: 0.0078189487  recall:   0.37%  precision:  66.67%  [  130/  250]
loss: 0.0078020526  recall:   0.18%  precision:  50.00%  [  131/  250]
loss: 0.0077833352  recall:   0.37%  precision:  66.67%  [  132/  250]
loss: 0.0077643412  recall:   0.00%  precision:   0.00%  [  133/  250]
loss: 0.0077495679  recall:   0.37%  precision: 100.00%  [  134/  250]
loss: 0.0077324209  recall:   0.55%  precision:  50.00%  [  135/  250]
loss: 0.0077141338  recall:   0.18%  precision: 100.00%  [  136/  250]
loss: 0.0076972242  recall:   0.18%  precision: 100.00%  [  137/  250]
^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[A^[[Aloss: 0.0076786457  recall:   0.18%  precision:  33.33%  [  138/  250]
loss: 0.0076631220  recall:   0.37%  precision: 100.00%  [  139/  250]
loss: 0.0076424868  recall:   0.37%  precision:  66.67%  [  140/  250]
loss: 0.0076269256  recall:   0.00%  precision:   0.00%  [  141/  250]
loss: 0.0076114121  recall:   0.18%  precision:  33.33%  [  142/  250]
loss: 0.0075960513  recall:   0.00%  precision:   0.00%  [  143/  250]
loss: 0.0075763950  recall:   0.55%  precision:  50.00%  [  144/  250]
loss: 0.0075640881  recall:   0.00%  precision:   0.00%  [  145/  250]
loss: 0.0075499567  recall:   0.55%  precision:  50.00%  [  146/  250]
loss: 0.0075351591  recall:   0.18%  precision:  25.00%  [  147/  250]
loss: 0.0075198399  recall:   0.37%  precision:  50.00%  [  148/  250]
loss: 0.0075036969  recall:   0.18%  precision:  33.33%  [  149/  250]
loss: 0.0074853617  recall:   0.18%  precision:  25.00%  [  150/  250]
####  eval set loss: 0.0077922702  recall:   0.52%  precision:  25.00%
loss: 0.0074701170  recall:   0.37%  precision:  66.67%  [  151/  250]
loss: 0.0074568564  recall:   0.18%  precision:  50.00%  [  152/  250]
loss: 0.0074426079  recall:   0.55%  precision:  60.00%  [  153/  250]
loss: 0.0074290432  recall:   0.55%  precision:  60.00%  [  154/  250]
loss: 0.0074175405  recall:   0.37%  precision:  40.00%  [  155/  250]
loss: 0.0073980709  recall:   0.00%  precision:   0.00%  [  156/  250]
loss: 0.0073867598  recall:   0.18%  precision:  20.00%  [  157/  250]
loss: 0.0073708251  recall:   0.37%  precision:  50.00%  [  158/  250]
loss: 0.0073586187  recall:   0.18%  precision:  33.33%  [  159/  250]
loss: 0.0073516283  recall:   0.55%  precision:  50.00%  [  160/  250]
loss: 0.0073333268  recall:   0.74%  precision:  40.00%  [  161/  250]
loss: 0.0073187092  recall:   0.37%  precision:  50.00%  [  162/  250]
loss: 0.0073072653  recall:   0.37%  precision:  66.67%  [  163/  250]
loss: 0.0072930879  recall:   0.55%  precision:  42.86%  [  164/  250]
loss: 0.0072838567  recall:   0.37%  precision:  33.33%  [  165/  250]
loss: 0.0072678237  recall:   0.55%  precision:  42.86%  [  166/  250]
loss: 0.0072597785  recall:   0.18%  precision:  20.00%  [  167/  250]
loss: 0.0072452992  recall:   0.55%  precision:  42.86%  [  168/  250]
loss: 0.0072337460  recall:   0.37%  precision:  50.00%  [  169/  250]
loss: 0.0072223138  recall:   0.74%  precision:  57.14%  [  170/  250]
loss: 0.0072088148  recall:   0.55%  precision:  50.00%  [  171/  250]
loss: 0.0071970176  recall:   0.37%  precision:  28.57%  [  172/  250]
loss: 0.0071874228  recall:   0.37%  precision:  28.57%  [  173/  250]
loss: 0.0071714516  recall:   0.55%  precision:  37.50%  [  174/  250]
loss: 0.0071609496  recall:   0.55%  precision:  33.33%  [  175/  250]
loss: 0.0071501872  recall:   0.74%  precision:  36.36%  [  176/  250]
loss: 0.0071396520  recall:   0.37%  precision:  28.57%  [  177/  250]
loss: 0.0071240620  recall:   0.37%  precision:  33.33%  [  178/  250]
loss: 0.0071174284  recall:   0.37%  precision:  25.00%  [  179/  250]
loss: 0.0071023675  recall:   1.10%  precision:  60.00%  [  180/  250]
loss: 0.0070971843  recall:   0.55%  precision:  33.33%  [  181/  250]
loss: 0.0070930140  recall:   1.10%  precision:  42.86%  [  182/  250]
loss: 0.0070753797  recall:   0.92%  precision:  45.45%  [  183/  250]
loss: 0.0070666234  recall:   0.55%  precision:  33.33%  [  184/  250]
loss: 0.0070496508  recall:   0.92%  precision:  41.67%  [  185/  250]
loss: 0.0070409302  recall:   0.37%  precision:  28.57%  [  186/  250]
loss: 0.0070283937  recall:   0.74%  precision:  28.57%  [  187/  250]
loss: 0.0070177535  recall:   0.55%  precision:  37.50%  [  188/  250]
loss: 0.0070092022  recall:   0.74%  precision:  36.36%  [  189/  250]
loss: 0.0070090238  recall:   0.92%  precision:  38.46%  [  190/  250]
loss: 0.0069945085  recall:   0.18%  precision:  10.00%  [  191/  250]
loss: 0.0069784381  recall:   1.29%  precision:  58.33%  [  192/  250]
loss: 0.0069662493  recall:   1.10%  precision:  46.15%  [  193/  250]
loss: 0.0069568212  recall:   0.55%  precision:  33.33%  [  194/  250]
loss: 0.0069544881  recall:   1.65%  precision:  45.00%  [  195/  250]
loss: 0.0069425344  recall:   0.37%  precision:  14.29%  [  196/  250]
loss: 0.0069417222  recall:   1.47%  precision:  44.44%  [  197/  250]
loss: 0.0069238038  recall:   1.29%  precision:  58.33%  [  198/  250]
loss: 0.0069153496  recall:   1.65%  precision:  60.00%  [  199/  250]
loss: 0.0069028533  recall:   1.10%  precision:  50.00%  [  200/  250]
####  eval set loss: 0.0072090779  recall:   1.05%  precision:  40.00%
loss: 0.0068915181  recall:   0.92%  precision:  41.67%  [  201/  250]
loss: 0.0068924479  recall:   1.10%  precision:  42.86%  [  202/  250]
loss: 0.0068817055  recall:   1.10%  precision:  42.86%  [  203/  250]
loss: 0.0068712515  recall:   1.10%  precision:  42.86%  [  204/  250]
loss: 0.0068588192  recall:   1.47%  precision:  53.33%  [  205/  250]
loss: 0.0068514375  recall:   0.55%  precision:  21.43%  [  206/  250]
loss: 0.0068426045  recall:   1.65%  precision:  45.00%  [  207/  250]
loss: 0.0068333627  recall:   0.92%  precision:  35.71%  [  208/  250]
loss: 0.0068226024  recall:   1.47%  precision:  53.33%  [  209/  250]
loss: 0.0068177521  recall:   1.10%  precision:  50.00%  [  210/  250]
loss: 0.0068162616  recall:   1.29%  precision:  50.00%  [  211/  250]
loss: 0.0068026875  recall:   1.29%  precision:  46.67%  [  212/  250]
loss: 0.0067897549  recall:   1.10%  precision:  50.00%  [  213/  250]
loss: 0.0067811372  recall:   1.65%  precision:  52.94%  [  214/  250]
loss: 0.0067716908  recall:   1.10%  precision:  42.86%  [  215/  250]
loss: 0.0067735069  recall:   1.10%  precision:  33.33%  [  216/  250]
loss: 0.0067586047  recall:   1.29%  precision:  43.75%  [  217/  250]
loss: 0.0067544109  recall:   1.47%  precision:  44.44%  [  218/  250]
loss: 0.0067422993  recall:   1.29%  precision:  53.85%  [  219/  250]
loss: 0.0067350278  recall:   1.29%  precision:  41.18%  [  220/  250]
loss: 0.0067324131  recall:   1.10%  precision:  37.50%  [  221/  250]
loss: 0.0067199286  recall:   1.29%  precision:  35.00%  [  222/  250]
loss: 0.0067137224  recall:   1.47%  precision:  40.00%  [  223/  250]
loss: 0.0067105754  recall:   0.92%  precision:  25.00%  [  224/  250]
loss: 0.0067043019  recall:   1.10%  precision:  50.00%  [  225/  250]
loss: 0.0066929584  recall:   1.84%  precision:  45.45%  [  226/  250]
loss: 0.0066886579  recall:   1.47%  precision:  34.78%  [  227/  250]
loss: 0.0066700958  recall:   1.47%  precision:  53.33%  [  228/  250]
loss: 0.0066676196  recall:   0.74%  precision:  25.00%  [  229/  250]
loss: 0.0066624716  recall:   1.29%  precision:  35.00%  [  230/  250]
loss: 0.0066669897  recall:   2.39%  precision:  41.94%  [  231/  250]
loss: 0.0066509946  recall:   1.29%  precision:  36.84%  [  232/  250]
loss: 0.0066363778  recall:   1.29%  precision:  35.00%  [  233/  250]
loss: 0.0066333021  recall:   1.29%  precision:  31.82%  [  234/  250]
loss: 0.0066303285  recall:   1.10%  precision:  30.00%  [  235/  250]
loss: 0.0066204419  recall:   1.65%  precision:  33.33%  [  236/  250]
loss: 0.0066174990  recall:   1.65%  precision:  37.50%  [  237/  250]
loss: 0.0066039377  recall:   1.65%  precision:  36.00%  [  238/  250]
loss: 0.0065965709  recall:   1.29%  precision:  36.84%  [  239/  250]
loss: 0.0065975907  recall:   1.65%  precision:  33.33%  [  240/  250]
loss: 0.0065891998  recall:   1.47%  precision:  29.63%  [  241/  250]
loss: 0.0065789429  recall:   1.84%  precision:  41.67%  [  242/  250]
loss: 0.0065744163  recall:   1.65%  precision:  32.14%  [  243/  250]
loss: 0.0065752314  recall:   1.29%  precision:  25.93%  [  244/  250]
loss: 0.0065582001  recall:   1.47%  precision:  42.11%  [  245/  250]
loss: 0.0065542222  recall:   1.47%  precision:  33.33%  [  246/  250]
loss: 0.0065447421  recall:   1.65%  precision:  32.14%  [  247/  250]
loss: 0.0065373446  recall:   1.10%  precision:  31.58%  [  248/  250]
loss: 0.0065334315  recall:   1.65%  precision:  36.00%  [  249/  250]
loss: 0.0065325509  recall:   2.02%  precision:  39.29%  [  250/  250]
####  eval set loss: 0.0068025080  recall:   1.57%  precision:  37.50%
@@@@  test set loss: 0.0068395105  recall:   1.43%  precision:  40.00%

-------------------------------------------------------------

RESULT: 0.037288

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
~/Projects/LearnableInteractionKernel (main*) » conda env export > learnable_kernel.ym:l                                                                                                               iwe20@iwe20
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
~/Projects/LearnableInteractionKernel (main*) »                                                                                                                                                  130 ↵ iwe20@iwe20



